{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f828f4ec",
   "metadata": {},
   "source": [
    "# **1. Loading the PETase dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now load \"masterdb.csv\" found under data\n",
    "import pandas as pd \n",
    "import os \n",
    "path = \"data/masterdb.tsv\"\n",
    "df = pd.read_csv(path,sep=\"\\t\")\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"(\", \"\")\n",
    "    .str.replace(\")\", \"\")\n",
    "    .str.replace(\"?\", \"\")\n",
    ")\n",
    "df2 = pd.DataFrame()\n",
    "df2[\"id\"]=df[\"name\"]\n",
    "df2[\"sequence\"] = df[\"protein_sequence\"].astype(str)\n",
    "print(df2)\n",
    "\n",
    "def dftofasta(df,outfile):\n",
    "    with open(outfile,\"w\") as f:\n",
    "        for index,row in df.iterrows():\n",
    "            f.write(f\">{row['id']}\\n\")\n",
    "            f.write(f\"{row['sequence']}\\n\")\n",
    "    return outfile \n",
    "\n",
    "#dftofasta(df2,\"data/masterdb.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3b47a",
   "metadata": {},
   "source": [
    "# **2. Loading external stability and expression datasets**\n",
    "\n",
    "**NESG Solubility** \n",
    "(https://loschmidt.chemi.muni.cz/soluprot/?page=download)\n",
    "* 10k proteins\n",
    "* Labels: exp, sol, uniprot id or local ID \n",
    "* Units: integer \n",
    "\n",
    "**Soluprot Solubility**\n",
    "(https://loschmidt.chemi.muni.cz/soluprot/?page=download)\n",
    "* 11k training, 3k test\n",
    "* Label: solubility, number IDs with no conversion map (has seq)\n",
    "* Unit: 0/1\n",
    "\n",
    "**Price Solubility**\n",
    "(https://pmc.ncbi.nlm.nih.gov/articles/PMC3372292/)\n",
    "* 7k proteins \n",
    "* Label: usability. uniprot id\n",
    "* Unit: 0/1\n",
    "\n",
    "**PSI Solubility** \n",
    "(https://academic.oup.com/bioinformatics/article/36/18/4691/5860015?login=false)\n",
    "* 11k proteins\n",
    "* Label: solubility, Aa0000 ID scheme (has seq)\n",
    "* Unit: 0/1\n",
    "* Note: ecoli with custom IDs, dropped for now \n",
    "\n",
    "**Meltome Stability** \n",
    "(https://meltomeatlas.proteomics.wzw.tum.de/master_meltomeatlasapp/)\n",
    "* 1M variants \n",
    "* Label: temperature, meltpoint, fold_change, uniprot id \n",
    "* Note: ecoli with custom IDs, dropped for now  \n",
    "\n",
    "**FireprotDB Stability** \n",
    "(https://loschmidt.chemi.muni.cz/fireprotdb/)\n",
    "* 53k variants\n",
    "* Label: ddG, dTm, pH, Tm, mutation_effect, uniprot id \n",
    "\n",
    "**ThermomutDB Stability**\n",
    "(https://biosig.lab.uq.edu.au/thermomutdb/downloads)\n",
    "* 12k variants\n",
    "* Label: pH, ddG, temperature, dTm, uniprot/pdb id \n",
    "* Note: these genes were not retrieved from the database due to removal from uniprotkb:  A0A410ZNC6 D0WVP7 G7LSK3 GQ884175 M5A5Y8 Q9REI6\n",
    "\n",
    "**CAFA** \n",
    "(https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/code)\n",
    "* 142k variants\n",
    "\n",
    "**Novozyme**\n",
    "(https://www.kaggle.com/code/jinyuansun/eda-and-finetune-esm)\n",
    "* 31k variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9baf06",
   "metadata": {},
   "source": [
    "**Protsol Solubility**\n",
    "(https://huggingface.co/datasets/AI4Protein/ProtSolM)\n",
    "* 71k proteins\n",
    "* Label: solubility, no ID but has sequence\n",
    "* Unit: 0/1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cb14bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Setting up functions \n",
    "############################################\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "\n",
    "def read_fasta_dict(path: str):\n",
    "    seqs = {}\n",
    "    with open(path) as fh:\n",
    "        for header, seq in SimpleFastaParser(fh):\n",
    "            sid = header.split()[0].strip()\n",
    "            seqs[sid] = seq.strip()\n",
    "    return seqs\n",
    "\n",
    "def load_nesg(csv_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)  # uses CSV header row directly: id, exp, sol\n",
    "    seqs = read_fasta_dict(fasta_path)\n",
    "    #\"sid\" \"usability\" \"fasta\" \n",
    "    df[\"sequence\"] = df[\"id\"].map(seqs)\n",
    "    return df \n",
    "\n",
    "def load_psi(csv_path: str, psi_detail_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)                        # has sid + fasta + labels\n",
    "    psi_all = pd.read_csv(psi_detail_path, sep=\"\\t\")  # extra metadata\n",
    "\n",
    "    # merge on sid\n",
    "    df = df.merge(psi_all, on=\"sid\", how=\"left\")\n",
    "\n",
    "    # sequence is already in the \"fasta\" column\n",
    "    df[\"sequence\"] = df[\"fasta\"]\n",
    "\n",
    "    return df\n",
    "def load_price(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sequence\"] = df[\"fasta\"]\n",
    "    # \"sid\" \"usability\" \"fasta\" \n",
    "    return df\n",
    "\n",
    "def load_soluprot(train_csv: str, test_csv: str,\n",
    "                  train_fasta_path: str, test_fasta_path: str) -> pd.DataFrame:\n",
    "\n",
    "    # load FASTA → dict, keys exactly as in FASTA headers\n",
    "    train_fasta = read_fasta_dict(train_fasta_path)\n",
    "    test_fasta  = read_fasta_dict(test_fasta_path)\n",
    "\n",
    "    # merge FASTA dicts\n",
    "    fasta = {**train_fasta, **test_fasta}\n",
    "\n",
    "    # load CSVs\n",
    "    df1 = pd.read_csv(train_csv)\n",
    "    df2 = pd.read_csv(test_csv)\n",
    "\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # map sequences using the exact ids\n",
    "    df[\"sequence\"] = df[\"sid\"].map(fasta)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_meltome(csv_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # load fasta into dict: {uniprot_id: sequence}\n",
    "    fasta = read_fasta_dict(fasta_path)\n",
    "\n",
    "    # extract uniprot prefix from Protein_ID (before \"_\")\n",
    "    df[\"uniprot_id\"] = df[\"Protein_ID\"].astype(str).apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "    # map sequences\n",
    "    df[\"sequence\"] = df[\"uniprot_id\"].map(fasta)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_fireprot(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # \"uniprot_id\" \"pdb_id\" \"muutation\" \"ddG\" \"dTm\" \"pH\" \"tm\" \"mutation_effect\" \"sequence\"\n",
    "    return df \n",
    "\n",
    "def load_thermomut(json_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    # load JSON metadata\n",
    "    with open(json_path) as fh:\n",
    "        data = json.load(fh)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # load FASTA sequences\n",
    "    fasta_dict = read_fasta_dict(fasta_path)\n",
    "\n",
    "    # map UniProt → sequence\n",
    "    # JSON column is \"uniprot\"\n",
    "    df[\"sequence\"] = df[\"uniprot\"].map(fasta_dict)\n",
    "\n",
    "    # ensure required labels exist even if missing in JSON\n",
    "    for col in [\"ph\",\"ddg\",\"temperature\",\"dtm\",\"PDB_wild\",\n",
    "                \"pdb_mutant\",\"mutation_code\",\"mutated chain\",\"effect\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_protsolm(train_csv: str, test_csv: str) -> pd.DataFrame:\n",
    "    df1 = pd.read_csv(train_csv)\n",
    "    df2 = pd.read_csv(test_csv)\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    # \"aa_seq\" \"detail\"\n",
    "    return df\n",
    "\n",
    "def fasta_merger(fasta_paths: list, outpath: str):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for path in fasta_paths:\n",
    "            with open(path) as fh:\n",
    "                for line in fh:\n",
    "                    out.write(line)\n",
    "\n",
    "def parse_cd_hit_clusters(clstr_path):\n",
    "    clusters = {}\n",
    "    current = None\n",
    "\n",
    "    with open(clstr_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">Cluster\"):\n",
    "                current = line.split()[1]\n",
    "                clusters[current] = []\n",
    "            else:\n",
    "                # Example: \"0       50aa, >SEQ123... *\"\n",
    "                sid = line.split(\">\")[1].split(\"...\")[0]\n",
    "                clusters[current].append(sid)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def fasta_merger_from_dfs(datasets, outpath):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for i, df in enumerate(datasets):\n",
    "            for idx, row in df.iterrows():\n",
    "                seq = row[\"sequence\"]\n",
    "                if seq is None or pd.isna(seq):\n",
    "                    continue\n",
    "                header = f\"{df.__class__.__name__}_{i}_{idx}\"\n",
    "                out.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "def fetch_uniprot_fasta(ids, out_fasta, delay=0.15):\n",
    "    \"\"\"Fetch FASTA for many UniProt IDs with error handling.\"\"\"\n",
    "    with open(out_fasta, \"w\") as out:\n",
    "        for uid in ids:\n",
    "            url = f\"https://rest.uniprot.org/uniprotkb/{uid}.fasta\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "\n",
    "            if r.status_code == 200 and r.text.startswith(\">\"):\n",
    "                out.write(r.text.strip() + \"\\n\")\n",
    "            else:\n",
    "                # write placeholder for failed fetch\n",
    "                out.write(f\">{uid}\\nFAILED_FETCH\\n\")\n",
    "\n",
    "            time.sleep(delay)  # rate-limit to avoid 500 errors\n",
    "\n",
    "\n",
    "\n",
    "def load_novozymes(train_path: str, test_path: str, test_labels_path: str) -> pd.DataFrame:\n",
    "    train_df = pd.read_csv(train_path)         # seq_id, protein_sequence, pH, Tm\n",
    "    test_df = pd.read_csv(test_path)           # seq_id, protein_sequence, pH\n",
    "    test_labels = pd.read_csv(test_labels_path)  # seq_id, Tm\n",
    "\n",
    "    # merge test with its labels\n",
    "    test_df = test_df.merge(test_labels, on=\"seq_id\", how=\"left\")\n",
    "\n",
    "    # unify column names to match your other datasets\n",
    "    train_df[\"sequence\"] = train_df[\"protein_sequence\"]\n",
    "    test_df[\"sequence\"]  = test_df[\"protein_sequence\"]\n",
    "\n",
    "    # combine train + test into one dataframe\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06426298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y8/spmd84w16l78w44w6_yz5h340000gn/T/ipykernel_28883/3962239254.py:81: DtypeWarning: Columns (23,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#setting the paths, loading them into dataframes, and making the merged fasta file to cdhit \n",
    "############################################\n",
    "\n",
    "\n",
    "#PSI_PATH      = \"data/benchmark/sol_benchmark/PSI_Biology_solubility_trainset.csv\"\n",
    "#psi_detail_path = \"data/benchmark/sol_benchmark/PSI_all_data_esol.tab\"\n",
    "NESG_PATH     = \"data/benchmark/sol_benchmark/nesg/nesg.csv\"\n",
    "nesg_fasta_path = \"data/benchmark/sol_benchmark/nesg/nesg.fasta\"\n",
    "PRICE_PATH    = \"data/benchmark/sol_benchmark/Price_usability_trainset.csv\"\n",
    "soluprot_train_path = \"data/benchmark/sol_benchmark/soluprot_data/training_set.csv\"\n",
    "soluprot_test_path = \"data/benchmark/sol_benchmark/soluprot_data/test_set.csv\" \n",
    "soluprot_train_fasta = \"data/benchmark/sol_benchmark/soluprot_data/training_set.fasta\"\n",
    "soluprot_test_fasta = \"data/benchmark/sol_benchmark/soluprot_data/test_set.fasta\" \n",
    "#meltome_path = \"data/benchmark/stab_benchmark/meltome_cross-species.csv\"\n",
    "#meltome_fasta_path = \"data/benchmark/stab_benchmark/meltome_fasta.fasta\"\n",
    "fireprot_path = \"data/benchmark/stab_benchmark/fireprotdb_results_stability.csv\"\n",
    "thermomutdb_path = \"data/benchmark/stab_benchmark/thermomutdb.json\"\n",
    "thermomutdb_fasta = \"data/benchmark/stab_benchmark/thermomutdb.fasta\"\n",
    "protsol_train_path = \"data/benchmark/protsolm_data/protsolm_train.csv\"\n",
    "protsol_test_path = \"data/benchmark/protsolm_data/protsolm_test.csv\"\n",
    "novozyme_test_path = \"data/benchmark/novozymes-enzyme-stability-prediction/test.csv\"\n",
    "novozyme_train_path = \"data/benchmark/novozymes-enzyme-stability-prediction/train.csv\"\n",
    "novozyme_test_labels_path = \"data/benchmark/novozymes-enzyme-stability-prediction/test_labels.csv\"\n",
    "\n",
    "\n",
    "#LOAD DATASETS \n",
    "#psi = load_psi(\"data/benchmark/sol_benchmark/PSI_Biology_solubility_trainset.csv\",\"data/benchmark/sol_benchmark/PSI_all_data_esol.tab\")\n",
    "nesg = load_nesg(NESG_PATH, nesg_fasta_path) #no seq col \n",
    "price = load_price(PRICE_PATH) #fasta \n",
    "soluprot = load_soluprot(\n",
    "    soluprot_train_path,\n",
    "    soluprot_test_path,\n",
    "    soluprot_train_fasta,\n",
    "    soluprot_test_fasta    \n",
    ")\n",
    "fireprot = load_fireprot(fireprot_path) #sequence\n",
    "thermomut = load_thermomut(thermomutdb_path,thermomutdb_fasta)\n",
    "#meltome = load_meltome(meltome_path,meltome_fasta_path) \n",
    "protsolm = load_protsolm(protsol_train_path, protsol_test_path) #aa_seq\n",
    "novozymes = load_novozymes(\n",
    "    novozyme_train_path, novozyme_test_path, novozyme_test_labels_path\n",
    ")\n",
    "\n",
    "#ADD FASTA TO DATAFRAMES IF THE CSV DID NOT HAVE IT\n",
    "nesg_fasta = read_fasta_dict(nesg_fasta_path)\n",
    "nesg[\"sequence\"] = nesg[\"id\"].map(nesg_fasta)\n",
    "price[\"sequence\"] = price[\"fasta\"]\n",
    "protsolm[\"sequence\"] = protsolm[\"aa_seq\"]\n",
    "\n",
    "\n",
    "#MERGE ALL FASTA FILES FOR CD-HIT\n",
    "def fasta_merger_from_dfs(datasets, outpath):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for i, df in enumerate(datasets):\n",
    "            for idx, row in df.iterrows():\n",
    "                seq = row[\"sequence\"]\n",
    "                if seq is None or pd.isna(seq):\n",
    "                    continue\n",
    "                header = f\"{df.__class__.__name__}_{i}_{idx}\"\n",
    "                out.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "datasets = [nesg, price, soluprot, fireprot, protsolm,thermomut, novozymes]\n",
    "fasta_merger_from_dfs(datasets, \"allbenchmarks.fasta\")\n",
    "\n",
    "#SETTING  CANONICAL IDS\n",
    "for i, df in enumerate(datasets):\n",
    "    df[\"canonical_id\"] = [\n",
    "        f\"{df.__class__.__name__}_{i}_{idx}\" for idx in df.index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8943dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96118, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>canonical_ids</th>\n",
       "      <th>representative</th>\n",
       "      <th>merged_block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[DataFrame_3_25912, DataFrame_3_27807, DataFra...</td>\n",
       "      <td>DataFrame_3_25912</td>\n",
       "      <td>experiment_id protein_name uniprot_id pdb_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[DataFrame_6_28079]</td>\n",
       "      <td>DataFrame_6_28079</td>\n",
       "      <td>seq_id                                   pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[DataFrame_6_28080]</td>\n",
       "      <td>DataFrame_6_28080</td>\n",
       "      <td>seq_id                                   pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[DataFrame_6_28081]</td>\n",
       "      <td>DataFrame_6_28081</td>\n",
       "      <td>seq_id                                   pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[DataFrame_6_28082]</td>\n",
       "      <td>DataFrame_6_28082</td>\n",
       "      <td>seq_id                                   pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster_id                                      canonical_ids  \\\n",
       "0          0  [DataFrame_3_25912, DataFrame_3_27807, DataFra...   \n",
       "1          1                                [DataFrame_6_28079]   \n",
       "2          2                                [DataFrame_6_28080]   \n",
       "3          3                                [DataFrame_6_28081]   \n",
       "4          4                                [DataFrame_6_28082]   \n",
       "\n",
       "      representative                                       merged_block  \n",
       "0  DataFrame_3_25912     experiment_id protein_name uniprot_id pdb_i...  \n",
       "1  DataFrame_6_28079     seq_id                                   pr...  \n",
       "2  DataFrame_6_28080     seq_id                                   pr...  \n",
       "3  DataFrame_6_28081     seq_id                                   pr...  \n",
       "4  DataFrame_6_28082     seq_id                                   pr...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################\n",
    "#Running CD-HIT, parsing output and merging the final merged_df\n",
    "############################################\n",
    "\n",
    "#cd-hit -i all_sequences.fasta -o all_sequences_100.fasta -c 1.0 -n 5 -d 0\n",
    "#PARSING THE CLUSTERS FROM CD-HIT INTO ONE MERGED DATAFRAME \n",
    "datasets = {\n",
    "    \"nesg\": nesg,\n",
    "    \"price\": price,\n",
    "    \"soluprot\": soluprot,\n",
    "    \"fireprot\": fireprot,\n",
    "    \"protsolm\": protsolm,\n",
    "    \"thermomutdb\":thermomut,\n",
    "    \"novozymes\": novozymes\n",
    "}\n",
    "\n",
    "# Ensure every df has canonical_id as previously assigned\n",
    "for name, df in datasets.items():\n",
    "    if \"canonical_id\" not in df.columns:\n",
    "        raise ValueError(f\"{name} missing canonical_id\")\n",
    "\n",
    "\n",
    "clusters = parse_cd_hit_clusters(\"data/benchmark/benchmark_cdhit100_cluster.txt\")\n",
    "merged_rows = []\n",
    "for clust_id, members in clusters.items():\n",
    "    rep = members[0]\n",
    "    collected = []\n",
    "    for name, df in datasets.items():\n",
    "        sub = df[df[\"canonical_id\"].isin(members)]\n",
    "        if len(sub) > 0:\n",
    "            sub = sub.copy()\n",
    "            sub[\"source_dataset\"] = name\n",
    "            collected.append(sub)\n",
    "\n",
    "    if collected:\n",
    "        merged_block = pd.concat(collected, ignore_index=True)\n",
    "    else:\n",
    "        merged_block = pd.DataFrame()\n",
    "\n",
    "    merged_rows.append({\n",
    "        \"cluster_id\": clust_id,\n",
    "        \"canonical_ids\": members,\n",
    "        \"representative\": rep,\n",
    "        \"merged_block\": merged_block\n",
    "    })\n",
    "\n",
    "# this is your final merged output\n",
    "merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "print(merged_df.shape)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8d24902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cluster_id                                      canonical_ids  \\\n",
      "0              0  [DataFrame_3_25912, DataFrame_3_27807, DataFra...   \n",
      "1              1                                [DataFrame_6_28079]   \n",
      "2              2                                [DataFrame_6_28080]   \n",
      "3              3                                [DataFrame_6_28081]   \n",
      "4              4                                [DataFrame_6_28082]   \n",
      "...          ...                                                ...   \n",
      "96113      96113                                 [DataFrame_6_2392]   \n",
      "96114      96114                                [DataFrame_6_31072]   \n",
      "96115      96115                                 [DataFrame_6_1497]   \n",
      "96116      96116                                [DataFrame_6_30387]   \n",
      "96117      96117                                 [DataFrame_6_2806]   \n",
      "\n",
      "          representative                                       merged_block  \n",
      "0      DataFrame_3_25912     experiment_id protein_name uniprot_id pdb_i...  \n",
      "1      DataFrame_6_28079     seq_id                                   pr...  \n",
      "2      DataFrame_6_28080     seq_id                                   pr...  \n",
      "3      DataFrame_6_28081     seq_id                                   pr...  \n",
      "4      DataFrame_6_28082     seq_id                                   pr...  \n",
      "...                  ...                                                ...  \n",
      "96113   DataFrame_6_2392     seq_id     protein_sequence   pH           ...  \n",
      "96114  DataFrame_6_31072     seq_id   protein_sequence   pH             ...  \n",
      "96115   DataFrame_6_1497     seq_id  protein_sequence   pH              ...  \n",
      "96116  DataFrame_6_30387     seq_id protein_sequence   pH               ...  \n",
      "96117   DataFrame_6_2806     seq_id protein_sequence   pH               ...  \n",
      "\n",
      "[96118 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c8d640ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_COLS = {\n",
    "    \"fireprot\": [\n",
    "        \"uniprot_id\",\"pdb_id\",\"chain\",\"wild_type\",\"position\",\"mutation\",\n",
    "        \"pH\",\"tm\",\"dTm\",\"ddG\",\"interpro_families\",\"is_essential\",\"sequence\"\n",
    "    ],\n",
    "    \"protsolm\": [\"name\",\"label\",\"detail\",\"sequence\"],\n",
    "    \"novozymes\": [\"seq_id\",\"pH\",\"tm\",\"protein_sequence\",\"sequence\"],\n",
    "    \"nesg\": [\"sid\",\"exp\",\"sol\",\"solubility\",\"fasta\",\"sequence\"],\n",
    "    \"price\": [\"sid\",\"Usability|0=NotUsable|1=Usable\",\"fasta\",\"sequence\"]\n",
    "}\n",
    "\n",
    "def clean_block(df):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    src = df[\"source_dataset\"].iloc[0]\n",
    "    if src not in KEEP_COLS:\n",
    "        return df\n",
    "    keep = KEEP_COLS[src]\n",
    "    # always preserve canonical_id + source_dataset\n",
    "    keep = [c for c in keep if c in df.columns] + [\"canonical_id\",\"source_dataset\"]\n",
    "    return df[keep]\n",
    "\n",
    "merged_df[\"merged_block\"] = merged_df[\"merged_block\"].apply(clean_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdadffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE:\n",
      " missing_pdb_uniprot_ids.txt\n",
      " has_pdb_ids.txt\n",
      " no_uniprot_no_pdb.fasta\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# Attempt to get structure of this benchmark database to make multimodal db but will need to filter the proteins\n",
    "############################################################\n",
    "\n",
    "missing_pdb_uniprot = set()\n",
    "has_pdb_ids = set()\n",
    "no_uniprot_no_pdb_seqs = set()\n",
    "\n",
    "for block in merged_df[\"merged_block\"]:\n",
    "    if not isinstance(block, pd.DataFrame) or block.empty:\n",
    "        continue\n",
    "\n",
    "    cols = block.columns\n",
    "\n",
    "    # 1. uniprot_id present but pdb_id missing\n",
    "    if \"uniprot_id\" in cols:\n",
    "        b = block[[\"uniprot_id\", \"pdb_id\"]].copy()\n",
    "        b = b[b[\"uniprot_id\"].notna()]  # entries with uniprot\n",
    "        b = b[b[\"pdb_id\"].isna()]       # missing pdb\n",
    "        missing_pdb_uniprot.update(b[\"uniprot_id\"].dropna().astype(str).tolist())\n",
    "\n",
    "    # 2. unique pdb ids\n",
    "    if \"pdb_id\" in cols:\n",
    "        ids = block[\"pdb_id\"].dropna().astype(str).tolist()\n",
    "        has_pdb_ids.update(ids)\n",
    "\n",
    "    # 3. no uniprot, no pdb → need sequences\n",
    "    if \"sequence\" in cols:\n",
    "        mask = pd.Series(True, index=block.index)\n",
    "        if \"uniprot_id\" in cols:\n",
    "            mask &= block[\"uniprot_id\"].isna()\n",
    "        if \"pdb_id\" in cols:\n",
    "            mask &= block[\"pdb_id\"].isna()\n",
    "        seqs = block.loc[mask, \"sequence\"].dropna().tolist()\n",
    "        for s in seqs:\n",
    "            no_uniprot_no_pdb_seqs.add(s)\n",
    "\n",
    "\n",
    "############################################################\n",
    "# WRITE OUTPUT FILES\n",
    "############################################################\n",
    "\n",
    "# 1. uniprot present but no pdb\n",
    "with open(\"missing_pdb_uniprot_ids.txt\", \"w\") as f:\n",
    "    for uid in sorted(missing_pdb_uniprot):\n",
    "        f.write(uid + \"\\n\")\n",
    "\n",
    "# 2. pdb ids\n",
    "with open(\"has_pdb_ids.txt\", \"w\") as f:\n",
    "    for pid in sorted(has_pdb_ids):\n",
    "        f.write(pid + \"\\n\")\n",
    "\n",
    "# 3. fasta for no uniprot AND no pdb\n",
    "with open(\"no_uniprot_no_pdb.fasta\", \"w\") as f:\n",
    "    for i, seq in enumerate(sorted(no_uniprot_no_pdb_seqs)):\n",
    "        f.write(f\">seq_{i}\\n{seq}\\n\")\n",
    "\n",
    "print(\"DONE:\")\n",
    "print(\" missing_pdb_uniprot_ids.txt\")\n",
    "print(\" has_pdb_ids.txt\")\n",
    "print(\" no_uniprot_no_pdb.fasta\")\n",
    "\n",
    "\n",
    "#Run mmseqs2 to seq2struct to find top hits, if not, use AF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8feb512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1FEP', '1UHG', '1AZP|1AZP|1AZP|1BF4', '1IMQ', '3VUB|3VUB', '2ADA', '1BNI|1A2P', '1AXB|1BTL|1XPB|1ZG4', '1YPI|1YPI', '1IDS|1IDS|1IDS|1IDS', '2TRX', '2IMM', '1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ', '1CYC', '1CAH', '1DPM', '1B5M', '1UZC', '3MBP|1SVX|1SVX', '3PG0', '1RX4|1DDR|1DDR|1DYJ|5DFR', '1DKT|1DKT', '1HME', '1C52', '1P2P|1P2P', '1AAR|1AAR', '1PDO', '1JU3', '1LBI|1LBI|1LBI|1LBI', '1RIS|1RIS|1RIS|1RIS', '1C9O|1C9O', '3BLS|1KE4', '1YYX', '1BNL', '3HHR|1HGU', '1KCQ', '1QGD|1QGD', '1E21', '1LVE', '1BCX', '1TIN', '1ROP|1ROP', '1ZYM', '1ARR|1ARR', '1HNG|1HNG|1CDC|1CDC', '1I4N', '1RTP', '1TPK', '1IR3|1IR3|1IR3|1IR3', '1TTG|1FNA|1FNF', '1APS', '1BP2|1G4I', '3D2A|1ISP|3D2C', '1CF3', '1HK0', '1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON', '1WIT', '1M21|1M21', '1CYO', '8TIM', '451C', '1KFW', '2Q98', '4ZLU|4ZLU', '2ABD', '3PGK', '3WP4', '1LUC', '1DIL|3SIL', '2CBR', '1OLR', '1BAH', '1HZ6', '1AM7|1AM7|1AM7', '1K9Q|1K9Q', '1POH', '2TS1|2TS1', '1AYF|1AYF', '1IFB|1IFC|2IFB', '1GUY|1GUY|1GUY|1GUY', '1WQ5', '1RHG', '1EL1', '1URK', '1IHB', '2CHF', '1KEV|1KEV|1KEV|1KEV', '1AQH', '1DEC', '1TPE|1TPE', '1ANK', '5CRO|5CRO', '5AZU|5AZU|5AZU|5AZU', '1BYW', '6BQG', '1CEY', '2WSY|2WSY|2WSY|2WSY|1TTQ|1BKS', '1SCE|1SCE|1SCE|1SCE', '3TGL', '1YEA', '1FRD', '2CI2|2CI2|2CI2|2CI2|2CI2|2CI2|1YPC', '1IGV', '1BTA', '1SHF|1SHF|1SHF|1SHF', '1BTM|1BTM', '1QQV|1YU5', '1DPM|1DPM', '1IET', '1TDJ|1TDJ', '1EVQ', '1QGV', '2DRI', '1STN|1EY0', '1FTG|1FLV|1QHE', '1VQB|1VQB', '1PX0|1PX0|1PX0|1PX0', '1MJ5', '1BLC|1BLC', '2A36', '1MJC', '1YCC|1YCC', '1RTB|1KF2|1KF3|1KF5', '1B26|1B26|1B26|1B26|1B26|1B26', '1TUX', '1XAS|1E0W', '2UXY|2UXY|2UXY|2UXY|2UXY|2UXY', '4BLM', '1RBP', '1TUP|1TUP|1TUP|1TUP|1TUP|1SAK|1SAK|1SAK|1SAK|2OCJ', '1A43', '1C8C', '1RGG|1RGG|1SAR|1LNI', '1SSO', '1HFZ|1F6R', '1UWO|1UWO|2H61', '1FXA', '1PGA|1EM7|2GB1', '5ZYR|5ZYR', '1ONC', '1AV1|1AV1|1AV1|1AV1', '1RN1', '1DIV|1DIV|2HBB', '1W4E|1W3D', '1PIN|1PIN', '1IOB|2NVH', '6G4B|6G4B', '1RRO', '1CHK', '1SVX', '6TQ3|6TQ3|6TQ3|6TQ3', '1UBQ', '1AJ3|1SHG|1CUN', '1LS4|1AEP', '1HFY|1HFY|1HMK', '4LYZ', '1FC1|1FC1', '2RN2', '1THQ', '1ANT|1ANT', '1OH0|1OH0', '2O9P|2JIE', '1A23', '1BOY|1BOY', '3PG4|4EY2', '1JIW', '1H8V', '1B0O|1B0O|1B8E', '1HYN|1HYN', '1ADO|1ADO|1ADO|1ADO', '1TEN', '1DKG|1DKG|1DKG', '1MGR', '2AKY|1AKY', '1TCA', '1A5E', '1CTS|1CTS', '1FKJ|1FKJ|1FKB', '2HPR|2HID', '2TRT', '1ACB|1ACB|1CSE', '1CLW|1TYV', '1TIT', '1CQW', '1BPI|5PTI', '1OIA', '1BRF', '2ZTA|2ZTA', '1AG2', '1KA6|1KA6', '2HIP', '1JNX', '1C5G', '1JK9|1JK9', '1C2R', '2LZM|1L63', '6JHM|6JHM|6JHM|6JHM', '1BVC', '1G6N|1G6N', '1BFG|1FGA', '1CSP', '1H7M', '1QM4', '1HUE|1HUE', '1QLP', '1HTI|1HTI|2JK2|2JK2', '2BRD|2BRD|2BRD', '1SUP', '1LRP|1LMB', '1AKK|1I5T', '1MSI', '1N0J|1N0J|1N0J|1N0J', '2AFG|1RG8', '1AMQ|1AMQ', '2CRK|2CRK', '1QJP', '1YNR|1YNR|1YNR|1YNR', '1QND', '1MBG|1GV5', '1H0C|1H0C', '1AYE', '2HMB|1HMS', '1FMK', '2CPP', '1IRO|1IRO|1IRO', '1BVU|1BVU|1BVU|1BVU|1BVU|1BVU', '1IO2', '3SSI|3SSI', '1OTR', '4E5K|4E5K', '1LZ1|1REX|2BQA|1LHM'}\n"
     ]
    }
   ],
   "source": [
    "all_pdb_ids = set()\n",
    "\n",
    "for block in merged_df[\"merged_block\"]:\n",
    "    if isinstance(block, pd.DataFrame) and \"pdb_id\" in block.columns:\n",
    "        all_pdb_ids.update(block[\"pdb_id\"].dropna().unique())\n",
    "print(all_pdb_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcaad5b",
   "metadata": {},
   "source": [
    "# **Fetch esm2 and esm3 embeddings of PETase dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823510a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cda2ad4",
   "metadata": {},
   "source": [
    "# 3. The activity label of the PETase dataset \n",
    "\n",
    "**Docking** \n",
    "\n",
    "**PET catalysis rate** \n",
    "\n",
    "**PET-specific biophysical features** \n",
    "\n",
    "**Computational chemistry of PET and using it as a feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364efbba",
   "metadata": {},
   "source": [
    "# 4. Fine-tuning esm2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b8815",
   "metadata": {},
   "source": [
    "# 5. Fine Tuning esm3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb20a9",
   "metadata": {},
   "source": [
    "# 6. Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd63680",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
