{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f828f4ec",
   "metadata": {},
   "source": [
    "# **1. Loading the PETase dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now load \"masterdb.csv\" found under data\n",
    "import pandas as pd \n",
    "import os \n",
    "path = \"data/masterdb.tsv\"\n",
    "df = pd.read_csv(path,sep=\"\\t\")\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"(\", \"\")\n",
    "    .str.replace(\")\", \"\")\n",
    "    .str.replace(\"?\", \"\")\n",
    ")\n",
    "df2 = pd.DataFrame()\n",
    "df2[\"id\"]=df[\"name\"]\n",
    "df2[\"sequence\"] = df[\"protein_sequence\"].astype(str)\n",
    "print(df2)\n",
    "\n",
    "def dftofasta(df,outfile):\n",
    "    with open(outfile,\"w\") as f:\n",
    "        for index,row in df.iterrows():\n",
    "            f.write(f\">{row['id']}\\n\")\n",
    "            f.write(f\"{row['sequence']}\\n\")\n",
    "    return outfile \n",
    "\n",
    "#dftofasta(df2,\"data/masterdb.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3b47a",
   "metadata": {},
   "source": [
    "# **2. Loading external stability and expression datasets**\n",
    "\n",
    "**NESG Solubility** \n",
    "(https://loschmidt.chemi.muni.cz/soluprot/?page=download)\n",
    "* 10k proteins\n",
    "* Labels: exp, sol, uniprot id or local ID \n",
    "* Units: integer \n",
    "\n",
    "**Soluprot Solubility**\n",
    "(https://loschmidt.chemi.muni.cz/soluprot/?page=download)\n",
    "* 11k training, 3k test\n",
    "* Label: solubility, number IDs with no conversion map (has seq)\n",
    "* Unit: 0/1\n",
    "\n",
    "**Price Solubility**\n",
    "(https://pmc.ncbi.nlm.nih.gov/articles/PMC3372292/)\n",
    "* 7k proteins \n",
    "* Label: usability. uniprot id\n",
    "* Unit: 0/1\n",
    "\n",
    "**PSI Solubility** \n",
    "(https://academic.oup.com/bioinformatics/article/36/18/4691/5860015?login=false)\n",
    "* 11k proteins\n",
    "* Label: solubility, Aa0000 ID scheme (has seq)\n",
    "* Unit: 0/1\n",
    "* Note: ecoli with custom IDs, dropped for now \n",
    "\n",
    "**Meltome Stability** \n",
    "(https://meltomeatlas.proteomics.wzw.tum.de/master_meltomeatlasapp/)\n",
    "* 1M variants \n",
    "* Label: temperature, meltpoint, fold_change, uniprot id \n",
    "* Note: ecoli with custom IDs, dropped for now  \n",
    "\n",
    "**FireprotDB Stability** \n",
    "(https://loschmidt.chemi.muni.cz/fireprotdb/)\n",
    "* 53k variants\n",
    "* Label: ddG, dTm, pH, Tm, mutation_effect, uniprot id \n",
    "\n",
    "**ThermomutDB Stability**\n",
    "(https://biosig.lab.uq.edu.au/thermomutdb/downloads)\n",
    "* 12k variants\n",
    "* Label: pH, ddG, temperature, dTm, uniprot/pdb id \n",
    "* Note: these genes were not retrieved from the database due to removal from uniprotkb:  A0A410ZNC6 D0WVP7 G7LSK3 GQ884175 M5A5Y8 Q9REI6\n",
    "\n",
    "**CAFA** \n",
    "(https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/code)\n",
    "* 142k variants\n",
    "\n",
    "**Novozyme**\n",
    "(https://www.kaggle.com/code/jinyuansun/eda-and-finetune-esm)\n",
    "* 31k variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9baf06",
   "metadata": {},
   "source": [
    "**Protsol Solubility**\n",
    "(https://huggingface.co/datasets/AI4Protein/ProtSolM)\n",
    "* 71k proteins\n",
    "* Label: solubility, no ID but has sequence\n",
    "* Unit: 0/1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb14bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Setting up functions \n",
    "############################################\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "\n",
    "def read_fasta_dict(path: str):\n",
    "    seqs = {}\n",
    "    with open(path) as fh:\n",
    "        for header, seq in SimpleFastaParser(fh):\n",
    "            sid = header.split()[0].strip()\n",
    "            seqs[sid] = seq.strip()\n",
    "    return seqs\n",
    "\n",
    "def load_nesg(csv_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)  # uses CSV header row directly: id, exp, sol\n",
    "    seqs = read_fasta_dict(fasta_path)\n",
    "    #\"sid\" \"usability\" \"fasta\" \n",
    "    df[\"sequence\"] = df[\"id\"].map(seqs)\n",
    "    return df \n",
    "\n",
    "def load_psi(csv_path: str, psi_detail_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)                        # has sid + fasta + labels\n",
    "    psi_all = pd.read_csv(psi_detail_path, sep=\"\\t\")  # extra metadata\n",
    "\n",
    "    # merge on sid\n",
    "    df = df.merge(psi_all, on=\"sid\", how=\"left\")\n",
    "\n",
    "    # sequence is already in the \"fasta\" column\n",
    "    df[\"sequence\"] = df[\"fasta\"]\n",
    "\n",
    "    return df\n",
    "def load_price(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sequence\"] = df[\"fasta\"]\n",
    "    # \"sid\" \"usability\" \"fasta\" \n",
    "    return df\n",
    "\n",
    "def load_soluprot(train_csv: str, test_csv: str,\n",
    "                  train_fasta_path: str, test_fasta_path: str) -> pd.DataFrame:\n",
    "\n",
    "    # load FASTA → dict, keys exactly as in FASTA headers\n",
    "    train_fasta = read_fasta_dict(train_fasta_path)\n",
    "    test_fasta  = read_fasta_dict(test_fasta_path)\n",
    "\n",
    "    # merge FASTA dicts\n",
    "    fasta = {**train_fasta, **test_fasta}\n",
    "\n",
    "    # load CSVs\n",
    "    df1 = pd.read_csv(train_csv)\n",
    "    df2 = pd.read_csv(test_csv)\n",
    "\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # map sequences using the exact ids\n",
    "    df[\"sequence\"] = df[\"sid\"].map(fasta)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_meltome(csv_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # load fasta into dict: {uniprot_id: sequence}\n",
    "    fasta = read_fasta_dict(fasta_path)\n",
    "\n",
    "    # extract uniprot prefix from Protein_ID (before \"_\")\n",
    "    df[\"uniprot_id\"] = df[\"Protein_ID\"].astype(str).apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "    # map sequences\n",
    "    df[\"sequence\"] = df[\"uniprot_id\"].map(fasta)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_fireprot(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # \"uniprot_id\" \"pdb_id\" \"muutation\" \"ddG\" \"dTm\" \"pH\" \"tm\" \"mutation_effect\" \"sequence\"\n",
    "    return df \n",
    "\n",
    "def load_thermomut(json_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    # load JSON metadata\n",
    "    with open(json_path) as fh:\n",
    "        data = json.load(fh)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # load FASTA sequences\n",
    "    fasta_dict = read_fasta_dict(fasta_path)\n",
    "\n",
    "    # map UniProt → sequence\n",
    "    # JSON column is \"uniprot\"\n",
    "    df[\"sequence\"] = df[\"uniprot\"].map(fasta_dict)\n",
    "\n",
    "    # ensure required labels exist even if missing in JSON\n",
    "    for col in [\"ph\",\"ddg\",\"temperature\",\"dtm\",\"PDB_wild\",\n",
    "                \"pdb_mutant\",\"mutation_code\",\"mutated chain\",\"effect\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_protsolm(train_csv: str, test_csv: str) -> pd.DataFrame:\n",
    "    df1 = pd.read_csv(train_csv)\n",
    "    df2 = pd.read_csv(test_csv)\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    # \"aa_seq\" \"detail\"\n",
    "    return df\n",
    "\n",
    "def fasta_merger(fasta_paths: list, outpath: str):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for path in fasta_paths:\n",
    "            with open(path) as fh:\n",
    "                for line in fh:\n",
    "                    out.write(line)\n",
    "\n",
    "def parse_cd_hit_clusters(clstr_path):\n",
    "    clusters = {}\n",
    "    current = None\n",
    "\n",
    "    with open(clstr_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">Cluster\"):\n",
    "                current = line.split()[1]\n",
    "                clusters[current] = []\n",
    "            else:\n",
    "                # Example: \"0       50aa, >SEQ123... *\"\n",
    "                sid = line.split(\">\")[1].split(\"...\")[0]\n",
    "                clusters[current].append(sid)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def fasta_merger_from_dfs(datasets, outpath):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for i, df in enumerate(datasets):\n",
    "            for idx, row in df.iterrows():\n",
    "                seq = row[\"sequence\"]\n",
    "                if seq is None or pd.isna(seq):\n",
    "                    continue\n",
    "                header = f\"{df.__class__.__name__}_{i}_{idx}\"\n",
    "                out.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "def fetch_uniprot_fasta(ids, out_fasta, delay=0.15):\n",
    "    \"\"\"Fetch FASTA for many UniProt IDs with error handling.\"\"\"\n",
    "    with open(out_fasta, \"w\") as out:\n",
    "        for uid in ids:\n",
    "            url = f\"https://rest.uniprot.org/uniprotkb/{uid}.fasta\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "\n",
    "            if r.status_code == 200 and r.text.startswith(\">\"):\n",
    "                out.write(r.text.strip() + \"\\n\")\n",
    "            else:\n",
    "                # write placeholder for failed fetch\n",
    "                out.write(f\">{uid}\\nFAILED_FETCH\\n\")\n",
    "\n",
    "            time.sleep(delay)  # rate-limit to avoid 500 errors\n",
    "\n",
    "\n",
    "\n",
    "def load_novozymes(train_path: str, test_path: str, test_labels_path: str) -> pd.DataFrame:\n",
    "    train_df = pd.read_csv(train_path)         # seq_id, protein_sequence, pH, Tm\n",
    "    test_df = pd.read_csv(test_path)           # seq_id, protein_sequence, pH\n",
    "    test_labels = pd.read_csv(test_labels_path)  # seq_id, Tm\n",
    "\n",
    "    # merge test with its labels\n",
    "    test_df = test_df.merge(test_labels, on=\"seq_id\", how=\"left\")\n",
    "\n",
    "    # unify column names to match your other datasets\n",
    "    train_df[\"sequence\"] = train_df[\"protein_sequence\"]\n",
    "    test_df[\"sequence\"]  = test_df[\"protein_sequence\"]\n",
    "\n",
    "    # combine train + test into one dataframe\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ac22482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\petase-1\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06426298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_27684\\3962239254.py:81: DtypeWarning: Columns (23,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/benchmark/novozymes/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#meltome = load_meltome(meltome_path,meltome_fasta_path) \u001b[39;00m\n\u001b[0;32m     40\u001b[0m protsolm \u001b[38;5;241m=\u001b[39m load_protsolm(protsol_train_path, protsol_test_path) \u001b[38;5;66;03m#aa_seq\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m novozymes \u001b[38;5;241m=\u001b[39m \u001b[43mload_novozymes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/benchmark/novozymes/train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/benchmark/novozymes/test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/benchmark/novozymes/test_labels.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     45\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#ADD FASTA TO DATAFRAMES IF THE CSV DID NOT HAVE IT\u001b[39;00m\n\u001b[0;32m     48\u001b[0m nesg_fasta \u001b[38;5;241m=\u001b[39m read_fasta_dict(nesg_fasta_path)\n",
      "Cell \u001b[1;32mIn[56], line 165\u001b[0m, in \u001b[0;36mload_novozymes\u001b[1;34m(train_path, test_path, test_labels_path)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_novozymes\u001b[39m(train_path: \u001b[38;5;28mstr\u001b[39m, test_path: \u001b[38;5;28mstr\u001b[39m, test_labels_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m--> 165\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# seq_id, protein_sequence, pH, Tm\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_path)           \u001b[38;5;66;03m# seq_id, protein_sequence, pH\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     test_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_labels_path)  \u001b[38;5;66;03m# seq_id, Tm\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\justi\\petase-1\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\justi\\petase-1\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\justi\\petase-1\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\justi\\petase-1\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\justi\\petase-1\\venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/benchmark/novozymes/train.csv'"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#setting the paths, loading them into dataframes, and making the merged fasta file to cdhit \n",
    "############################################\n",
    "\n",
    "\n",
    "#PSI_PATH      = \"data/benchmark/sol_benchmark/PSI_Biology_solubility_trainset.csv\"\n",
    "#psi_detail_path = \"data/benchmark/sol_benchmark/PSI_all_data_esol.tab\"\n",
    "NESG_PATH     = \"data/benchmark/sol_benchmark/nesg/nesg.csv\"\n",
    "nesg_fasta_path = \"data/benchmark/sol_benchmark/nesg/nesg.fasta\"\n",
    "PRICE_PATH    = \"data/benchmark/sol_benchmark/Price_usability_trainset.csv\"\n",
    "soluprot_train_path = \"data/benchmark/sol_benchmark/soluprot_data/training_set.csv\"\n",
    "soluprot_test_path = \"data/benchmark/sol_benchmark/soluprot_data/test_set.csv\" \n",
    "soluprot_train_fasta = \"data/benchmark/sol_benchmark/soluprot_data/training_set.fasta\"\n",
    "soluprot_test_fasta = \"data/benchmark/sol_benchmark/soluprot_data/test_set.fasta\" \n",
    "#meltome_path = \"data/benchmark/stab_benchmark/meltome_cross-species.csv\"\n",
    "#meltome_fasta_path = \"data/benchmark/stab_benchmark/meltome_fasta.fasta\"\n",
    "fireprot_path = \"data/benchmark/stab_benchmark/fireprotdb_results_stability.csv\"\n",
    "thermomutdb_path = \"data/benchmark/stab_benchmark/thermomutdb.json\"\n",
    "thermomutdb_fasta = \"data/benchmark/stab_benchmark/thermomutdb.fasta\"\n",
    "protsol_train_path = \"data/benchmark/protsolm_data/protsolm_train.csv\"\n",
    "protsol_test_path = \"data/benchmark/protsolm_data/protsolm_test.csv\"\n",
    "novozyme_test_path = \"data/benchmark/novozymes-enzyme-stability-prediction/test.csv.csv\"\n",
    "novozyme_train_path = \"data/benchmark/novozymes-enzyme-stability-prediction/train.csv.csv\"\n",
    "novozyme_test_labels_path = \"data/benchmark/novozymes-enzyme-stability-prediction/test_labels.csv\"\n",
    "\n",
    "\n",
    "#LOAD DATASETS \n",
    "#psi = load_psi(\"data/benchmark/sol_benchmark/PSI_Biology_solubility_trainset.csv\",\"data/benchmark/sol_benchmark/PSI_all_data_esol.tab\")\n",
    "nesg = load_nesg(NESG_PATH, nesg_fasta_path) #no seq col \n",
    "price = load_price(PRICE_PATH) #fasta \n",
    "soluprot = load_soluprot(\n",
    "    soluprot_train_path,\n",
    "    soluprot_test_path,\n",
    "    soluprot_train_fasta,\n",
    "    soluprot_test_fasta    \n",
    ")\n",
    "fireprot = load_fireprot(fireprot_path) #sequence\n",
    "thermomut = load_thermomut(thermomutdb_path,thermomutdb_fasta)\n",
    "#meltome = load_meltome(meltome_path,meltome_fasta_path) \n",
    "protsolm = load_protsolm(protsol_train_path, protsol_test_path) #aa_seq\n",
    "novozymes = load_novozymes(\n",
    "    \"data/benchmark/novozymes/train.csv\",\n",
    "    \"data/benchmark/novozymes/test.csv\",\n",
    "    \"data/benchmark/novozymes/test_labels.csv\"\n",
    ")\n",
    "\n",
    "#ADD FASTA TO DATAFRAMES IF THE CSV DID NOT HAVE IT\n",
    "nesg_fasta = read_fasta_dict(nesg_fasta_path)\n",
    "nesg[\"sequence\"] = nesg[\"id\"].map(nesg_fasta)\n",
    "price[\"sequence\"] = price[\"fasta\"]\n",
    "protsolm[\"sequence\"] = protsolm[\"aa_seq\"]\n",
    "\n",
    "\n",
    "#MERGE ALL FASTA FILES FOR CD-HIT\n",
    "def fasta_merger_from_dfs(datasets, outpath):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for i, df in enumerate(datasets):\n",
    "            for idx, row in df.iterrows():\n",
    "                seq = row[\"sequence\"]\n",
    "                if seq is None or pd.isna(seq):\n",
    "                    continue\n",
    "                header = f\"{df.__class__.__name__}_{i}_{idx}\"\n",
    "                out.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "datasets = [nesg, price, soluprot, fireprot, protsolm,thermomut, novozymes]\n",
    "fasta_merger_from_dfs(datasets, \"allbenchmarks.fasta\")\n",
    "\n",
    "\n",
    "#SETTING  CANONICAL IDS\n",
    "for i, df in enumerate(datasets):\n",
    "    df[\"canonical_id\"] = [\n",
    "        f\"{df.__class__.__name__}_{i}_{idx}\" for idx in df.index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67661, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>canonical_ids</th>\n",
       "      <th>representative</th>\n",
       "      <th>merged_block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[DataFrame_3_25912, DataFrame_3_27807, DataFra...</td>\n",
       "      <td>DataFrame_3_25912</td>\n",
       "      <td>experiment_id protein_name uniprot_id pdb_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[DataFrame_3_20645, DataFrame_3_51479, DataFra...</td>\n",
       "      <td>DataFrame_3_20645</td>\n",
       "      <td>experiment_id protein_name uniprot_id pdb_id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[DataFrame_3_16169, DataFrame_3_25782, DataFra...</td>\n",
       "      <td>DataFrame_3_16169</td>\n",
       "      <td>experiment_id                             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[DataFrame_3_21285, DataFrame_3_21286, DataFra...</td>\n",
       "      <td>DataFrame_3_21285</td>\n",
       "      <td>experiment_id protein_name uniprot_id     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[DataFrame_3_20650, DataFrame_3_20651, DataFra...</td>\n",
       "      <td>DataFrame_3_20650</td>\n",
       "      <td>experiment_id protein_name uniprot_id pdb_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster_id                                      canonical_ids  \\\n",
       "0          0  [DataFrame_3_25912, DataFrame_3_27807, DataFra...   \n",
       "1          1  [DataFrame_3_20645, DataFrame_3_51479, DataFra...   \n",
       "2          2  [DataFrame_3_16169, DataFrame_3_25782, DataFra...   \n",
       "3          3  [DataFrame_3_21285, DataFrame_3_21286, DataFra...   \n",
       "4          4  [DataFrame_3_20650, DataFrame_3_20651, DataFra...   \n",
       "\n",
       "      representative                                       merged_block  \n",
       "0  DataFrame_3_25912     experiment_id protein_name uniprot_id pdb_i...  \n",
       "1  DataFrame_3_20645    experiment_id protein_name uniprot_id pdb_id...  \n",
       "2  DataFrame_3_16169      experiment_id                             ...  \n",
       "3  DataFrame_3_21285      experiment_id protein_name uniprot_id     ...  \n",
       "4  DataFrame_3_20650      experiment_id protein_name uniprot_id pdb_...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################\n",
    "#Running CD-HIT, parsing output and merging the final merged_df\n",
    "############################################\n",
    "\n",
    "#cd-hit -i all_sequences.fasta -o all_sequences_100.fasta -c 1.0 -n 5 -d 0\n",
    "#PARSING THE CLUSTERS FROM CD-HIT INTO ONE MERGED DATAFRAME \n",
    "datasets = {\n",
    "    \"nesg\": nesg,\n",
    "    \"price\": price,\n",
    "    \"soluprot\": soluprot,\n",
    "    \"fireprot\": fireprot,\n",
    "    \"protsolm\": protsolm,\n",
    "    \"thermomutdb\":thermomut,\n",
    "    \"meltome\":meltome\n",
    "}\n",
    "\n",
    "# Ensure every df has canonical_id as previously assigned\n",
    "for name, df in datasets.items():\n",
    "    if \"canonical_id\" not in df.columns:\n",
    "        raise ValueError(f\"{name} missing canonical_id\")\n",
    "\n",
    "\n",
    "clusters = parse_cd_hit_clusters(\"data/benchmark/benchmark_cdhit100_cluster.txt\")\n",
    "merged_rows = []\n",
    "for clust_id, members in clusters.items():\n",
    "    rep = members[0]\n",
    "    collected = []\n",
    "    for name, df in datasets.items():\n",
    "        sub = df[df[\"canonical_id\"].isin(members)]\n",
    "        if len(sub) > 0:\n",
    "            sub = sub.copy()\n",
    "            sub[\"source_dataset\"] = name\n",
    "            collected.append(sub)\n",
    "\n",
    "    if collected:\n",
    "        merged_block = pd.concat(collected, ignore_index=True)\n",
    "    else:\n",
    "        merged_block = pd.DataFrame()\n",
    "\n",
    "    merged_rows.append({\n",
    "        \"cluster_id\": clust_id,\n",
    "        \"canonical_ids\": members,\n",
    "        \"representative\": rep,\n",
    "        \"merged_block\": merged_block\n",
    "    })\n",
    "\n",
    "# this is your final merged output\n",
    "merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "print(merged_df.shape)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8823e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_dataset\n",
      "protsolm    68512\n",
      "fireprot    53445\n",
      "nesg         9703\n",
      "price        7259\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all merged_block dfs into one big df\n",
    "all_rows = []\n",
    "for i, row in merged_df.iterrows():\n",
    "    block = row[\"merged_block\"]\n",
    "    if not block.empty:\n",
    "        all_rows.append(block)\n",
    "\n",
    "all_rows_df = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "# Count proteins per dataset\n",
    "dataset_totals = all_rows_df[\"source_dataset\"].value_counts()\n",
    "print(dataset_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcaad5b",
   "metadata": {},
   "source": [
    "# **Fetch esm2 and esm3 embeddings of PETase dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823510a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cda2ad4",
   "metadata": {},
   "source": [
    "# 3. The activity label of the PETase dataset \n",
    "\n",
    "**Docking** \n",
    "\n",
    "**PET catalysis rate** \n",
    "\n",
    "**PET-specific biophysical features** \n",
    "\n",
    "**Computational chemistry of PET and using it as a feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364efbba",
   "metadata": {},
   "source": [
    "# 4. Fine-tuning esm2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b8815",
   "metadata": {},
   "source": [
    "# 5. Fine Tuning esm3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb20a9",
   "metadata": {},
   "source": [
    "# 6. Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd63680",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
