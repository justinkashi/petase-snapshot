{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614e9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813465d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 313 IDs. Fetching taxonomy safely...\n",
      "Processing batch 0 to 20...\n",
      "Processing batch 20 to 40...\n",
      "Processing batch 40 to 60...\n",
      "Processing batch 60 to 80...\n",
      "Processing batch 80 to 100...\n",
      "Processing batch 100 to 120...\n",
      "Processing batch 120 to 140...\n",
      "Processing batch 140 to 160...\n",
      "Processing batch 160 to 180...\n",
      "Processing batch 180 to 200...\n",
      "Processing batch 200 to 220...\n",
      "Processing batch 220 to 240...\n",
      "Processing batch 240 to 260...\n",
      "Processing batch 260 to 280...\n",
      "Processing batch 280 to 300...\n",
      "Processing batch 300 to 313...\n",
      "Done! Results saved to taxonomy_map.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from Bio import Entrez\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"tournament_wt_id.txt\"  # Your file name\n",
    "OUTPUT_FILE = \"taxonomy_map.csv\"\n",
    "EMAIL = \"justin.seyedmoomenkashi@mail.mcgill.ca\"     # REQUIRED: Change this\n",
    "# ---------------------\n",
    "\n",
    "Entrez.email = EMAIL\n",
    "\n",
    "def get_taxonomy_robust(input_path, output_path):\n",
    "    # 1. Read IDs\n",
    "    with open(input_path, \"r\") as f:\n",
    "        id_list = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(f\"Loaded {len(id_list)} IDs. Fetching taxonomy safely...\")\n",
    "\n",
    "    with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Original_ID\", \"Accession\", \"Organism\", \"TaxId\"])\n",
    "\n",
    "        # 2. Process in small batches using ESEARCH (safer for mixed IDs)\n",
    "        batch_size = 20  # Smaller batch size for Search URL limits\n",
    "        \n",
    "        for i in range(0, len(id_list), batch_size):\n",
    "            batch = id_list[i:i+batch_size]\n",
    "            print(f\"Processing batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "            try:\n",
    "                # Step A: Search for the IDs to get their numeric GI numbers\n",
    "                # We join them with \" OR \" to search multiple at once\n",
    "                search_term = \" OR \".join(batch)\n",
    "                search_handle = Entrez.esearch(db=\"protein\", term=search_term, retmax=batch_size)\n",
    "                search_results = Entrez.read(search_handle)\n",
    "                \n",
    "                # Get the list of numeric UIDs (GI numbers) found\n",
    "                uids = search_results[\"IdList\"]\n",
    "\n",
    "                if not uids:\n",
    "                    print(f\"  Warning: No results found for batch starting at {batch[0]}\")\n",
    "                    continue\n",
    "\n",
    "                # Step B: Fetch details using the numeric UIDs (Guaranteed to work)\n",
    "                summary_handle = Entrez.esummary(db=\"protein\", id=\",\".join(uids))\n",
    "                records = Entrez.read(summary_handle)\n",
    "\n",
    "                for record in records:\n",
    "                    # AccessionVersion is the clean ID (e.g. WP_123.1)\n",
    "                    acc = record.get(\"AccessionVersion\", \"Unknown\")\n",
    "                    organism = record.get(\"Organism\", \"Unknown\")\n",
    "                    taxid = record.get(\"TaxId\", \"Unknown\")\n",
    "                    \n",
    "                    # We try to match it back to your original input list if possible\n",
    "                    # (Note: The order might shuffle slightly due to 'OR' search)\n",
    "                    writer.writerow([acc, acc, organism, taxid])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error on batch {i}: {e}\")\n",
    "                # Fallback: Try one-by-one if a batch fails completely\n",
    "                print(\"  -> Retrying this batch one-by-one...\")\n",
    "                for single_id in batch:\n",
    "                    try:\n",
    "                        sh = Entrez.esearch(db=\"protein\", term=single_id)\n",
    "                        sr = Entrez.read(sh)\n",
    "                        if sr[\"IdList\"]:\n",
    "                            sumh = Entrez.esummary(db=\"protein\", id=sr[\"IdList\"][0])\n",
    "                            rec = Entrez.read(sumh)[0]\n",
    "                            writer.writerow([single_id, rec.get(\"AccessionVersion\"), rec.get(\"Organism\"), rec.get(\"TaxId\")])\n",
    "                        else:\n",
    "                            writer.writerow([single_id, \"Not Found\", \"-\", \"-\"])\n",
    "                    except:\n",
    "                        writer.writerow([single_id, \"Error\", \"-\", \"-\"])\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "\n",
    "    print(f\"Done! Results saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_taxonomy_robust(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828f4ec",
   "metadata": {},
   "source": [
    "# **1. Tournament Dataset Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7664dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ca613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PETase Tournament FASTA — QC & Stats Notebook Template\n",
    "# ============================================================\n",
    "# Assumptions:\n",
    "# - Input: wild-type PETase FASTA (600k+ seqs)\n",
    "# - Goal: sanity checks, redundancy, motifs, readiness metrics\n",
    "# - Modular: each cell runnable independently\n",
    "# ============================================================\n",
    "\n",
    "# -------------------------\n",
    "# 0) CONFIG\n",
    "# -------------------------\n",
    "FASTA = \"tournament_wt.fasta\"\n",
    "OUTDIR = \"qc_outputs\"\n",
    "REF_FASTA = \"reference_petase.fasta\"  # optional (e.g., IsPETase)\n",
    "MIN_LEN, MAX_LEN = 280, 320\n",
    "\n",
    "import os, sys, subprocess, json, math\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 1) LOAD FASTA + BASIC COUNTS\n",
    "# -------------------------\n",
    "records = list(SeqIO.parse(FASTA, \"fasta\"))\n",
    "N_total = len(records)\n",
    "\n",
    "seqs = [str(r.seq) for r in records]\n",
    "lengths = np.array([len(s) for s in seqs])\n",
    "\n",
    "basic_stats = {\n",
    "    \"total_sequences\": N_total,\n",
    "    \"unique_sequences\": len(set(seqs)),\n",
    "    \"min_len\": int(lengths.min()),\n",
    "    \"max_len\": int(lengths.max()),\n",
    "    \"mean_len\": float(lengths.mean()),\n",
    "    \"median_len\": float(np.median(lengths)),\n",
    "    \"IQR_len\": float(np.percentile(lengths, 75) - np.percentile(lengths, 25)),\n",
    "    \"pct_in_expected_window\": float(((lengths>=MIN_LEN)&(lengths<=MAX_LEN)).mean()*100),\n",
    "}\n",
    "basic_stats\n",
    "\n",
    "# -------------------------\n",
    "# 2) RESIDUE QUALITY\n",
    "# -------------------------\n",
    "ambiguous = set(\"XBZJUO\")\n",
    "def has_amb(s): return any(c in ambiguous for c in s)\n",
    "\n",
    "qc_residue = {\n",
    "    \"pct_with_ambiguous\": 100 * np.mean([has_amb(s) for s in seqs]),\n",
    "    \"pct_with_stop\": 100 * np.mean([\"*\" in s for s in seqs]),\n",
    "}\n",
    "qc_residue\n",
    "\n",
    "# -------------------------\n",
    "# 3) REDUNDANCY (CD-HIT)\n",
    "# -------------------------\n",
    "# Requires: cd-hit in PATH\n",
    "def run(cmd):\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "run(f\"cd-hit -i {FASTA} -o {OUTDIR}/cdhit_100.fasta -c 1.00 -n 5 -d 0\")\n",
    "run(f\"cd-hit -i {FASTA} -o {OUTDIR}/cdhit_95.fasta  -c 0.95 -n 5 -d 0\")\n",
    "run(f\"cd-hit -i {FASTA} -o {OUTDIR}/cdhit_90.fasta  -c 0.90 -n 5 -d 0\")\n",
    "\n",
    "def fasta_count(fp): return sum(1 for _ in SeqIO.parse(fp, \"fasta\"))\n",
    "\n",
    "redundancy = {\n",
    "    \"clusters_100\": fasta_count(f\"{OUTDIR}/cdhit_100.fasta\"),\n",
    "    \"clusters_95\":  fasta_count(f\"{OUTDIR}/cdhit_95.fasta\"),\n",
    "    \"clusters_90\":  fasta_count(f\"{OUTDIR}/cdhit_90.fasta\"),\n",
    "}\n",
    "redundancy\n",
    "\n",
    "# -------------------------\n",
    "# 4) MOTIF / SITE SANITY (PETase)\n",
    "# -------------------------\n",
    "# crude motif windows; adjust numbering if aligned\n",
    "def motif_present(seq, motif): return motif in seq\n",
    "\n",
    "motifs = {\n",
    "    \"has_GXSXG_like\": lambda s: any(s[i]==\"G\" and s[i+2]==\"S\" for i in range(len(s)-2)),\n",
    "    \"has_W185_like\":  lambda s: \"W\" in s,  # proxy; replace with alignment-based check\n",
    "    \"has_Cys_pair\":  lambda s: s.count(\"C\") >= 2,\n",
    "}\n",
    "\n",
    "motif_stats = {k: 100*np.mean([fn(s) for s in seqs]) for k,fn in motifs.items()}\n",
    "motif_stats\n",
    "\n",
    "# -------------------------\n",
    "# 5) SIGNAL PEPTIDE / TM (external tools)\n",
    "# -------------------------\n",
    "# Optional; placeholders for batch runs\n",
    "# signalp, tmhmm typically run outside notebook\n",
    "# Save FASTA subsets if needed\n",
    "with open(f\"{OUTDIR}/all.fasta\",\"w\") as fh:\n",
    "    SeqIO.write(records, fh, \"fasta\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) DOMAIN CHECK (InterProScan)\n",
    "# -------------------------\n",
    "# Requires interproscan.sh\n",
    "# Example (run externally for scale):\n",
    "# interproscan.sh -i all.fasta -f tsv -dp -o interpro.tsv\n",
    "\n",
    "# -------------------------\n",
    "# 7) STRUCTURAL PROXIES (cheap)\n",
    "# -------------------------\n",
    "# disorder proxy: fraction of low-complexity residues\n",
    "low_complex = set(\"PASTEG\")\n",
    "def lc_frac(s): return sum(c in low_complex for c in s)/len(s)\n",
    "\n",
    "struct_proxy = {\n",
    "    \"mean_low_complex_frac\": float(np.mean([lc_frac(s) for s in seqs])),\n",
    "}\n",
    "struct_proxy\n",
    "\n",
    "# -------------------------\n",
    "# 8) REFERENCE IDENTITY (optional)\n",
    "# -------------------------\n",
    "# blastp vs reference; DIAMOND recommended\n",
    "# diamond makedb --in reference_petase.fasta -d ref_db\n",
    "# diamond blastp -q tournament_wt.fasta -d ref_db -o ref_hits.tsv -f 6 qseqid pident length\n",
    "\n",
    "# -------------------------\n",
    "# 9) DATASET READINESS SUMMARY\n",
    "# -------------------------\n",
    "summary = {}\n",
    "summary.update(basic_stats)\n",
    "summary.update(qc_residue)\n",
    "summary.update(redundancy)\n",
    "summary.update(motif_stats)\n",
    "summary.update(struct_proxy)\n",
    "\n",
    "pd.Series(summary).to_csv(f\"{OUTDIR}/summary_metrics.csv\")\n",
    "summary\n",
    "\n",
    "# -------------------------\n",
    "# 10) FINAL NOTES\n",
    "# -------------------------\n",
    "# - Enforce filters (length, ambiguity, motifs) to get trainable set\n",
    "# - Recompute stats post-filter\n",
    "# - Freeze deduplicated FASTA for benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3b47a",
   "metadata": {},
   "source": [
    "# **2. Loading external stability and expression datasets**\n",
    "\n",
    "**NESG Solubility** \n",
    "(https://loschmidt.chemi.muni.cz/soluprot/?page=download)\n",
    "* 10k proteins\n",
    "* Labels: exp, sol, uniprot id or local ID \n",
    "* Units: integer \n",
    "\n",
    "**Soluprot Solubility**\n",
    "(https://loschmidt.chemi.muni.cz/soluprot/?page=download)\n",
    "* 11k training, 3k test\n",
    "* Label: solubility, number IDs with no conversion map (has seq)\n",
    "* Unit: 0/1\n",
    "\n",
    "**Price Solubility**\n",
    "(https://pmc.ncbi.nlm.nih.gov/articles/PMC3372292/)\n",
    "* 7k proteins \n",
    "* Label: usability. uniprot id\n",
    "* Unit: 0/1\n",
    "\n",
    "**PSI Solubility** \n",
    "(https://academic.oup.com/bioinformatics/article/36/18/4691/5860015?login=false)\n",
    "* 11k proteins\n",
    "* Label: solubility, Aa0000 ID scheme (has seq)\n",
    "* Unit: 0/1\n",
    "* Note: ecoli with custom IDs, dropped for now \n",
    "\n",
    "**Meltome Stability** \n",
    "(https://meltomeatlas.proteomics.wzw.tum.de/master_meltomeatlasapp/)\n",
    "* 1M variants \n",
    "* Label: temperature, meltpoint, fold_change, uniprot id \n",
    "* Note: ecoli with custom IDs, dropped for now  \n",
    "\n",
    "**FireprotDB Stability** \n",
    "(https://loschmidt.chemi.muni.cz/fireprotdb/)\n",
    "* 53k variants\n",
    "* Label: ddG, dTm, pH, Tm, mutation_effect, uniprot id \n",
    "\n",
    "**ThermomutDB Stability**\n",
    "(https://biosig.lab.uq.edu.au/thermomutdb/downloads)\n",
    "* 12k variants\n",
    "* Label: pH, ddG, temperature, dTm, uniprot/pdb id \n",
    "* Note: these genes were not retrieved from the database due to removal from uniprotkb:  A0A410ZNC6 D0WVP7 G7LSK3 GQ884175 M5A5Y8 Q9REI6\n",
    "\n",
    "**CAFA** \n",
    "(https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/code)\n",
    "* 142k variants\n",
    "\n",
    "**Novozyme**\n",
    "(https://www.kaggle.com/code/jinyuansun/eda-and-finetune-esm)\n",
    "* 31k variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9baf06",
   "metadata": {},
   "source": [
    "**Protsol Solubility**\n",
    "(https://huggingface.co/datasets/AI4Protein/ProtSolM)\n",
    "* 71k proteins\n",
    "* Label: solubility, no ID but has sequence\n",
    "* Unit: 0/1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now load \"masterdb.csv\" found under data\n",
    "import pandas as pd \n",
    "import os \n",
    "path = \"data/masterdb.tsv\"\n",
    "df = pd.read_csv(path,sep=\"\\t\")\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"(\", \"\")\n",
    "    .str.replace(\")\", \"\")\n",
    "    .str.replace(\"?\", \"\")\n",
    ")\n",
    "df2 = pd.DataFrame()\n",
    "df2[\"id\"]=df[\"name\"]\n",
    "df2[\"sequence\"] = df[\"protein_sequence\"].astype(str)\n",
    "print(df2)\n",
    "\n",
    "def dftofasta(df,outfile):\n",
    "    with open(outfile,\"w\") as f:\n",
    "        for index,row in df.iterrows():\n",
    "            f.write(f\">{row['id']}\\n\")\n",
    "            f.write(f\"{row['sequence']}\\n\")\n",
    "    return outfile \n",
    "\n",
    "#dftofasta(df2,\"data/masterdb.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cb14bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Setting up functions \n",
    "############################################\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "\n",
    "def read_fasta_dict(path: str):\n",
    "    seqs = {}\n",
    "    with open(path) as fh:\n",
    "        for header, seq in SimpleFastaParser(fh):\n",
    "            sid = header.split()[0].strip()\n",
    "            seqs[sid] = seq.strip()\n",
    "    return seqs\n",
    "\n",
    "def load_nesg(csv_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)  # uses CSV header row directly: id, exp, sol\n",
    "    seqs = read_fasta_dict(fasta_path)\n",
    "    #\"sid\" \"usability\" \"fasta\" \n",
    "    df[\"sequence\"] = df[\"id\"].map(seqs)\n",
    "    return df \n",
    "\n",
    "def load_psi(csv_path: str, psi_detail_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)                        # has sid + fasta + labels\n",
    "    psi_all = pd.read_csv(psi_detail_path, sep=\"\\t\")  # extra metadata\n",
    "\n",
    "    # merge on sid\n",
    "    df = df.merge(psi_all, on=\"sid\", how=\"left\")\n",
    "\n",
    "    # sequence is already in the \"fasta\" column\n",
    "    df[\"sequence\"] = df[\"fasta\"]\n",
    "\n",
    "    return df\n",
    "def load_price(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sequence\"] = df[\"fasta\"]\n",
    "    # \"sid\" \"usability\" \"fasta\" \n",
    "    return df\n",
    "\n",
    "def load_soluprot(train_csv: str, test_csv: str,\n",
    "                  train_fasta_path: str, test_fasta_path: str) -> pd.DataFrame:\n",
    "\n",
    "    # load FASTA → dict, keys exactly as in FASTA headers\n",
    "    train_fasta = read_fasta_dict(train_fasta_path)\n",
    "    test_fasta  = read_fasta_dict(test_fasta_path)\n",
    "\n",
    "    # merge FASTA dicts\n",
    "    fasta = {**train_fasta, **test_fasta}\n",
    "\n",
    "    # load CSVs\n",
    "    df1 = pd.read_csv(train_csv)\n",
    "    df2 = pd.read_csv(test_csv)\n",
    "\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # map sequences using the exact ids\n",
    "    df[\"sequence\"] = df[\"sid\"].map(fasta)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_meltome(csv_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # load fasta into dict: {uniprot_id: sequence}\n",
    "    fasta = read_fasta_dict(fasta_path)\n",
    "\n",
    "    # extract uniprot prefix from Protein_ID (before \"_\")\n",
    "    df[\"uniprot_id\"] = df[\"Protein_ID\"].astype(str).apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "    # map sequences\n",
    "    df[\"sequence\"] = df[\"uniprot_id\"].map(fasta)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_fireprot(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # \"uniprot_id\" \"pdb_id\" \"muutation\" \"ddG\" \"dTm\" \"pH\" \"tm\" \"mutation_effect\" \"sequence\"\n",
    "    return df \n",
    "\n",
    "def load_thermomut(json_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    # load JSON metadata\n",
    "    with open(json_path) as fh:\n",
    "        data = json.load(fh)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # load FASTA sequences\n",
    "    fasta_dict = read_fasta_dict(fasta_path)\n",
    "\n",
    "    # map UniProt → sequence\n",
    "    # JSON column is \"uniprot\"\n",
    "    df[\"sequence\"] = df[\"uniprot\"].map(fasta_dict)\n",
    "\n",
    "    # ensure required labels exist even if missing in JSON\n",
    "    for col in [\"ph\",\"ddg\",\"temperature\",\"dtm\",\"PDB_wild\",\n",
    "                \"pdb_mutant\",\"mutation_code\",\"mutated chain\",\"effect\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_protsolm(train_csv: str, test_csv: str) -> pd.DataFrame:\n",
    "    df1 = pd.read_csv(train_csv)\n",
    "    df2 = pd.read_csv(test_csv)\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    # \"aa_seq\" \"detail\"\n",
    "    return df\n",
    "\n",
    "def fasta_merger(fasta_paths: list, outpath: str):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for path in fasta_paths:\n",
    "            with open(path) as fh:\n",
    "                for line in fh:\n",
    "                    out.write(line)\n",
    "\n",
    "def parse_cd_hit_clusters(clstr_path):\n",
    "    clusters = {}\n",
    "    current = None\n",
    "\n",
    "    with open(clstr_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">Cluster\"):\n",
    "                current = line.split()[1]\n",
    "                clusters[current] = []\n",
    "            else:\n",
    "                # Example: \"0       50aa, >SEQ123... *\"\n",
    "                sid = line.split(\">\")[1].split(\"...\")[0]\n",
    "                clusters[current].append(sid)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def fasta_merger_from_dfs(datasets, outpath):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for i, df in enumerate(datasets):\n",
    "            for idx, row in df.iterrows():\n",
    "                seq = row[\"sequence\"]\n",
    "                if seq is None or pd.isna(seq):\n",
    "                    continue\n",
    "                header = f\"{df.__class__.__name__}_{i}_{idx}\"\n",
    "                out.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "def fetch_uniprot_fasta(ids, out_fasta, delay=0.15):\n",
    "    \"\"\"Fetch FASTA for many UniProt IDs with error handling.\"\"\"\n",
    "    with open(out_fasta, \"w\") as out:\n",
    "        for uid in ids:\n",
    "            url = f\"https://rest.uniprot.org/uniprotkb/{uid}.fasta\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "\n",
    "            if r.status_code == 200 and r.text.startswith(\">\"):\n",
    "                out.write(r.text.strip() + \"\\n\")\n",
    "            else:\n",
    "                # write placeholder for failed fetch\n",
    "                out.write(f\">{uid}\\nFAILED_FETCH\\n\")\n",
    "\n",
    "            time.sleep(delay)  # rate-limit to avoid 500 errors\n",
    "\n",
    "\n",
    "\n",
    "def load_novozymes(train_path: str, test_path: str, test_labels_path: str) -> pd.DataFrame:\n",
    "    train_df = pd.read_csv(train_path)         # seq_id, protein_sequence, pH, Tm\n",
    "    test_df = pd.read_csv(test_path)           # seq_id, protein_sequence, pH\n",
    "    test_labels = pd.read_csv(test_labels_path)  # seq_id, Tm\n",
    "\n",
    "    # merge test with its labels\n",
    "    test_df = test_df.merge(test_labels, on=\"seq_id\", how=\"left\")\n",
    "\n",
    "    # unify column names to match your other datasets\n",
    "    train_df[\"sequence\"] = train_df[\"protein_sequence\"]\n",
    "    test_df[\"sequence\"]  = test_df[\"protein_sequence\"]\n",
    "\n",
    "    # combine train + test into one dataframe\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06426298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y8/spmd84w16l78w44w6_yz5h340000gn/T/ipykernel_28883/3962239254.py:81: DtypeWarning: Columns (23,24,25,26,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#setting the paths, loading them into dataframes, and making the merged fasta file to cdhit \n",
    "############################################\n",
    "\n",
    "\n",
    "#PSI_PATH      = \"data/benchmark/sol_benchmark/PSI_Biology_solubility_trainset.csv\"\n",
    "#psi_detail_path = \"data/benchmark/sol_benchmark/PSI_all_data_esol.tab\"\n",
    "NESG_PATH     = \"data/benchmark/sol_benchmark/nesg/nesg.csv\"\n",
    "nesg_fasta_path = \"data/benchmark/sol_benchmark/nesg/nesg.fasta\"\n",
    "PRICE_PATH    = \"data/benchmark/sol_benchmark/Price_usability_trainset.csv\"\n",
    "soluprot_train_path = \"data/benchmark/sol_benchmark/soluprot_data/training_set.csv\"\n",
    "soluprot_test_path = \"data/benchmark/sol_benchmark/soluprot_data/test_set.csv\" \n",
    "soluprot_train_fasta = \"data/benchmark/sol_benchmark/soluprot_data/training_set.fasta\"\n",
    "soluprot_test_fasta = \"data/benchmark/sol_benchmark/soluprot_data/test_set.fasta\" \n",
    "#meltome_path = \"data/benchmark/stab_benchmark/meltome_cross-species.csv\"\n",
    "#meltome_fasta_path = \"data/benchmark/stab_benchmark/meltome_fasta.fasta\"\n",
    "fireprot_path = \"data/benchmark/stab_benchmark/fireprotdb_results_stability.csv\"\n",
    "thermomutdb_path = \"data/benchmark/stab_benchmark/thermomutdb.json\"\n",
    "thermomutdb_fasta = \"data/benchmark/stab_benchmark/thermomutdb.fasta\"\n",
    "protsol_train_path = \"data/benchmark/protsolm_data/protsolm_train.csv\"\n",
    "protsol_test_path = \"data/benchmark/protsolm_data/protsolm_test.csv\"\n",
    "novozyme_test_path = \"data/benchmark/novozymes-enzyme-stability-prediction/test.csv\"\n",
    "novozyme_train_path = \"data/benchmark/novozymes-enzyme-stability-prediction/train.csv\"\n",
    "novozyme_test_labels_path = \"data/benchmark/novozymes-enzyme-stability-prediction/test_labels.csv\"\n",
    "\n",
    "\n",
    "#LOAD DATASETS \n",
    "#psi = load_psi(\"data/benchmark/sol_benchmark/PSI_Biology_solubility_trainset.csv\",\"data/benchmark/sol_benchmark/PSI_all_data_esol.tab\")\n",
    "nesg = load_nesg(NESG_PATH, nesg_fasta_path) #no seq col \n",
    "price = load_price(PRICE_PATH) #fasta \n",
    "soluprot = load_soluprot(\n",
    "    soluprot_train_path,\n",
    "    soluprot_test_path,\n",
    "    soluprot_train_fasta,\n",
    "    soluprot_test_fasta    \n",
    ")\n",
    "fireprot = load_fireprot(fireprot_path) #sequence\n",
    "thermomut = load_thermomut(thermomutdb_path,thermomutdb_fasta)\n",
    "#meltome = load_meltome(meltome_path,meltome_fasta_path) \n",
    "protsolm = load_protsolm(protsol_train_path, protsol_test_path) #aa_seq\n",
    "novozymes = load_novozymes(\n",
    "    novozyme_train_path, novozyme_test_path, novozyme_test_labels_path\n",
    ")\n",
    "\n",
    "#ADD FASTA TO DATAFRAMES IF THE CSV DID NOT HAVE IT\n",
    "nesg_fasta = read_fasta_dict(nesg_fasta_path)\n",
    "nesg[\"sequence\"] = nesg[\"id\"].map(nesg_fasta)\n",
    "price[\"sequence\"] = price[\"fasta\"]\n",
    "protsolm[\"sequence\"] = protsolm[\"aa_seq\"]\n",
    "\n",
    "\n",
    "#MERGE ALL FASTA FILES FOR CD-HIT\n",
    "def fasta_merger_from_dfs(datasets, outpath):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for i, df in enumerate(datasets):\n",
    "            for idx, row in df.iterrows():\n",
    "                seq = row[\"sequence\"]\n",
    "                if seq is None or pd.isna(seq):\n",
    "                    continue\n",
    "                header = f\"{df.__class__.__name__}_{i}_{idx}\"\n",
    "                out.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "datasets = [nesg, price, soluprot, fireprot, protsolm,thermomut, novozymes]\n",
    "fasta_merger_from_dfs(datasets, \"allbenchmarks.fasta\")\n",
    "\n",
    "#SETTING  CANONICAL IDS\n",
    "for i, df in enumerate(datasets):\n",
    "    df[\"canonical_id\"] = [\n",
    "        f\"{df.__class__.__name__}_{i}_{idx}\" for idx in df.index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8943dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96118, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>canonical_ids</th>\n",
       "      <th>representative</th>\n",
       "      <th>merged_block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[DataFrame_3_25912, DataFrame_3_27807, DataFra...</td>\n",
       "      <td>DataFrame_3_25912</td>\n",
       "      <td>experiment_id protein_name uniprot_id pdb_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[DataFrame_6_28079]</td>\n",
       "      <td>DataFrame_6_28079</td>\n",
       "      <td>seq_id                                   pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[DataFrame_6_28080]</td>\n",
       "      <td>DataFrame_6_28080</td>\n",
       "      <td>seq_id                                   pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[DataFrame_6_28081]</td>\n",
       "      <td>DataFrame_6_28081</td>\n",
       "      <td>seq_id                                   pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[DataFrame_6_28082]</td>\n",
       "      <td>DataFrame_6_28082</td>\n",
       "      <td>seq_id                                   pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster_id                                      canonical_ids  \\\n",
       "0          0  [DataFrame_3_25912, DataFrame_3_27807, DataFra...   \n",
       "1          1                                [DataFrame_6_28079]   \n",
       "2          2                                [DataFrame_6_28080]   \n",
       "3          3                                [DataFrame_6_28081]   \n",
       "4          4                                [DataFrame_6_28082]   \n",
       "\n",
       "      representative                                       merged_block  \n",
       "0  DataFrame_3_25912     experiment_id protein_name uniprot_id pdb_i...  \n",
       "1  DataFrame_6_28079     seq_id                                   pr...  \n",
       "2  DataFrame_6_28080     seq_id                                   pr...  \n",
       "3  DataFrame_6_28081     seq_id                                   pr...  \n",
       "4  DataFrame_6_28082     seq_id                                   pr...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################\n",
    "#Running CD-HIT, parsing output and merging the final merged_df\n",
    "############################################\n",
    "\n",
    "#cd-hit -i all_sequences.fasta -o all_sequences_100.fasta -c 1.0 -n 5 -d 0\n",
    "#PARSING THE CLUSTERS FROM CD-HIT INTO ONE MERGED DATAFRAME \n",
    "datasets = {\n",
    "    \"nesg\": nesg,\n",
    "    \"price\": price,\n",
    "    \"soluprot\": soluprot,\n",
    "    \"fireprot\": fireprot,\n",
    "    \"protsolm\": protsolm,\n",
    "    \"thermomutdb\":thermomut,\n",
    "    \"novozymes\": novozymes\n",
    "}\n",
    "\n",
    "# Ensure every df has canonical_id as previously assigned\n",
    "for name, df in datasets.items():\n",
    "    if \"canonical_id\" not in df.columns:\n",
    "        raise ValueError(f\"{name} missing canonical_id\")\n",
    "\n",
    "\n",
    "clusters = parse_cd_hit_clusters(\"data/benchmark/benchmark_cdhit100_cluster.txt\")\n",
    "merged_rows = []\n",
    "for clust_id, members in clusters.items():\n",
    "    rep = members[0]\n",
    "    collected = []\n",
    "    for name, df in datasets.items():\n",
    "        sub = df[df[\"canonical_id\"].isin(members)]\n",
    "        if len(sub) > 0:\n",
    "            sub = sub.copy()\n",
    "            sub[\"source_dataset\"] = name\n",
    "            collected.append(sub)\n",
    "\n",
    "    if collected:\n",
    "        merged_block = pd.concat(collected, ignore_index=True)\n",
    "    else:\n",
    "        merged_block = pd.DataFrame()\n",
    "\n",
    "    merged_rows.append({\n",
    "        \"cluster_id\": clust_id,\n",
    "        \"canonical_ids\": members,\n",
    "        \"representative\": rep,\n",
    "        \"merged_block\": merged_block\n",
    "    })\n",
    "\n",
    "# this is your final merged output\n",
    "merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "print(merged_df.shape)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8d24902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cluster_id                                      canonical_ids  \\\n",
      "0              0  [DataFrame_3_25912, DataFrame_3_27807, DataFra...   \n",
      "1              1                                [DataFrame_6_28079]   \n",
      "2              2                                [DataFrame_6_28080]   \n",
      "3              3                                [DataFrame_6_28081]   \n",
      "4              4                                [DataFrame_6_28082]   \n",
      "...          ...                                                ...   \n",
      "96113      96113                                 [DataFrame_6_2392]   \n",
      "96114      96114                                [DataFrame_6_31072]   \n",
      "96115      96115                                 [DataFrame_6_1497]   \n",
      "96116      96116                                [DataFrame_6_30387]   \n",
      "96117      96117                                 [DataFrame_6_2806]   \n",
      "\n",
      "          representative                                       merged_block  \n",
      "0      DataFrame_3_25912     experiment_id protein_name uniprot_id pdb_i...  \n",
      "1      DataFrame_6_28079     seq_id                                   pr...  \n",
      "2      DataFrame_6_28080     seq_id                                   pr...  \n",
      "3      DataFrame_6_28081     seq_id                                   pr...  \n",
      "4      DataFrame_6_28082     seq_id                                   pr...  \n",
      "...                  ...                                                ...  \n",
      "96113   DataFrame_6_2392     seq_id     protein_sequence   pH           ...  \n",
      "96114  DataFrame_6_31072     seq_id   protein_sequence   pH             ...  \n",
      "96115   DataFrame_6_1497     seq_id  protein_sequence   pH              ...  \n",
      "96116  DataFrame_6_30387     seq_id protein_sequence   pH               ...  \n",
      "96117   DataFrame_6_2806     seq_id protein_sequence   pH               ...  \n",
      "\n",
      "[96118 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c8d640ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_COLS = {\n",
    "    \"fireprot\": [\n",
    "        \"uniprot_id\",\"pdb_id\",\"chain\",\"wild_type\",\"position\",\"mutation\",\n",
    "        \"pH\",\"tm\",\"dTm\",\"ddG\",\"interpro_families\",\"is_essential\",\"sequence\"\n",
    "    ],\n",
    "    \"protsolm\": [\"name\",\"label\",\"detail\",\"sequence\"],\n",
    "    \"novozymes\": [\"seq_id\",\"pH\",\"tm\",\"protein_sequence\",\"sequence\"],\n",
    "    \"nesg\": [\"sid\",\"exp\",\"sol\",\"solubility\",\"fasta\",\"sequence\"],\n",
    "    \"price\": [\"sid\",\"Usability|0=NotUsable|1=Usable\",\"fasta\",\"sequence\"]\n",
    "}\n",
    "\n",
    "def clean_block(df):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    src = df[\"source_dataset\"].iloc[0]\n",
    "    if src not in KEEP_COLS:\n",
    "        return df\n",
    "    keep = KEEP_COLS[src]\n",
    "    # always preserve canonical_id + source_dataset\n",
    "    keep = [c for c in keep if c in df.columns] + [\"canonical_id\",\"source_dataset\"]\n",
    "    return df[keep]\n",
    "\n",
    "merged_df[\"merged_block\"] = merged_df[\"merged_block\"].apply(clean_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdadffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE:\n",
      " missing_pdb_uniprot_ids.txt\n",
      " has_pdb_ids.txt\n",
      " no_uniprot_no_pdb.fasta\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# Attempt to get structure of this benchmark database to make multimodal db but will need to filter the proteins\n",
    "############################################################\n",
    "\n",
    "missing_pdb_uniprot = set()\n",
    "has_pdb_ids = set()\n",
    "no_uniprot_no_pdb_seqs = set()\n",
    "\n",
    "for block in merged_df[\"merged_block\"]:\n",
    "    if not isinstance(block, pd.DataFrame) or block.empty:\n",
    "        continue\n",
    "\n",
    "    cols = block.columns\n",
    "\n",
    "    # 1. uniprot_id present but pdb_id missing\n",
    "    if \"uniprot_id\" in cols:\n",
    "        b = block[[\"uniprot_id\", \"pdb_id\"]].copy()\n",
    "        b = b[b[\"uniprot_id\"].notna()]  # entries with uniprot\n",
    "        b = b[b[\"pdb_id\"].isna()]       # missing pdb\n",
    "        missing_pdb_uniprot.update(b[\"uniprot_id\"].dropna().astype(str).tolist())\n",
    "\n",
    "    # 2. unique pdb ids\n",
    "    if \"pdb_id\" in cols:\n",
    "        ids = block[\"pdb_id\"].dropna().astype(str).tolist()\n",
    "        has_pdb_ids.update(ids)\n",
    "\n",
    "    # 3. no uniprot, no pdb → need sequences\n",
    "    if \"sequence\" in cols:\n",
    "        mask = pd.Series(True, index=block.index)\n",
    "        if \"uniprot_id\" in cols:\n",
    "            mask &= block[\"uniprot_id\"].isna()\n",
    "        if \"pdb_id\" in cols:\n",
    "            mask &= block[\"pdb_id\"].isna()\n",
    "        seqs = block.loc[mask, \"sequence\"].dropna().tolist()\n",
    "        for s in seqs:\n",
    "            no_uniprot_no_pdb_seqs.add(s)\n",
    "\n",
    "\n",
    "############################################################\n",
    "# WRITE OUTPUT FILES\n",
    "############################################################\n",
    "\n",
    "# 1. uniprot present but no pdb\n",
    "with open(\"missing_pdb_uniprot_ids.txt\", \"w\") as f:\n",
    "    for uid in sorted(missing_pdb_uniprot):\n",
    "        f.write(uid + \"\\n\")\n",
    "\n",
    "# 2. pdb ids\n",
    "with open(\"has_pdb_ids.txt\", \"w\") as f:\n",
    "    for pid in sorted(has_pdb_ids):\n",
    "        f.write(pid + \"\\n\")\n",
    "\n",
    "# 3. fasta for no uniprot AND no pdb\n",
    "with open(\"no_uniprot_no_pdb.fasta\", \"w\") as f:\n",
    "    for i, seq in enumerate(sorted(no_uniprot_no_pdb_seqs)):\n",
    "        f.write(f\">seq_{i}\\n{seq}\\n\")\n",
    "\n",
    "print(\"DONE:\")\n",
    "print(\" missing_pdb_uniprot_ids.txt\")\n",
    "print(\" has_pdb_ids.txt\")\n",
    "print(\" no_uniprot_no_pdb.fasta\")\n",
    "\n",
    "\n",
    "#Run mmseqs2 to seq2struct to find top hits, if not, use AF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8feb512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1FEP', '1UHG', '1AZP|1AZP|1AZP|1BF4', '1IMQ', '3VUB|3VUB', '2ADA', '1BNI|1A2P', '1AXB|1BTL|1XPB|1ZG4', '1YPI|1YPI', '1IDS|1IDS|1IDS|1IDS', '2TRX', '2IMM', '1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ|1ZNJ', '1CYC', '1CAH', '1DPM', '1B5M', '1UZC', '3MBP|1SVX|1SVX', '3PG0', '1RX4|1DDR|1DDR|1DYJ|5DFR', '1DKT|1DKT', '1HME', '1C52', '1P2P|1P2P', '1AAR|1AAR', '1PDO', '1JU3', '1LBI|1LBI|1LBI|1LBI', '1RIS|1RIS|1RIS|1RIS', '1C9O|1C9O', '3BLS|1KE4', '1YYX', '1BNL', '3HHR|1HGU', '1KCQ', '1QGD|1QGD', '1E21', '1LVE', '1BCX', '1TIN', '1ROP|1ROP', '1ZYM', '1ARR|1ARR', '1HNG|1HNG|1CDC|1CDC', '1I4N', '1RTP', '1TPK', '1IR3|1IR3|1IR3|1IR3', '1TTG|1FNA|1FNF', '1APS', '1BP2|1G4I', '3D2A|1ISP|3D2C', '1CF3', '1HK0', '1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON|1AON', '1WIT', '1M21|1M21', '1CYO', '8TIM', '451C', '1KFW', '2Q98', '4ZLU|4ZLU', '2ABD', '3PGK', '3WP4', '1LUC', '1DIL|3SIL', '2CBR', '1OLR', '1BAH', '1HZ6', '1AM7|1AM7|1AM7', '1K9Q|1K9Q', '1POH', '2TS1|2TS1', '1AYF|1AYF', '1IFB|1IFC|2IFB', '1GUY|1GUY|1GUY|1GUY', '1WQ5', '1RHG', '1EL1', '1URK', '1IHB', '2CHF', '1KEV|1KEV|1KEV|1KEV', '1AQH', '1DEC', '1TPE|1TPE', '1ANK', '5CRO|5CRO', '5AZU|5AZU|5AZU|5AZU', '1BYW', '6BQG', '1CEY', '2WSY|2WSY|2WSY|2WSY|1TTQ|1BKS', '1SCE|1SCE|1SCE|1SCE', '3TGL', '1YEA', '1FRD', '2CI2|2CI2|2CI2|2CI2|2CI2|2CI2|1YPC', '1IGV', '1BTA', '1SHF|1SHF|1SHF|1SHF', '1BTM|1BTM', '1QQV|1YU5', '1DPM|1DPM', '1IET', '1TDJ|1TDJ', '1EVQ', '1QGV', '2DRI', '1STN|1EY0', '1FTG|1FLV|1QHE', '1VQB|1VQB', '1PX0|1PX0|1PX0|1PX0', '1MJ5', '1BLC|1BLC', '2A36', '1MJC', '1YCC|1YCC', '1RTB|1KF2|1KF3|1KF5', '1B26|1B26|1B26|1B26|1B26|1B26', '1TUX', '1XAS|1E0W', '2UXY|2UXY|2UXY|2UXY|2UXY|2UXY', '4BLM', '1RBP', '1TUP|1TUP|1TUP|1TUP|1TUP|1SAK|1SAK|1SAK|1SAK|2OCJ', '1A43', '1C8C', '1RGG|1RGG|1SAR|1LNI', '1SSO', '1HFZ|1F6R', '1UWO|1UWO|2H61', '1FXA', '1PGA|1EM7|2GB1', '5ZYR|5ZYR', '1ONC', '1AV1|1AV1|1AV1|1AV1', '1RN1', '1DIV|1DIV|2HBB', '1W4E|1W3D', '1PIN|1PIN', '1IOB|2NVH', '6G4B|6G4B', '1RRO', '1CHK', '1SVX', '6TQ3|6TQ3|6TQ3|6TQ3', '1UBQ', '1AJ3|1SHG|1CUN', '1LS4|1AEP', '1HFY|1HFY|1HMK', '4LYZ', '1FC1|1FC1', '2RN2', '1THQ', '1ANT|1ANT', '1OH0|1OH0', '2O9P|2JIE', '1A23', '1BOY|1BOY', '3PG4|4EY2', '1JIW', '1H8V', '1B0O|1B0O|1B8E', '1HYN|1HYN', '1ADO|1ADO|1ADO|1ADO', '1TEN', '1DKG|1DKG|1DKG', '1MGR', '2AKY|1AKY', '1TCA', '1A5E', '1CTS|1CTS', '1FKJ|1FKJ|1FKB', '2HPR|2HID', '2TRT', '1ACB|1ACB|1CSE', '1CLW|1TYV', '1TIT', '1CQW', '1BPI|5PTI', '1OIA', '1BRF', '2ZTA|2ZTA', '1AG2', '1KA6|1KA6', '2HIP', '1JNX', '1C5G', '1JK9|1JK9', '1C2R', '2LZM|1L63', '6JHM|6JHM|6JHM|6JHM', '1BVC', '1G6N|1G6N', '1BFG|1FGA', '1CSP', '1H7M', '1QM4', '1HUE|1HUE', '1QLP', '1HTI|1HTI|2JK2|2JK2', '2BRD|2BRD|2BRD', '1SUP', '1LRP|1LMB', '1AKK|1I5T', '1MSI', '1N0J|1N0J|1N0J|1N0J', '2AFG|1RG8', '1AMQ|1AMQ', '2CRK|2CRK', '1QJP', '1YNR|1YNR|1YNR|1YNR', '1QND', '1MBG|1GV5', '1H0C|1H0C', '1AYE', '2HMB|1HMS', '1FMK', '2CPP', '1IRO|1IRO|1IRO', '1BVU|1BVU|1BVU|1BVU|1BVU|1BVU', '1IO2', '3SSI|3SSI', '1OTR', '4E5K|4E5K', '1LZ1|1REX|2BQA|1LHM'}\n"
     ]
    }
   ],
   "source": [
    "all_pdb_ids = set()\n",
    "\n",
    "for block in merged_df[\"merged_block\"]:\n",
    "    if isinstance(block, pd.DataFrame) and \"pdb_id\" in block.columns:\n",
    "        all_pdb_ids.update(block[\"pdb_id\"].dropna().unique())\n",
    "print(all_pdb_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364efbba",
   "metadata": {},
   "source": [
    "# 4. Fine-tuning esm2/3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9068a2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncbidatasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
