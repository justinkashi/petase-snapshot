{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cb98f7",
   "metadata": {},
   "source": [
    "# Bionemo ESM2 fine-tuning \n",
    "(https://docs.nvidia.com/bionemo-framework/2.3/user-guide/examples/bionemo-esm2/finetune/)\n",
    "\n",
    "**Startup**\\\n",
    "`docker login nvcr.io`\\\n",
    "`username = $oauthtoken`\\\n",
    "`pw = api key` \\\n",
    "`docker pull nvcr.io/nvidia/clara/bionemo-framework:2.3`\n",
    "\n",
    "**Starting a Shell Inside the Container**\\\n",
    "`docker run` \\\n",
    "  `--rm -it` \\\n",
    "  `--gpus all` \\\n",
    "  `--network host` \\\n",
    "  `--shm-size=4g` \\\n",
    "  `-e WANDB_API_KEY` \\\n",
    "  `-e NGC_CLI_API_KEY` \\\n",
    "  `-e NGC_CLI_ORG` \\\n",
    "  `-e NGC_CLI_TEAM` \\\n",
    "  `-e NGC_CLI_FORMAT_TYPE` \\\n",
    "  `-v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH` \\\n",
    "  `-v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH` \\\n",
    "  `-v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH `\\\n",
    "  `nvcr.io/nvidia/clara/bionemo-framework:2.3` \\\n",
    "\n",
    "\n",
    "**Running a Model Training Script Inside the Container**\\\n",
    "`docker run --rm -it --gpus all` \\\n",
    "  `-e NGC_CLI_API_KEY` \\\n",
    "  `-e WANDB_API_KEY` \\\n",
    "  `-v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH` \\\n",
    "  `-v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH` \\\n",
    " ` -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH` \\\n",
    " ` nvcr.io/nvidia/clara/bionemo-framework:2.3` \\\n",
    "`  python $DOCKER_RESULTS_PATH/training.py --option1 --option2 --output=$DOCKER_RESULTS_PATH`\n",
    "  \n",
    "**running jupyter lab inside the container** \n",
    "\n",
    "`docker run --rm -d --gpus all` \\\n",
    "`  -p $JUPYTER_PORT:$JUPYTER_PORT `\\\n",
    "`  -e NGC_CLI_API_KEY` \\\n",
    "`  -e WANDB_API_KEY` \\\n",
    "`  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH` \\\n",
    "  `-v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH` \\\n",
    "`  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH` \\\n",
    "  `nvcr.io/nvidia/clara/bionemo-framework:2.3` \\\n",
    "`  jupyter lab` \\\n",
    "    `--allow-root` \\\n",
    " `   --ip=* `\\\n",
    "`    --port=$JUPYTER_PORT` \\\n",
    "   ` --no-browser` \\\n",
    "   ` --NotebookApp.token=''` \\\n",
    "    `--NotebookApp.allow_origin='*' `\\\n",
    "    `--ContentsManager.allow_hidden=True` \\\n",
    "    `--notebook-dir=$DOCKER_RESULTS_PATH`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6dbaf",
   "metadata": {},
   "source": [
    "# BIONEMO ESM2 Fine Tuning classes \n",
    "\n",
    "(https://docs.nvidia.com/bionemo-framework/latest/main/examples/bionemo-esm2/finetune/)\n",
    "\n",
    "1. **Loss Reduction Class** \n",
    "2. **Fine-Tuned Model Head**\n",
    "3. **Fine-tuned model** \n",
    "4. ***Fine-tuning config**\n",
    "5. **Dataset**\n",
    "\n",
    "**Sequence-level regression** \n",
    "\n",
    "`%%capture --no-display cell_output`\n",
    "\n",
    "`finetune_esm2` \\\n",
    "    `--restore-from-checkpoint-path {pretrain_checkpoint_path}` \\\n",
    "    `--train-data-path {regression_data_path}` \\\n",
    "    `--valid-data-path {regression_data_path}` \\\n",
    "    `--config-class ESM2FineTuneSeqConfig` \\\n",
    "   ` --dataset-class InMemorySingleValueDataset` \\\n",
    "  `  --task-type \"regression\"` \\\n",
    "  `  --mlp-ft-dropout 0.25` \\\n",
    "  `  --mlp-hidden-size 256 `\\\n",
    "   ` --mlp-target-size 1 `\\\n",
    "    `--experiment-name \"sequence-level-regression\"` \\\n",
    "   ` --num-steps 50` \\\n",
    "   ` --num-gpus 1 `\\\n",
    "    `--limit-val-batches 10 `\\\n",
    " `   --val-check-interval 10` \\\n",
    "   ` --log-every-n-steps 10` \\\n",
    "   ` --encoder-frozen` \\\n",
    "  `  --lr 1e-5` \\\n",
    "    `--lr-multiplier 1e2` \\\n",
    "    `--scale-lr-layer \"regression_head\"` \\\n",
    "  `  --result-dir {work_dir}`  \\\n",
    " `   --micro-batch-size 4 `\\\n",
    " `   --label-column \"labels\"` \\\n",
    "`    --num-gpus 1` \\\n",
    "   ` --precision \"bf16-mixed\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ad97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorLossReduction(BERTMLMLossWithReduction):\n",
    "    def forward(\n",
    "        self, batch: Dict[str, torch.Tensor], forward_out: Dict[str, torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "\n",
    "        regression_output = forward_out[\"regression_output\"]\n",
    "        targets = batch[\"labels\"].to(dtype=regression_output.dtype)  # [b, 1]\n",
    "        num_valid_tokens = torch.tensor(targets.numel(), dtype=torch.int, device=targets.device)\n",
    "        loss_sum = ((regression_output - targets) ** 2).sum()\n",
    "        loss_sum_and_ub_size = torch.cat([loss_sum.clone().detach().view(1), num_valid_tokens.view(1)])\n",
    "        return loss_sum, num_valid_tokens, {\"loss_sum_and_ub_size\": loss_sum_and_ub_size}\n",
    "\n",
    "class MegatronMLPHead(MegatronModule):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__(config)\n",
    "        layer_sizes = [config.hidden_size, config.mlp_hidden_size, config.mlp_target_size]\n",
    "        self.linear_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        )\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=config.ft_dropout)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> List[torch.Tensor]:\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            hidden_states = self.dropout(self.act(layer(hidden_states)))\n",
    "\n",
    "        output = self.linear_layers[-1](hidden_states)\n",
    "        return output\n",
    "\n",
    "class ESM2FineTuneSeqModel(ESM2Model):\n",
    "    def __init__(self, config, *args, post_process: bool = True, include_embeddings: bool = False, **kwargs):\n",
    "        super().__init__(config, *args, post_process=post_process, include_embeddings=True, **kwargs)\n",
    "\n",
    "        # freeze encoder parameters\n",
    "        if config.encoder_frozen:\n",
    "            for _, param in self.named_parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if post_process:\n",
    "            self.regression_head = MegatronMLPHead(config)\n",
    "\n",
    "    def forward(self, *args, **kwargs,):\n",
    "        output = super().forward(*args, **kwargs)\n",
    "        ...\n",
    "        output[\"regression_output\"] = self.regression_head(output[\"embeddings\"])\n",
    "        return output\n",
    "\n",
    "@dataclass\n",
    "class ESM2FineTuneSeqConfig(\n",
    "    ESM2GenericConfig[ESM2FineTuneSeqModel, RegressorLossReduction], iom.IOMixinWithGettersSetters\n",
    "):\n",
    "    model_cls: Type[ESM2FineTuneSeqModel] = ESM2FineTuneSeqModel\n",
    "    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n",
    "    # that has this new head and want to keep using these weights, please drop this next line or set to []\n",
    "    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n",
    "    encoder_frozen: bool = True  # freeze encoder parameters\n",
    "    # MLP head layer parameters\n",
    "    mlp_ft_dropout: float = 0.25  \n",
    "    mlp_hidden_size: int = 256\n",
    "    mlp_target_size: int = 1\n",
    "\n",
    "    def get_loss_reduction_class(self) -> Type[BERTMLMLossWithReduction]:\n",
    "        return RegressorLossReduction\n",
    "\n",
    "class InMemorySingleValueDataset(InMemoryProteinDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequences: pd.Series,\n",
    "        labels: pd.Series,\n",
    "        task_type: str = \"regression\",\n",
    "        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n",
    "        seed: int = np.random.SeedSequence().entropy,\n",
    "    ):\n",
    "        super().__init__(sequences, labels, task_type, tokenizer, seed)\n",
    "\n",
    "    def transform_label(self, label: float | str) -> Tensor:\n",
    "        return torch.tensor([label], dtype=torch.float)\n",
    "\n",
    "\n",
    "dataset = InMemorySingleValueDataset.from_csv(data_path)\n",
    "data_module = ESM2FineTuneDataModule(train_dataset=dataset,\n",
    "    valid_dataset=dataset,\n",
    "    micro_batch_size=4,   # size of a batch to be processed in a device\n",
    "    global_batch_size=8,  # size of batch across all devices. Should be multiple of micro_batch_size\n",
    ")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "cleanup: bool = True\n",
    "work_dir = \"/workspace/bionemo2/esm2_finetune_tutorial\"\n",
    "\n",
    "if cleanup and os.path.exists(work_dir):\n",
    "    shutil.rmtree(work_dir)\n",
    "\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "    print(f\"Directory '{work_dir}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{work_dir}' already exists.\")\n",
    "\n",
    "from bionemo.core.data.load import load\n",
    "\n",
    "\n",
    "pretrain_checkpoint_path = load(\"esm2/8m:2.0\")\n",
    "print(pretrain_checkpoint_path)\n",
    "#num_steps * global_batch_size = len(dataset) * desired_num_epochs\n",
    "\n",
    "artificial_sequence_data = [\n",
    "    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n",
    "    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",\n",
    "    \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n",
    "    \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n",
    "    \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n",
    "    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n",
    "]\n",
    "\n",
    "regression_data = [(seq, len(seq) / 100.0) for seq in artificial_sequence_data]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(regression_data, columns=[\"sequences\", \"labels\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "regression_data_path = os.path.join(work_dir, \"regression_data.csv\")\n",
    "df.to_csv(regression_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1d8e3",
   "metadata": {},
   "source": [
    "# Naity's CAFA5 finetuned esm2\n",
    "\n",
    "(https://github.com/naity/finetune-esm)\n",
    "\n",
    "`git clone https://github.com/naity/finetune-esm.git`\\\n",
    "`python finetune-esm/train.py --help`\\\n",
    "`pip install -r requirements.txt`\\\n",
    "\n",
    "`python finetune esm/train.py` \\\n",
    "  `--experiment-name esm2_t6_8M_UR50D_lora `\\\n",
    " ` --dataset-loc data/cafa5/top100_train_split.parquet` \\\n",
    " ` --targets-loc data/cafa5/train_bp_top100_targets.npy` \\\n",
    "  `--esm-model esm2_t6_8M_UR50D` \\\n",
    "`  --num-workers 1` \\\n",
    " ` --num-devices 1` \\\n",
    "`  --training-mode lora `\\\n",
    " ` --learning-rate 0.0001` \\\n",
    " ` --num-epochs 5`\n",
    "\n",
    "`mlflow server --host 127.0.0.1 --port 8080 --backend-store-uri ./finetune_results/mlflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c271c5",
   "metadata": {},
   "source": [
    "# ESM3 academy's tailoring output heads code \n",
    "\n",
    "(https://esm3academy.com/customizing-esm3-for-specialized-tasks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Classification Heads\n",
    "class ClassificationModel(nn.Module): \n",
    "    def __init__(self, esm_model, num_classes): \n",
    "        super(ClassificationModel, self).__init__() \n",
    "        self.esm = esm_model \n",
    "        self.fc = nn.Linear(768, num_classes) # Adjust for embedding size \n",
    "\n",
    "    def forward(self, tokens): \n",
    "        outputs = self.esm(tokens) \n",
    "        cls_embedding = outputs[\"representations\"][0][:, 0, :] # CLS token \n",
    "        return self.fc(cls_embedding) \n",
    "        \n",
    "    \n",
    "    def forward(self, tokens): \n",
    "        outputs = self.esm(tokens) \n",
    "        cls_embedding = outputs[\"representations\"][0][:, 0, :] # CLS token \n",
    "        return self.fc(cls_embedding)\n",
    "    \n",
    "#Adding Token Classification Heads\n",
    "\n",
    "class TokenClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model, num_classes):\n",
    "        super(TokenClassificationModel, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.fc = nn.Linear(768, num_classes)  # Residue-level labels\n",
    "\n",
    "    def forward(self, tokens): \n",
    "        outputs = self.esm(tokens)\n",
    "        residue_embeddings = outputs[\"representations\"][0]\n",
    "        return self.fc(residue_embeddings)\n",
    "\n",
    "#Adding Regression Heads\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.fc = nn.Linear(768, 1)  # Single regression output\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        outputs = self.esm(tokens)\n",
    "        cls_embedding = outputs[\"representations\"][0][:, 0, :]  # CLS token\n",
    "        return self.fc(cls_embedding)\n",
    "\n",
    "#Multi-task model \n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model, num_classes_task1, num_classes_task2):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.fc_task1 = nn.Linear(768, num_classes_task1)\n",
    "        self.fc_task2 = nn.Linear(768, num_classes_task2)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        outputs = self.esm(tokens)\n",
    "        cls_embedding = outputs[\"representations\"][0][:, 0, :]  # CLS token\n",
    "        task1_output = self.fc_task1(cls_embedding)\n",
    "        task2_output = self.fc_task2(cls_embedding)\n",
    "        return task1_output, task2_output\n",
    "\n",
    "#function pred*\n",
    "\n",
    "class FunctionClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model, num_classes):\n",
    "        super(FunctionClassifier, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.fc = nn.Linear(768, num_classes)  # Adjust for embedding dimension\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        outputs = self.esm(tokens)\n",
    "        cls_embedding = outputs[\"representations\"][0][:, 0, :]  # CLS token\n",
    "        return self.fc(cls_embedding)\n",
    "\n",
    "class StabilityPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model):\n",
    "        super(StabilityPredictor, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.fc = nn.Linear(768, 1)  # Single regression output\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        outputs = self.esm(tokens)\n",
    "        cls_embedding = outputs[\"representations\"][0][:, 0, :]  # CLS token\n",
    "        return self.fc(cls_embedding)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model, cnn_model, output_dim):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.cnn = cnn_model\n",
    "        self.fc = nn.Linear(esm_model.embedding_dim + cnn_model.output_dim, output_dim)\n",
    "\n",
    "    def forward(self, sequence_tokens, structural_data):\n",
    "        sequence_embeddings = self.esm(sequence_tokens)[\"representations\"][0][:, 0, :]\n",
    "        structural_features = self.cnn(structural_data)\n",
    "        combined_features = torch.cat((sequence_embeddings, structural_features), dim=1)\n",
    "        return self.fc(combined_features)\n",
    "\n",
    "class GraphProteinModel(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model, gnn_model, output_dim):\n",
    "        super(GraphProteinModel, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.gnn = gnn_model\n",
    "        self.fc = nn.Linear(gnn_model.output_dim, output_dim)\n",
    "\n",
    "    def forward(self, sequence_tokens, graph_data):\n",
    "        sequence_embeddings = self.esm(sequence_tokens)[\"representations\"][0]\n",
    "        graph_embeddings = self.gnn(graph_data)\n",
    "        combined_features = torch.cat((sequence_embeddings, graph_embeddings), dim=1)\n",
    "        return self.fc(combined_features)\n",
    "\n",
    "class MutationalEffectModel(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model):\n",
    "        super(MutationalEffectModel, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.fc = nn.Linear(768, 1)  # Predict a single functional score\n",
    "\n",
    "    def forward(self, tokens_wt, tokens_mutant):\n",
    "        embeddings_wt = self.esm(tokens_wt)[\"representations\"][0][:, 0, :]\n",
    "        embeddings_mutant = self.esm(tokens_mutant)[\"representations\"][0][:, 0, :]\n",
    "        delta_embedding = embeddings_mutant - embeddings_wt\n",
    "        return self.fc(delta_embedding)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from esm import pretrained\n",
    "\n",
    "# Load pre-trained ESM3\n",
    "\n",
    "model, alphabet = pretrained.esm3_t30_150M()\n",
    "\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# Modify the model for classification\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, esm_model, num_classes):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.esm = esm_model\n",
    "        self.fc = nn.Linear(768, num_classes)  # Adjust for embedding dimension\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        outputs = self.esm(tokens)\n",
    "        cls_embedding = outputs[\"representations\"][0][:, 0, :]  # CLS token\n",
    "        return self.fc(cls_embedding)\n",
    "\n",
    "num_classes = 5  # Example: 5 functional categories\n",
    "\n",
    "classification_model = ClassificationModel(model, num_classes)\n",
    "\n",
    "#Training loop\n",
    "\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=1e-5)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_labels, batch_strs, batch_tokens in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = classification_model(batch_tokens)\n",
    "        loss = loss_function(predictions, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64403eb4",
   "metadata": {},
   "source": [
    "# novozyme competition attempt to predict Tm fine tuning esm \n",
    "\n",
    "(https://www.kaggle.com/code/jinyuansun/eda-and-finetune-esm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c1b87",
   "metadata": {},
   "source": [
    "# CAFA5 protein function prediction tournament \n",
    "(https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739297f9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
