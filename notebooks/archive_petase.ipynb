{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a0f89a1",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "\n",
    "Loading data (Uniprot / Mgnify / NCBI) from a IDs input\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bdd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from Bio import Entrez\n",
    "\n",
    "Entrez.email = \"justin.neo2@gmail.com\"   # required by NCBI\n",
    "OUTPUT_FILE = \"erickson_petasevariant.fasta\"\n",
    "#Some entries like R4YKL9_OLEAN, E9LVI0_THEFU, Q47RJ6_THEFY are UniProt accession + organism suffix.\n",
    "#The REST API expects only the accession (R4YKL9, E9LVI0, Q47RJ6) — not the _XXXX part.\n",
    "# read IDs from file, pre-clean the     IDs to remove any .1 .2 etc. \n",
    "#some IDs are in a different format than the one REST needs, on uniprot use the entry name strictly  \n",
    "#\n",
    "with open(\"petase_db/erickson_petasevariant_id.txt\") as f:\n",
    "    ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "#UNIPROT \n",
    "def fetch_uniprot(uid):\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uid}.fasta\"\n",
    "    r = requests.get(url)\n",
    "    return r.text if r.status_code == 200 else None\n",
    "\n",
    "#MGNIFY\n",
    "def fetch_mgnify(mid):\n",
    "    url = f\"https://www.ebi.ac.uk/metagenomics/api/latest/sequence/{mid}.fasta\"\n",
    "    r = requests.get(url)\n",
    "    return r.text if r.status_code == 200 else None\n",
    "\n",
    "#ENTREZ NCBI \n",
    "def fetch_ncbi(nid):\n",
    "    for db in [\"protein\", \"nuccore\"]:\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=db, id=nid, rettype=\"fasta\", retmode=\"text\")\n",
    "            fasta = handle.read()\n",
    "            if fasta.strip():\n",
    "                return fasta\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as out:\n",
    "    for seq_id in ids:\n",
    "        print(f\"Fetching {seq_id} ...\")\n",
    "        fasta = None\n",
    "        #if seq_id.startswith((\"A0\", \"Q\", \"P\", \"E\")) or seq_id == \"MPZ00478\" or seq_id == \"MBO9532528\" or seq_id == \"MBX2857432\" or seq_id == \"MBX3625601\":\n",
    "        #    fasta = fetch_uniprot(seq_id)\n",
    "        #elif seq_id.startswith((\"MGYP\", \"MBX\", \"MBO\", \"MPZ\")):\n",
    "        #    fasta = fetch_mgnify(seq_id)\n",
    "        #else:\n",
    "        #    fasta = fetch_ncbi(seq_id)\n",
    "        fasta = fetch_ncbi(seq_id)\n",
    "        if fasta is None:\n",
    "            print(f\"⚠️ Could not fetch {seq_id}\")\n",
    "        else:\n",
    "            out.write(fasta)\n",
    "\n",
    "print(f\"✅ Done! Sequences saved in {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062fdb2",
   "metadata": {},
   "source": [
    "Unified PETase seq from Uniprotec + Pazy\n",
    "\n",
    "100: 464\n",
    "\n",
    "cd-hit 0.99: 489 ->  457\n",
    "\n",
    "0.98: 446\n",
    "\n",
    "0.97: 434\n",
    "\n",
    "0.96: 422\n",
    "\n",
    "95: 413 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92c236",
   "metadata": {},
   "source": [
    "CaPETase mutation adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd573c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "def apply_mutations_to_fasta(input_fasta, output_fasta, mutations, offset=27):\n",
    "    \"\"\"\n",
    "    Apply a list of mutations (e.g. [\"S121E\", \"D186H\"]) to a FASTA sequence.\n",
    "\n",
    "    Parameters:\n",
    "        input_fasta (str): path to input fasta (with WT sequence)\n",
    "        output_fasta (str): path to save mutated fasta\n",
    "        mutations (list): list of mutation strings (e.g. [\"S121E\", \"D186H\"])\n",
    "        offset (int): numbering offset (e.g. 27 if sequence is signal peptide–trimmed)\n",
    "    \"\"\"\n",
    "\n",
    "    # Read first sequence from fasta\n",
    "    record = next(SeqIO.parse(input_fasta, \"fasta\"))\n",
    "    seq_list = list(str(record.seq))\n",
    "    original_seq = str(record.seq)\n",
    "\n",
    "    applied = []\n",
    "    for mut in mutations:\n",
    "        wt_res = mut[0]            # first letter\n",
    "        new_res = mut[-1]          # last letter\n",
    "        pos = int(mut[1:-1])       # number in between\n",
    "        idx = pos - offset - 1     # adjust and convert to 0-based\n",
    "\n",
    "        if idx < 0 or idx >= len(seq_list):\n",
    "            applied.append((mut, \"⚠️ out of range\"))\n",
    "            continue\n",
    "\n",
    "        if seq_list[idx] != wt_res:\n",
    "            applied.append((mut, f\"⚠️ mismatch: found {seq_list[idx]} at pos {pos}, expected {wt_res}\"))\n",
    "        else:\n",
    "            seq_list[idx] = new_res\n",
    "            applied.append((mut, \"ok\"))\n",
    "\n",
    "    # Make new record\n",
    "    mutated_seq = \"\".join(seq_list)\n",
    "    new_id = record.id + \"_\" + \"_\".join(mutations)\n",
    "    new_record = SeqRecord(Seq(mutated_seq), id=new_id, description=\"Mutated variant\")\n",
    "\n",
    "    # Write FASTA\n",
    "    SeqIO.write(new_record, output_fasta, \"fasta\")\n",
    "\n",
    "    return applied, mutated_seq\n",
    "\n",
    "# Example usage:\n",
    "mutations = [\"T250N\"]\n",
    "applied, mutated_seq = apply_mutations_to_fasta(\n",
    "    \"petase_db/wtcapetase.fasta\",\n",
    "    \"CaPETase_T250N.fasta\",\n",
    "    mutations,\n",
    "    offset=0\n",
    ")\n",
    "\n",
    "print(\"Applied mutations:\")\n",
    "for m in applied:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3ca51",
   "metadata": {},
   "source": [
    "BhrPETase LCC Son Mutations Adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b96f5be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 34 variants to bhrtest.fasta\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pandas as pd\n",
    "\n",
    "#mutations\n",
    "mutations_bhr = [\n",
    "    \"W104L\",\"W104S\",\"W104C\",\"W104H\",\"W104D\",\"W104R\",\"W104G\",\n",
    "    \"H164L\",\"H164S\",\"H164E\",\"H164Q\",\"H164F\",\n",
    "    \"M166L\",\"M166S\",\"M166D\",\"M166F\",\n",
    "    \"W190L\",\"W190M\",\"W190S\",\"W190H\",\"W190D\",\n",
    "    \"H191L\",\"H191M\",\"H191S\",\"H191D\",\"H191Y\",\n",
    "    \"F243I\",\"F243S\",\"F243T\",\"F243D\",\"F243N\",\"F243G\",\n",
    "    \"H218S\",\"H218S/F222I\"\n",
    "]\n",
    "mutations_son = [\n",
    "    \"S121D\",\n",
    "    \"S121E\",\n",
    "    \"D186H\",\n",
    "    \"D186F\",\n",
    "    \"D186I\",\n",
    "    \"D186L\",\n",
    "    \"D186V\",\n",
    "    \"P181A\",\n",
    "    \"P181G\",\n",
    "    \"P181S\"\n",
    "]\n",
    "\n",
    "mutations_lcc = [\n",
    "    \"T96M\",\n",
    "    \"Y127G\",\n",
    "    \"F243I\",\n",
    "    \"F243W\",\n",
    "    \"N246D\",\n",
    "    \"N246M\",\n",
    "    \"D238C/S283C\",\n",
    "    \"F243I/D238C/S283C\",\n",
    "    \"F243W/D238C/S283C\",\n",
    "    \"F243I/D238C/S283C/T96M\",\n",
    "    \"F243I/D238C/S283C/Y127G\",\n",
    "    \"F243I/D238C/S283C/N246D\",\n",
    "    \"F243I/D238C/S283C/N246M\",\n",
    "    \"F243W/D238C/S283C/T96M\",\n",
    "    \"F243W/D238C/S283C/Y127G\",\n",
    "    \"F243W/D238C/S283C/N246D\",\n",
    "    \"F243W/D238C/S283C/N246M\"\n",
    "]\n",
    "# ---- Input ----\n",
    "wt_fasta = \"petase_db/sequences/wtbhrpetase.fasta\"   # WT FASTA\n",
    "output_fasta = \"bhrtest.fasta\"\n",
    "\n",
    "# ---- Load WT sequence ----\n",
    "wt_record = SeqIO.read(wt_fasta, \"fasta\")\n",
    "wt_seq = list(str(wt_record.seq))\n",
    "\n",
    "OFFSET = 0  # negative offset\n",
    "\n",
    "def apply_mutations(wt_seq, mutations_str):\n",
    "    seq = wt_seq.copy()\n",
    "    for mut in mutations_str.replace(\"+\", \"/\").split(\"/\"):\n",
    "        if not mut:\n",
    "            continue\n",
    "        wt_aa = mut[0]\n",
    "        pos = int(mut[1:-1])\n",
    "        pos += OFFSET   # << now applies negative offset if set\n",
    "        new_aa = mut[-1]\n",
    "        if pos-1 >= len(seq) or pos-1 < 0:\n",
    "            raise ValueError(f\"Mutation {mut} out of range for sequence length {len(seq)} (index {pos-1})\")\n",
    "        if seq[pos-1] != wt_aa:\n",
    "            print(f\"⚠️ Warning: expected {wt_aa} at {pos}, found {seq[pos-1]}\")\n",
    "        seq[pos-1] = new_aa\n",
    "    return \"\".join(seq)\n",
    "\n",
    "# ---- Generate all variants ----\n",
    "records = []\n",
    "for mut in mutations_bhr:\n",
    "    new_seq = apply_mutations(wt_seq, mut)\n",
    "    record = SeqRecord(Seq(new_seq), id=f\"bhrtest_{mut}\", description=mut)\n",
    "    records.append(record)\n",
    "\n",
    "# ---- Write output FASTA ----\n",
    "SeqIO.write(records, output_fasta, \"fasta\")\n",
    "print(f\"✅ Wrote {len(records)} variants to {output_fasta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92789202",
   "metadata": {},
   "source": [
    "PETase mutation scoring for IsPETase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def sequences_position_table(input_fasta, mutations, offset=27):\n",
    "    \"\"\"\n",
    "    Build a table with columns as mutation positions and rows as sequences,\n",
    "    showing the amino acid present at each position.\n",
    "\n",
    "    Parameters:\n",
    "        input_fasta (str): path to FASTA containing one or more sequences\n",
    "        mutations (list): list of mutation strings (e.g., [\"S160\", \"D206A\", \"W159[H/A]\"])\n",
    "        offset (int): numbering offset (subtract this many from the positions)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: table with mutations as columns and sequences as rows\n",
    "    \"\"\"\n",
    "    # Regex to parse mutation\n",
    "    pattern = re.compile(r\"([A-Z])?(\\d+)([A-Z]|\\[[A-Z/]+\\])?\")\n",
    "\n",
    "    # Parse mutation positions once\n",
    "    mut_info = []\n",
    "    for mut in mutations:\n",
    "        m = pattern.match(mut)\n",
    "        if not m:\n",
    "            continue\n",
    "        _, pos, _ = m.groups()\n",
    "        pos = int(pos) - offset\n",
    "        idx = pos - 1\n",
    "        mut_info.append((mut, idx))\n",
    "\n",
    "    # Collect rows for each sequence\n",
    "    rows = []\n",
    "    row_ids = []\n",
    "\n",
    "    for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        row_ids.append(record.id)\n",
    "\n",
    "        row_vals = []\n",
    "        for mut, idx in mut_info:\n",
    "            if 0 <= idx < len(seq):\n",
    "                row_vals.append(seq[idx])\n",
    "            else:\n",
    "                row_vals.append(\"NA\")  # if index out of range\n",
    "        rows.append(row_vals)\n",
    "\n",
    "    # Build DataFrame\n",
    "    col_labels = [mut for mut, _ in mut_info]\n",
    "    df = pd.DataFrame(rows, columns=col_labels, index=row_ids)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "mutations = [\n",
    "    \"S160\", \"D206\", \"H237\", \"T88\", \"W159\", \"W185\", \"I208\", \"S214\", \"Y87\", \"M161\",\n",
    "    \"C203\", \"C239\", \"C273\", \"C289\", \"S160A\", \"Y87A\", \"M161A\", \"T88A\", \"W159[H/A]\",\n",
    "    \"W185A\", \"I208A\", \"S214H\", \"C203S\", \"C239S\", \"S160\", \"D206\", \"H237\", \"Y87\",\n",
    "    \"M161\", \"W185\", \"I208\", \"T88\", \"A89\", \"W159\", \"I232\", \"N233\", \"S236\", \"S238F\",\n",
    "    \"N241\", \"N244\", \"S245\", \"N246\", \"R280\", \"S160A\", \"D206A\", \"H237A\", \"Y87A\",\n",
    "    \"M161A\", \"W185A\", \"I208A\", \"W159[H/A]\", \"S238F\", \"N241A\", \"R280A\", \"C203A\",\n",
    "    \"C239A\", \"R90A\", \"L117F\", \"I208F\", \"R280A\", \"W159H\", \"S238F\", \"L117F\", \"R280A\",\n",
    "    \"T88M\", \"Q119G\", \"N233C\", \"S238I\", \"N241D\", \"S282C\"\n",
    "]\n",
    "\n",
    "df = sequences_position_table(\"petase_db/sequences/all_petase_variants.fasta\", mutations, offset=27)\n",
    "\n",
    "print(df)\n",
    "df.to_csv(\"petase_mutation_table_allseqs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f5d5c",
   "metadata": {},
   "source": [
    "Mutation scoring for CaPETase variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "\n",
    "def sequences_position_table(input_fasta, positions):\n",
    "    \"\"\"\n",
    "    Build a DataFrame where columns are positions and rows are sequences,\n",
    "    showing the amino acid present at each position.\n",
    "\n",
    "    Parameters:\n",
    "        input_fasta (str): path to FASTA with one or more sequences\n",
    "        positions (list[int]): list of 1-based positions to check\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    row_ids = []\n",
    "\n",
    "    for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        row_ids.append(record.id)\n",
    "\n",
    "        row_vals = []\n",
    "        for pos in positions:\n",
    "            if 1 <= pos <= len(seq):\n",
    "                row_vals.append(seq[pos-1])  # convert to 0-based index\n",
    "            else:\n",
    "                row_vals.append(\"NA\")  # if position is out of range\n",
    "        rows.append(row_vals)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[str(p) for p in positions], index=row_ids)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "positions = [169, 246, 215, 194, 223, 227, 101, 102, 103,\n",
    "             107, 133, 168, 170, 195, 196, 217, 247, 250]\n",
    "\n",
    "df = sequences_position_table(\"petase_db/sequences/capetasevariants.fasta\", positions)\n",
    "\n",
    "print(df)\n",
    "df.to_csv(\"catase_positions_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4211fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def sequences_mutation_table(input_fasta, mutations, offset=0):\n",
    "    \"\"\"\n",
    "    Build a DataFrame where columns are mutation strings and rows are sequences,\n",
    "    showing the amino acid present at each (offset-adjusted) position.\n",
    "\n",
    "    Parameters:\n",
    "        input_fasta (str): path to FASTA with one or more sequences\n",
    "        mutations (list[str]): mutation strings (e.g., \"S160\", \"D206A\", \"W159[H/A]\")\n",
    "        offset (int): numeric offset to apply to positions (default=0)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Regex to parse strings like S160, D206A, W159[H/A]\n",
    "    pattern = re.compile(r\"([A-Z])?(\\d+)([A-Z]|\\[[A-Z/]+\\])?\")\n",
    "\n",
    "    # Parse mutation strings into (label, adjusted pos)\n",
    "    mut_info = []\n",
    "    for mut in mutations:\n",
    "        m = pattern.match(mut)\n",
    "        if not m:\n",
    "            continue\n",
    "        _, pos, _ = m.groups()\n",
    "        pos = int(pos) + offset  # apply offset\n",
    "        mut_info.append((mut, pos))\n",
    "\n",
    "    rows = []\n",
    "    row_ids = []\n",
    "\n",
    "    for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        row_ids.append(record.id)\n",
    "\n",
    "        row_vals = []\n",
    "        for label, pos in mut_info:\n",
    "            if 1 <= pos <= len(seq):\n",
    "                row_vals.append(seq[pos-1])  # 1-based to 0-based index\n",
    "            else:\n",
    "                row_vals.append(\"NA\")\n",
    "        rows.append(row_vals)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[label for label, _ in mut_info], index=row_ids)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "mutations = [\n",
    "    \"S160\", \"D206\", \"H237\", \"T88\", \"W159\", \"W185\", \"I208\", \"S214\", \"Y87\", \"M161\",\n",
    "    \"C203\", \"C239\", \"C273\", \"C289\", \"S160A\", \"Y87A\", \"M161A\", \"T88A\", \"W159[H/A]\",\n",
    "    \"W185A\", \"I208A\", \"S214H\", \"C203S\", \"C239S\", \"S160\", \"D206\", \"H237\", \"Y87\",\n",
    "    \"M161\", \"W185\", \"I208\", \"T88\", \"A89\", \"W159\", \"I232\", \"N233\", \"S236\", \"S238F\",\n",
    "    \"N241\", \"N244\", \"S245\", \"N246\", \"R280\", \"S160A\", \"D206A\", \"H237A\", \"Y87A\",\n",
    "    \"M161A\", \"W185A\", \"I208A\", \"W159[H/A]\", \"S238F\", \"N241A\", \"R280A\", \"C203A\",\n",
    "    \"C239A\", \"R90A\", \"L117F\", \"I208F\", \"R280A\", \"W159H\", \"S238F\", \"L117F\", \"R280A\",\n",
    "    \"T88M\", \"Q119G\", \"N233C\", \"S238I\", \"N241D\", \"S282C\"\n",
    "]\n",
    "\n",
    "df = sequences_mutation_table(\"petase_db/sequences/CaPETasevariants.fasta\", mutations, offset=9)\n",
    "\n",
    "print(df)\n",
    "df.to_csv(\"capetase_mutation_check_offset9.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6eb4e",
   "metadata": {},
   "source": [
    "fix mutation list agreement across the pdb-fasta for foldx\n",
    "\n",
    "python fix_mutations_for_pdb.py wtIsPETase_6ilw_Repair.pdb individual_list.txt fixed_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a6f41",
   "metadata": {},
   "source": [
    "---------------------\n",
    "Encoding sequences and structures of variants with esm3\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig\n",
    "import torch\n",
    "import esm\n",
    "from esm.sdk import client\n",
    "login(token=\"removed\")\n",
    "\n",
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "#Forge API\n",
    "model=client(\"esm3-large-2024-08\",token=\"removed\")\n",
    "#test with esmc-6b now instead of 600m to see diff in dimenson of embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e48e6",
   "metadata": {},
   "source": [
    "Sequence emebdding esm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "import torch\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "import pandas as pd \n",
    "from esm.sdk import client\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Sequence\n",
    "from esm.sdk.api import (\n",
    "    ESM3InferenceClient,\n",
    "    ESMProtein,\n",
    "    ESMProteinError,\n",
    "    LogitsConfig,\n",
    "    LogitsOutput,\n",
    "    ProteinType,\n",
    ")\n",
    "\n",
    "model = client(\n",
    "    model=\"esmc-6b-2024-12\", url=\"https://forge.evolutionaryscale.ai\", token=\"removed\"\n",
    ")\n",
    "\n",
    "#if esm-6b, change to ith_hidden_layer=55\n",
    "#reset to: LogitsConfig(return_embeddings=True, return_hidden_states=True)\n",
    "EMBEDDING_CONFIG = LogitsConfig(\n",
    "    sequence=True, ith_hidden_layer=20\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "def embed_sequence(model: ESM3InferenceClient, sequence: str) -> LogitsOutput:\n",
    "    protein = ESMProtein(sequence=sequence)\n",
    "    protein_tensor = model.encode(protein)\n",
    "    output = model.logits(protein_tensor, EMBEDDING_CONFIG)\n",
    "    return output\n",
    "\n",
    "\n",
    "def batch_embed(\n",
    "    model: ESM3InferenceClient, inputs: Sequence[ProteinType]\n",
    ") -> Sequence[LogitsOutput]:\n",
    "    \"\"\"Forge supports auto-batching. So batch_embed() is as simple as running a collection\n",
    "    of embed calls in parallel using asyncio.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(embed_sequence, model, protein) for protein in inputs\n",
    "        ]\n",
    "        results = []\n",
    "        for future in futures:\n",
    "            try:\n",
    "                results.append(future.result())\n",
    "            except Exception as e:\n",
    "                results.append(ESMProteinError(500, str(e)))\n",
    "    return results\n",
    "\n",
    "\n",
    "def read_fasta_fast(fasta_file):\n",
    "    records = []\n",
    "    with open(fasta_file) as in_handle:\n",
    "        for title, seq in SimpleFastaParser(in_handle):\n",
    "            gene_id = title.split()[0]\n",
    "            records.append((gene_id, seq, len(seq)))\n",
    "    return pd.DataFrame(records, columns=[\"Gene\", \"Sequence\", \"Length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2166ccdb",
   "metadata": {},
   "source": [
    "ESM-3 (large) ~ 98 billion\n",
    "\n",
    "ESM-3 medium ~ 7 billion\n",
    "\n",
    "ESM-3 small (open) / “open” ~ 1.4 billion\n",
    "\n",
    "ESM C (6B variant) ~ 6 billion\n",
    "\n",
    "ESM C (600M variant) ~ 600 million\n",
    "\n",
    "ESM C (300M variant) ~ 300 million\n",
    "\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1b091",
   "metadata": {},
   "source": [
    "[TO DO]\n",
    "[OCT25] Run esm3 seq/struct embedding locally to avoid credits limit (on benchmark DB) using ESM3-open (1.4b parameters)\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cc234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig\n",
    "import torch\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "import pandas as pd \n",
    "from esm.sdk import client\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Sequence\n",
    "from esm.sdk.api import (\n",
    "    ESM3InferenceClient,\n",
    "    ESMProtein,\n",
    "    ESMProteinError,\n",
    "    LogitsConfig,\n",
    "    LogitsOutput,\n",
    "    ProteinType,\n",
    ")\n",
    "\n",
    "# This will download the model weights and instantiate the model on your machine.\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3-open\").to(\"cpu\") # or \"cpu\"\n",
    "EMBEDDING_CONFIG = LogitsConfig(return_embeddings=True, return_hidden_states=True)\n",
    "\n",
    "def embed_sequence(model: ESM3InferenceClient, sequence: str) -> LogitsOutput:\n",
    "    protein = ESMProtein(sequence=sequence)\n",
    "    protein_tensor = model.encode(protein)\n",
    "    output = model.logits(protein_tensor, EMBEDDING_CONFIG)\n",
    "    return output\n",
    "\n",
    "\n",
    "def read_fasta_fast(fasta_file):\n",
    "    records = []\n",
    "    with open(fasta_file) as in_handle:\n",
    "        for title, seq in SimpleFastaParser(in_handle):\n",
    "            gene_id = title.split()[0]\n",
    "            records.append((gene_id, seq, len(seq)))\n",
    "    return pd.DataFrame(records, columns=[\"Gene\", \"Sequence\", \"Length\"])\n",
    "\n",
    "# Will instruct you how to get an API key from huggingface hub, make one with \"Read\" permission.\n",
    "login()\n",
    "\n",
    "\n",
    "input = read_fasta_fast(\"petase_db/sequences/esm3_seq/wtIsPETase_signaltrim.fasta\")\n",
    "seq = input[\"Sequence\"].iloc[0]\n",
    "protein = ESMProtein(sequence=seq)\n",
    "protein_tensor = model.encode(protein)\n",
    "\n",
    "# Try simplest logits config\n",
    "config = LogitsConfig(sequence=True)\n",
    "\n",
    "output = model.logits(protein_tensor, config)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c7ca7",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "\n",
    "Run seq embedding on one fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"wtIsPETase_signaltrim\"\n",
    "# === Save outputs to pickle ===\n",
    "with open(f\"petase_db/sequences/esm3_seq/seq_embeddings/{name}_outputs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output, f)\n",
    "\n",
    "with open(f\"petase_db/sequences/esm3_seq/seq_embeddings/{name}_meanembeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_mean_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e0b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.sdk.api import LogitsConfig\n",
    "input = read_fasta_fast(\"petase_db/sequences/esm3_seq/wtIsPETase_signaltrim.fasta\")\n",
    "seq = input[\"Sequence\"].iloc[0]\n",
    "protein = ESMProtein(sequence=seq)\n",
    "protein_tensor = model.encode(protein)\n",
    "\n",
    "# Try simplest logits config\n",
    "config = LogitsConfig(sequence=True)\n",
    "\n",
    "output = model.logits(protein_tensor, config)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d55099",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = read_fasta_fast(\"petase_db/sequences/esm3_seq/wtIsPETase_signaltrim.fasta\")\n",
    "seq = input[\"Sequence\"].iloc[0]\n",
    "protein = ESMProtein(sequence=seq)\n",
    "protein_tensor = model.encode(protein)\n",
    "output = model.logits(protein_tensor, EMBEDDING_CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f894a",
   "metadata": {},
   "source": [
    "note that all_mean_embeddings is the mean collapsing all the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a55ea",
   "metadata": {},
   "source": [
    "Batch input fasta files to embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f5f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished D1-PETase.fasta\n",
      "✅ Finished TM3-PETase.fasta\n",
      "✅ Finished DuraN233K.fasta\n",
      "✅ Finished CaPETase_T250N.fasta\n",
      "✅ Finished wtCaPETase.fasta\n",
      "✅ Finished wtLCC.fasta\n",
      "✅ Finished wtIsPETase_signaltrim.fasta\n",
      "✅ Finished LCC_variants_tournier.fasta\n",
      "✅ Finished FAST-PETase.fasta\n",
      "✅ Finished IsPETase_W159H_F229Y.fasta\n",
      "✅ Finished BhrPETase_variants.fasta\n",
      "✅ Finished thermopetase.fasta\n",
      "✅ Finished CaPETasevariants.fasta\n",
      "✅ Finished HOT-PETase.fasta\n",
      "✅ Finished IsPETase_S238FW159H.fasta\n",
      "✅ Finished all_petase_variants.fasta\n",
      "✅ Finished DuraPETase.fasta\n",
      "✅ Finished TS-PETase.fasta\n",
      "✅ Finished wtBhrPETase.fasta\n"
     ]
    }
   ],
   "source": [
    "#for the sequences\n",
    "for file in os.listdir(\"petase_db/sequences/esm3_seq\"):\n",
    "    if file.endswith(\".fasta\"):\n",
    "        fasta_path = os.path.join(\"petase_db/sequences/esm3_seq\", file)\n",
    "\n",
    "        # load sequences\n",
    "        df = read_fasta_fast(fasta_path)\n",
    "        seq = df[\"Sequence\"].to_list()\n",
    "\n",
    "        # run embeddings\n",
    "        output = batch_embed(model, seq)\n",
    "\n",
    "        # summarize mean embeddings\n",
    "        all_mean_embeddings = [\n",
    "            torch.mean(o.hidden_states, dim=0).squeeze().cpu()\n",
    "            for o in output\n",
    "        ]\n",
    "\n",
    "        name = os.path.splitext(file)[0]\n",
    "        # === Save outputs to pickle ===\n",
    "        with open(f\"petase_db/sequences/esm3_seq/seq_embeddings/{name}_outputs.pkl\", \"wb\") as f:\n",
    "            pickle.dump(output, f)\n",
    "        with open(f\"petase_db/sequences/esm3_seq/seq_embeddings/{name}_meanembeddings.pkl\", \"wb\") as f:\n",
    "            pickle.dump(all_mean_embeddings, f)\n",
    "        print(f\"✅ Finished {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf91e1",
   "metadata": {},
   "source": [
    "struct embed single pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install esm huggingface_hub  (once)\n",
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESMProtein, SamplingConfig\n",
    "\n",
    "# 0) (one time) auth so weights can download; you can skip if already logged in\n",
    "# login()  # uncomment if needed; follows the ESM README\n",
    "\n",
    "PDB_PATH = \"petase_db/structures/wtIsPETase_6ilw_Repair.pdb\"\n",
    "CHAIN_ID = \"A\"\n",
    "\n",
    "# 1) Load structure into an ESMProtein\n",
    "#    Prefer the direct helper; fallback uses ProteinChain if your wheel exposes that API.\n",
    "try:\n",
    "    protein = ESMProtein.from_pdb(PDB_PATH, chain_id=CHAIN_ID)\n",
    "except AttributeError:\n",
    "    # Fallback path used in ESM examples circulating online\n",
    "    from esm.utils.structure.protein_chain import ProteinChain\n",
    "    protein = ESMProtein.from_protein_chain(\n",
    "        ProteinChain.from_pdb(PDB_PATH, chain_id=CHAIN_ID)\n",
    "    )\n",
    "\n",
    "# Load ESM3 open weights and encode\n",
    "\n",
    "protein_tensor = model.encode(protein)               \n",
    "# 4) Forward with embeddings requested\n",
    "out = model.forward_and_sample(\n",
    "    protein_tensor,\n",
    "    SamplingConfig(\n",
    "        return_per_residue_embeddings=True,\n",
    "        return_mean_embedding=True\n",
    "    )\n",
    ")\n",
    "\n",
    "import pickle\n",
    "with open(\"esm3_output.pkl\", \"wb\") as f:\n",
    "    pickle.dump(out, f)\n",
    "\n",
    "# 5) Grab embeddings\n",
    "per_residue = out.per_residue_embedding   # shape [L, D]\n",
    "mean_embed  = out.mean_embedding          # shape [D]\n",
    "print(per_residue.shape, mean_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af860149",
   "metadata": {},
   "source": [
    "structure embedding (batch pdb files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c368ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for structures\n",
    "EMBEDDING_CONFIG = LogitsConfig(\n",
    "    sequence=True, return_embeddings=True, return_hidden_states=True\n",
    ")\n",
    "for file in os.listdir(\"petase_db/structures/esm3_struct\"):\n",
    "    if file.endswith(\".pdb\"):\n",
    "        pdb_path = os.path.join(\"petase_db/structures/esm3_struct\", file)\n",
    "        CHAIN_ID = \"A\"\n",
    "        # 1) Load structure into an ESMProtein\n",
    "        #    Prefer the direct helper; fallback uses ProteinChain if your wheel exposes that API.\n",
    "        try:\n",
    "            protein = ESMProtein.from_pdb(pdb_path, chain_id=CHAIN_ID)\n",
    "        except AttributeError:\n",
    "            # Fallback path used in ESM examples circulating online\n",
    "            from esm.utils.structure.protein_chain import ProteinChain\n",
    "            protein = ESMProtein.from_protein_chain(\n",
    "                ProteinChain.from_pdb(pdb_path, chain_id=CHAIN_ID)\n",
    "            )\n",
    "        protein_tensor = model.encode(protein)\n",
    "        out = model.forward_and_sample(protein_tensor,SamplingConfig(return_per_residue_embeddings=True, return_mean_embedding=True))\n",
    "        per_residue = out.per_residue_embedding   # shape [L, D]\n",
    "        mean_embed  = out.mean_embedding          # shape [D]\n",
    "\n",
    "        name = os.path.splitext(file)[0]\n",
    "        # === Save outputs to pickle ===\n",
    "        with open(f\"petase_db/structures/esm3_struct/embeddings_struct/{name}_outputs.pkl\", \"wb\") as f:\n",
    "            pickle.dump(out, f)\n",
    "        with open(f\"petase_db/structures/esm3_struct/embeddings_struct/{name}_meanembeddings.pkl\", \"wb\") as f:\n",
    "            pickle.dump(mean_embed, f)\n",
    "        print(f\"✅ Finished {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a9245",
   "metadata": {},
   "source": [
    "Working with the fireprot.sql file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "sql_file = \"fireprotdb.sql\"\n",
    "\n",
    "# Extract amino_acids_substitution table\n",
    "pattern = re.compile(\n",
    "    r\"INSERT INTO `amino_acids_substitution`\\s*\\([^)]+\\)\\s*VALUES\\s*(.+?);\",\n",
    "    re.S\n",
    ")\n",
    "matches = pattern.findall(open(sql_file).read())\n",
    "\n",
    "rows = []\n",
    "for match in matches:\n",
    "    for group in re.findall(r\"\\('([^']+)',\\s*(\\d+),\\s*(\\d+)\\)\", match):\n",
    "        rows.append(group)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"amino_acid\", \"from_substitutions\", \"to_substitutions\"])\n",
    "df.to_csv(\"amino_acids_substitution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9627e8b",
   "metadata": {},
   "source": [
    "Unifying the benchmark DBs for downstream embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ FIREPROTDB\n",
    "# -----------------------------\n",
    "fireprot = pd.read_csv(\"fireprotdb_results_stability.csv\")\n",
    "fireprot[\"uniprot_id\"].dropna().drop_duplicates().to_csv(\"uniprot_ids.txt\", index=False, header=False)\n",
    "fireprot[\"pdb_id\"].dropna().drop_duplicates().to_csv(\"pdb_ids.txt\", index=False, header=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ MELTOME (only gene_name)\n",
    "# -----------------------------\n",
    "meltome = pd.read_csv(\"meltome_cross-species.csv\")\n",
    "meltome[\"gene_name\"].dropna().drop_duplicates().to_csv(\"gene_names.txt\", index=False, header=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ THERMOMUTDB (JSON)\n",
    "# -----------------------------\n",
    "with open(\"thermomutdb.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare lists\n",
    "pdb_wild = []\n",
    "mutation_code = []\n",
    "mutated_chain = []\n",
    "uniprot = []\n",
    "pdb_mutant = []\n",
    "\n",
    "for rec in data:\n",
    "    pdb_wild.append(rec.get(\"PDB_wild\", \"NA\"))\n",
    "    mutation_code.append(rec.get(\"mutation_code\", \"NA\"))\n",
    "    mutated_chain.append(rec.get(\"mutated_chain\", \"NA\"))\n",
    "    uniprot.append(rec.get(\"uniprot\", \"NA\"))\n",
    "    pdb_mutant.append(rec.get(\"pdb_mutant\", \"NA\"))\n",
    "\n",
    "# Helper: save line-by-line (preserve NA)\n",
    "def save_list(values, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for v in values:\n",
    "            f.write(str(v).strip() + \"\\n\")\n",
    "\n",
    "# Save each list to its own file\n",
    "save_list(pdb_wild, \"thermomutdb_PDB_wild.txt\")\n",
    "save_list(mutation_code, \"thermomutdb_mutation_code.txt\")\n",
    "save_list(mutated_chain, \"thermomutdb_mutated_chain.txt\")\n",
    "save_list(uniprot, \"thermomutdb_uniprot.txt\")\n",
    "save_list(pdb_mutant, \"thermomutdb_pdb_mutant.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88258781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Build a single combined list of proteins that are NOT present in crossmatch_clusters.csv.\n",
    "# Also write a list of proteins that ARE present but NOT cross-matched (clusters with n_datasets == 1).\n",
    "# No console prints; writes CSVs (and optional FASTA).\n",
    "\n",
    "# ------------- CONFIG (edit) -------------\n",
    "PSI_PATH      = \"PSI_Biology_solubility_trainset.csv\"\n",
    "NESG_PATH     = \"NESG_testset.csv\"\n",
    "PRICE_PATH    = \"Price_usability_trainset.csv\"\n",
    "CLUSTERS_PATH = \"out/crossmatch_clusters.csv\"  # from crossmatch_blast_simple.py\n",
    "OUTDIR        = \"out\"\n",
    "\n",
    "WRITE_FASTA   = True    # also write a FASTA of missing sequences\n",
    "FASTA_PATH    = \"out/missing_from_crossmatch.fasta\"\n",
    "CSV_PATH      = \"out/missing_from_crossmatch.csv\"\n",
    "\n",
    "# NEW outputs:\n",
    "MISSING_SUMMARY_PATH = \"out/missing_from_crossmatch_summary.txt\"\n",
    "NOTX_CSV_PATH        = \"out/not_crossmatched.csv\"              # present in clusters, but n_datasets == 1\n",
    "NOTX_SUMMARY_PATH    = \"out/not_crossmatched_summary.txt\"\n",
    "# -----------------------------------------\n",
    "\n",
    "import os, pandas as pd\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def load_src(path: str, label: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    if \"sid\" not in df.columns or \"fasta\" not in df.columns:\n",
    "        raise ValueError(f\"{label}: expected columns ['sid','fasta']\")\n",
    "    df = df[[\"sid\",\"fasta\"]].dropna(subset=[\"sid\",\"fasta\"]).copy()\n",
    "    df[\"sid\"] = df[\"sid\"].astype(str).s tr.strip()\n",
    "    df[\"dataset\"] = label\n",
    "    return df\n",
    "\n",
    "def split_ids(s: str):\n",
    "    s = (s or \"\").strip()\n",
    "    return [x.strip() for x in s.split(\";\") if x.strip()] if s else []\n",
    "\n",
    "# Load originals\n",
    "psi_df   = load_src(PSI_PATH,   \"PSI_Biology\")\n",
    "nesg_df  = load_src(NESG_PATH,  \"NESG\")\n",
    "price_df = load_src(PRICE_PATH, \"Price\")\n",
    "\n",
    "# Load clusters\n",
    "cl = pd.read_csv(CLUSTERS_PATH, dtype=str).fillna(\"\")\n",
    "for col in [\"psi_ids\",\"nesg_ids\",\"price_ids\",\"n_datasets\",\"cluster_id\"]:\n",
    "    if col not in cl.columns:\n",
    "        raise ValueError(f\"crossmatch_clusters.csv missing expected column: {col}\")\n",
    "\n",
    "# IDs present in clusters (per dataset)\n",
    "psi_in   = set(x for ids in cl[\"psi_ids\"]   for x in split_ids(ids))\n",
    "nesg_in  = set(x for ids in cl[\"nesg_ids\"]  for x in split_ids(ids))\n",
    "price_in = set(x for ids in cl[\"price_ids\"] for x in split_ids(ids))\n",
    "\n",
    "# Compute missing by dataset (TRULY absent from clusters)\n",
    "psi_missing   = psi_df[~psi_df[\"sid\"].isin(psi_in)].copy()\n",
    "nesg_missing  = nesg_df[~nesg_df[\"sid\"].isin(nesg_in)].copy()\n",
    "price_missing = price_df[~price_df[\"sid\"].isin(price_in)].copy()\n",
    "\n",
    "missing_all = pd.concat([psi_missing, nesg_missing, price_missing], ignore_index=True)\n",
    "missing_all = missing_all[[\"dataset\",\"sid\",\"fasta\"]].sort_values([\"dataset\",\"sid\"])\n",
    "\n",
    "# Write CSV of truly missing\n",
    "missing_all.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Write summary of truly missing\n",
    "with open(MISSING_SUMMARY_PATH, \"w\") as f:\n",
    "    f.write(\"IDs present in source CSVs but NOT present in crossmatch_clusters.csv\\n\\n\")\n",
    "    f.write(f\"PSI_Biology: {len(psi_missing)}\\n\")\n",
    "    f.write(f\"NESG: {len(nesg_missing)}\\n\")\n",
    "    f.write(f\"Price: {len(price_missing)}\\n\")\n",
    "    f.write(f\"\\nTOTAL: {len(missing_all)}\\n\")\n",
    "\n",
    "# Optional FASTA for truly missing\n",
    "if WRITE_FASTA and not missing_all.empty:\n",
    "    with open(FASTA_PATH, \"w\") as fh:\n",
    "        for _, r in missing_all.iterrows():\n",
    "            hdr = f\"{r['dataset']}|{r['sid']}\"\n",
    "            seq = str(r[\"fasta\"])\n",
    "            fh.write(f\">{hdr}\\n\")\n",
    "            for i in range(0, len(seq), 80):\n",
    "                fh.write(seq[i:i+80] + \"\\n\")\n",
    "\n",
    "# NEW PART: proteins present in clusters but NOT cross-matched (clusters with n_datasets == 1)\n",
    "cl[\"n_datasets_int\"] = pd.to_numeric(cl[\"n_datasets\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "solo = cl[cl[\"n_datasets_int\"] == 1].copy()\n",
    "\n",
    "rows_notx = []\n",
    "for ds_col, ds_name in [(\"psi_ids\",\"PSI_Biology\"), (\"nesg_ids\",\"NESG\"), (\"price_ids\",\"Price\")]:\n",
    "    for _, row in solo.iterrows():\n",
    "        for sid in split_ids(row[ds_col]):\n",
    "            rows_notx.append({\"dataset\": ds_name, \"sid\": sid, \"cluster_id\": row[\"cluster_id\"]})\n",
    "\n",
    "notx_df = pd.DataFrame(rows_notx).sort_values([\"dataset\",\"sid\",\"cluster_id\"])\n",
    "notx_df.to_csv(NOTX_CSV_PATH, index=False)\n",
    "\n",
    "# Summary for not-crossmatched\n",
    "counts = notx_df.groupby(\"dataset\")[\"sid\"].nunique().reindex([\"PSI_Biology\",\"NESG\",\"Price\"]).fillna(0).astype(int)\n",
    "with open(NOTX_SUMMARY_PATH, \"w\") as f:\n",
    "    f.write(\"IDs present in clusters but NOT cross-matched (clusters with n_datasets == 1)\\n\\n\")\n",
    "    for ds in [\"PSI_Biology\",\"NESG\",\"Price\"]:\n",
    "        f.write(f\"{ds}: {int(counts.get(ds, 0))}\\n\")\n",
    "    f.write(f\"\\nTOTAL: {int(counts.sum())}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b068d800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 11856 entries to out/missing_from_crossmatch.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Adds truly missing proteins (not found in crossmatch_clusters.csv) to the output CSV and FASTA.\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "PSI_PATH      = \"PSI_Biology_solubility_trainset.csv\"\n",
    "NESG_PATH     = \"NESG_testset.csv\"\n",
    "PRICE_PATH    = \"Price_usability_trainset.csv\"\n",
    "CLUSTERS_PATH = \"out/crossmatch_clusters.csv\"\n",
    "OUTDIR        = \"out\"\n",
    "\n",
    "CSV_PATH      = \"out/missing_from_crossmatch.csv\"\n",
    "FASTA_PATH    = \"out/missing_from_crossmatch.fasta\"\n",
    "WRITE_FASTA   = True\n",
    "# ----------------------------------\n",
    "\n",
    "import os, pandas as pd\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def load_src(path, label):\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    if \"sid\" not in df.columns or \"fasta\" not in df.columns:\n",
    "        raise ValueError(f\"{label}: expected columns ['sid','fasta']\")\n",
    "    df = df[[\"sid\",\"fasta\"]].dropna().copy()\n",
    "    df[\"sid\"] = df[\"sid\"].astype(str).str.strip()\n",
    "    df[\"dataset\"] = label\n",
    "    return df\n",
    "\n",
    "def split_ids(s):\n",
    "    s = (s or \"\").strip()\n",
    "    return [x.strip() for x in s.split(\";\") if x.strip()] if s else []\n",
    "\n",
    "# Load source datasets\n",
    "psi   = load_src(PSI_PATH,   \"PSI_Biology\")\n",
    "nesg  = load_src(NESG_PATH,  \"NESG\")\n",
    "price = load_src(PRICE_PATH, \"Price\")\n",
    "\n",
    "# Load clusters (safe)\n",
    "cl = pd.read_csv(CLUSTERS_PATH, dtype=str).fillna(\"\")\n",
    "for col in [\"psi_ids\",\"nesg_ids\",\"price_ids\"]:\n",
    "    if col not in cl.columns:\n",
    "        raise ValueError(f\"{CLUSTERS_PATH} missing {col}\")\n",
    "\n",
    "# Extract IDs that appear anywhere in the cluster file\n",
    "psi_in   = set(x for ids in cl[\"psi_ids\"]   for x in split_ids(ids))\n",
    "nesg_in  = set(x for ids in cl[\"nesg_ids\"]  for x in split_ids(ids))\n",
    "price_in = set(x for ids in cl[\"price_ids\"] for x in split_ids(ids))\n",
    "\n",
    "# Identify proteins missing from clusters\n",
    "psi_missing   = psi[~psi[\"sid\"].isin(psi_in)]\n",
    "nesg_missing  = nesg[~nesg[\"sid\"].isin(nesg_in)]\n",
    "price_missing = price[~price[\"sid\"].isin(price_in)]\n",
    "\n",
    "missing_all = pd.concat([psi_missing, nesg_missing, price_missing], ignore_index=True)\n",
    "missing_all[\"reason\"] = \"not_in_crossmatch_clusters\"\n",
    "\n",
    "# Load all proteins that *were* in clusters but not crossmatched (n_datasets==1)\n",
    "if \"n_datasets\" in cl.columns:\n",
    "    cl[\"n_datasets_int\"] = pd.to_numeric(cl[\"n_datasets\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    solo = cl[cl[\"n_datasets_int\"] == 1].copy()\n",
    "\n",
    "    def collect(col, ds_name):\n",
    "        rows = []\n",
    "        for _, r in solo.iterrows():\n",
    "            for sid in split_ids(r[col]):\n",
    "                if sid:\n",
    "                    rows.append({\"dataset\": ds_name, \"sid\": sid, \"fasta\": None, \"reason\": \"not_crossmatched\"})\n",
    "        return rows\n",
    "\n",
    "    notx = pd.DataFrame(\n",
    "        collect(\"psi_ids\", \"PSI_Biology\") +\n",
    "        collect(\"nesg_ids\", \"NESG\") +\n",
    "        collect(\"price_ids\", \"Price\")\n",
    "    )\n",
    "else:\n",
    "    notx = pd.DataFrame(columns=[\"dataset\",\"sid\",\"fasta\",\"reason\"])\n",
    "\n",
    "# Merge: both not_crossmatched + truly missing\n",
    "final_df = pd.concat([notx, missing_all], ignore_index=True)\n",
    "final_df = final_df.sort_values([\"dataset\",\"sid\"]).reset_index(drop=True)\n",
    "\n",
    "# Re-attach sequences for all entries from source datasets\n",
    "all_src = pd.concat([psi, nesg, price])\n",
    "final_df = final_df.merge(all_src, on=[\"dataset\",\"sid\"], how=\"left\", suffixes=(\"\",\"_src\"))\n",
    "final_df[\"fasta\"] = final_df[\"fasta\"].fillna(final_df[\"fasta_src\"]).dropna()\n",
    "\n",
    "final_df[[\"dataset\",\"sid\",\"fasta\",\"reason\"]].to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# Optional FASTA output\n",
    "if WRITE_FASTA:\n",
    "    with open(FASTA_PATH, \"w\") as fh:\n",
    "        for _, r in final_df.iterrows():\n",
    "            seq = str(r[\"fasta\"])\n",
    "            hdr = f\"{r['dataset']}|{r['sid']}|{r['reason']}\"\n",
    "            fh.write(f\">{hdr}\\n\")\n",
    "            for i in range(0, len(seq), 80):\n",
    "                fh.write(seq[i:i+80] + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(final_df)} entries to {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#NESG\n",
    "\n",
    "#Soluprot train\n",
    "\n",
    "#Soluprot test \n",
    "\n",
    "#PRICE train\n",
    "\n",
    "#PSI solubility\n",
    "\n",
    "#PSI all data 5k \n",
    "\n",
    "#Fireprotdb.sql\n",
    "\n",
    "#Fireprotdb results\n",
    "\n",
    "#Thermomutdb json \n",
    "\n",
    "#Meltome stability\n",
    "\n",
    "#PROTSOL huggingface\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38abf3e9",
   "metadata": {},
   "source": [
    "pickle loading sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9713822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np \n",
    "# Load the pickle file\n",
    "\n",
    "with open(\"all_petase_variants_meanembeddings.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Check what it is\n",
    "print(type(data))\n",
    "\n",
    "# If it's a NumPy array\n",
    "try:\n",
    "    if isinstance(data, np.ndarray):\n",
    "        print(\"Shape:\", data.shape)\n",
    "    else:\n",
    "        print(\"Length:\", len(data))\n",
    "except Exception as e:\n",
    "    print(\"Error checking shape:\", e)\n",
    "\n",
    "print(len(data),len(data[0]),len(data[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0eaee",
   "metadata": {},
   "source": [
    "pickle loading structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../../structures/esm3_struct/embeddings_struct/\")\n",
    "with open(\"wtIsPETase_6ilw_Repair_25_meanembeddings.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Check what it is\n",
    "print(type(data))\n",
    "\n",
    "# If it's a NumPy array\n",
    "try:\n",
    "    if isinstance(data, np.ndarray):\n",
    "        print(\"Shape:\", data.shape)\n",
    "    else:\n",
    "        print(\"Length:\", len(data))\n",
    "except Exception as e:\n",
    "    print(\"Error checking shape:\", e)\n",
    "\n",
    "print(len(data[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab303d55",
   "metadata": {},
   "source": [
    "fetch esm2 seq embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a188b",
   "metadata": {},
   "source": [
    "Fetch sequence embedding\n",
    "This endpoint returns the ESM2 embedding vector for any protein ID that begins with MGYP. The embedding vector is obtained by averaging the final layer activations of the ESM2 transformer model over the sequence length. The model used is esm2_t36_3B_UR50D(), which underlies ESMFold v0 and v1, and produces a 2560-dimensional vector. You can pick any MGnify protein id from the MGnify protein sequence database. There are two versions of this endpoint: fetchEmbedding/ESM2/:id.json returns json format, while fetchEmbedding/ESM2/:id.bin returns a binary vector.\n",
    "\n",
    "URL /fetchEmbedding/ESM2/:id.json\n",
    "\n",
    "Method: GET\n",
    "\n",
    "URL Params: id=[string]\n",
    "\n",
    "Success Response:\n",
    "\n",
    "Code: 200 <br />\n",
    "Content: {\"embedding\": [...], \"id\": MGYP}\n",
    "Sample Call:\n",
    "\n",
    "curl \"https://api.esmatlas.com/fetchEmbedding/ESM2/MGYP003323900000.json\"\n",
    "URL /fetchEmbedding/ESM2/:id.bin\n",
    "\n",
    "Method: GET\n",
    "\n",
    "URL Params: id=[string]\n",
    "\n",
    "Success Response:\n",
    "\n",
    "Code: 200 <br />\n",
    "Content-Type: application/octet-stream\n",
    "Sample Call:\n",
    "\n",
    "curl -H \"Accept: application/octet-stream\" -O https://api.esmatlas.com/fetchEmbedding/ESM2/MGYP003323900000.bin\n",
    "# Read the file in python, using numpy:\n",
    "x = np.fromfile('MGYP003323900000.bin', dtype=np.float16)\n",
    "Using python, get the embedding directly into a numpy array using the requests library:\n",
    "\n",
    "url = \"https://api.esmatlas.com/fetchEmbedding/ESM2/MGYP003323900000.bin\"\n",
    "header = {\"Accept\": \"application/octet-stream\"}\n",
    "response = requests.get(url, headers=header)\n",
    "array = np.frombuffer(response.content, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422708dc",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "[TO DO]\n",
    "TmPredictor from Jinyuansun (novozymes comp) \n",
    "---------------------------------------\n",
    "https://www.kaggle.com/code/jinyuansun/eda-and-finetune-esm\n",
    "\n",
    "novozyme cont. https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/355209\n",
    "\n",
    "---------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TmPredictor(nn.Module):\n",
    "    def __init__(self, esm_model):\n",
    "        super().__init__()\n",
    "        self.encoder = esm_model\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(1280,1280),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1280,640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 1)\n",
    "        )\n",
    "    def forward(self, seq):\n",
    "        res = self.encoder(seq, repr_layers=[33], return_contacts=False)\n",
    "        rep = res['representations'][33]\n",
    "        rep = rep.mean(1)\n",
    "        y = self.predictor(rep)\n",
    "        return y\n",
    "    \n",
    "net = TmPredictor(esm_model).to(device)\n",
    "cal_mae = MeanAbsoluteError().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "train_mae = []\n",
    "test_mae = []\n",
    "test_rho = []\n",
    "for epoch in range(4):\n",
    "    for data in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        seqs, pH, tm = data\n",
    "        seqs = torch.tensor(seqs).long().to(device)\n",
    "        tm = torch.tensor(tm.reshape(-1,1)).float().to(device)\n",
    "        pred_tm = net(seqs)\n",
    "        loss = torch.nn.functional.l1_loss(pred_tm, tm)\n",
    "        mae = cal_mae(pred_tm, tm)\n",
    "        train_mae.append(mae)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        pred_all = []\n",
    "        gt_all = []\n",
    "        for data in tqdm(test_loader):\n",
    "            seqs, pH, tm = data\n",
    "            seqs = torch.tensor(seqs).long().to(device)\n",
    "            tm = torch.tensor(tm.reshape(-1,1)).float().to(device)\n",
    "            pred_tm = net(seqs)\n",
    "            mae = cal_mae(pred_tm, tm)\n",
    "            test_mae.append(mae)\n",
    "            pred_all += pred_tm.ravel().cpu().numpy().tolist()\n",
    "            gt_all += tm.ravel().cpu().numpy().tolist()\n",
    "        rho = stats.spearmanr(gt_all, pred_all)[0]\n",
    "        test_rho.append(rho)\n",
    "    print(f\"Epoch: {epoch + 1}| Train MAE = {train_mae[-1]}| Test MAE = {test_mae[-1]}| Spearmanr = {rho}\")\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot(data=torch.Tensor(train_mae).numpy())\n",
    "test_df = pd.read_csv(\"../input/novozymes-enzyme-stability-prediction/test.csv\")\n",
    "\n",
    "    pred_tm = []\n",
    "for i in range(0, len(test_df), 32):\n",
    "    x = test_df.protein_sequence[i: i+32].apply(convert)\n",
    "    x = np.array([i for i in x])\n",
    "    with torch.no_grad():\n",
    "        pred = net(torch.tensor(x).to(device))\n",
    "        pred_tm = pred_tm + pred.cpu().ravel().numpy().tolist()\n",
    "sns.histplot(pred_tm)\n",
    "test_df.insert(4, 'tm', pred_tm)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d98218",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "[TO DO]\n",
    "ESM-1v https://github.com/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb \n",
    "---------------------------------------\n",
    "\n",
    "This tutorial demonstrates how to train a simple variant predictor, i.e. we predict the biological activity of mutations of a protein, using fixed embeddings from ESM. You can adopt a similar protocol to train a model for any downstream task, even with limited data.\n",
    "\n",
    "We will use a simple classifier in sklearn (or \"head\" on top of the transformer features) to predict the mutation effect from precomputed ESM embeddings. The embeddings for your dataset can be dumped once using a GPU. Then, the rest of your analysis can be done on CPU.\n",
    "\n",
    "In this particular example, we will train a model to predict the activity of ß-lactamase variants.\n",
    "\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53fbab",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "[TO DO] CAFA ESM2 fine-tuning (from naity) https://github.com/naity/finetune-esm\n",
    "---------------------------------------\n",
    "\n",
    "In this example, we will finetune ESM-2 for the CAFA 5 Protein Function Prediction Challenge to predict the biological function of a protein based on its primary sequence. I have already preprocessed the data and formatted the problem as a multi-class, multi-label problem. This means that for a given protein sequence, we will predict whether it is positive for each of the 100 preselected Gene Ontology (GO) terms. Thus, the target for each protein sequence is a binary vector with a length of 100.\n",
    "\n",
    "The processed datasets can be downloaded from here. Details about the preprocessing steps can be found in the notebooks/cafa5_data_processing.ipynb notebook.\n",
    "\n",
    "Run the following example command to finetune ESM-2 models with the processed datasets. Here, we are using the smallest model esm2_t6_8M_UR50D with 1 GPU and the LoRA approach. If you want to finetune a larger model and have multiple GPUs, please adjust num_workers and/or num-devices accordingly.\n",
    "\n",
    "python finetune_esm/train.py \\\n",
    "  --experiment-name esm2_t6_8M_UR50D_lora \\\n",
    "  --dataset-loc data/cafa5/top100_train_split.parquet \\\n",
    "  --targets-loc data/cafa5/train_bp_top100_targets.npy \\\n",
    "  --esm-model esm2_t6_8M_UR50D \\\n",
    "  --num-workers 1 \\\n",
    "  --num-devices 1 \\\n",
    "  --training-mode lora \\\n",
    "  --learning-rate 0.0001 \\\n",
    "  --num-epochs 5\n",
    "\n",
    "mlflow server --host 127.0.0.1 --port 8080 --backend-store-uri ./finetune_results/mlflow\n",
    "\n",
    "*note: model = FinetuneESM(esm_model=esm_model, dropout_p=dropout_p, num_classes=num_classes)\n",
    "\n",
    "\t•\tFinetuneESM wraps a pretrained ESM-2 transformer from Hugging Face.\n",
    "\t•\tIt replaces the final classification head with a new one sized to your task (num_classes=100 for CAFA).\n",
    "\t•\tThe pretrained weights of ESM are loaded as the starting point.\n",
    "\n",
    "If you want distributed scaling or MLflow logging:\n",
    "\n",
    "\t•\tReplace FinetuneESM with RegressionHead\n",
    "\t•\tRemove tokenizer, collate_fn, and ESM loading\n",
    "\t•\tPass your pre-embedded arrays instead of sequences\n",
    "\t•\tSwitch the loss to MSELoss (for regression) or BCEWithLogitsLoss (if binary 0/1 classification)\n",
    "\n",
    "Example of this applied to our 150 PETase dataset:\n",
    "\n",
    "---------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e7c3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3373901559.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python finetune_esm/train.py \\\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_seq = np.load(\"esm3_seq_embeddings.npy\")\n",
    "X_struct = np.load(\"esm3_struct_embeddings.npy\")\n",
    "X = np.concatenate([X_seq, X_struct], axis=1)  # shape (150, 2560)\n",
    "y = np.load(\"labels_Tm.npy\")  # or solubility\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "\n",
    "class RegressionHead(L.LightningModule):\n",
    "    def __init__(self, input_dim=2560, hidden_dim=512, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # output between 0–1\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                         torch.tensor(y_train, dtype=torch.float32))\n",
    "val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                       torch.tensor(y_val, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "model = RegressionHead(input_dim=X.shape[1], lr=1e-4)\n",
    "trainer = L.Trainer(max_epochs=100, accelerator=\"gpu\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b7621",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a4aff",
   "metadata": {},
   "source": [
    "[TO DO] OBTAIN FUNCTIONAL ANNOTAITON OF PETASE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cad338",
   "metadata": {},
   "source": [
    "[TO DO] MLP HEAD ON PETASE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b86f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
