{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaeb4ffa",
   "metadata": {},
   "source": [
    "# ** 2.Loading external stability and expression datasets ** \n",
    "\n",
    "**NESG Solubility** \n",
    "(https://loschmidt.chemi.muni.cz/soluprot/?page=download)\n",
    "* 10k proteins\n",
    "* Labels: exp, sol, uniprot id or local ID \n",
    "* Units: integer \n",
    "\n",
    "**Soluprot Solubility**\n",
    "(https://loschmidt.chemi.muni.cz/soluprot/?page=download)\n",
    "* 11k training, 3k test\n",
    "* Label: solubility, number IDs with no conversion map (has seq)\n",
    "* Unit: 0/1\n",
    "\n",
    "**Price Solubility**\n",
    "(https://pmc.ncbi.nlm.nih.gov/articles/PMC3372292/)\n",
    "* 7k proteins \n",
    "* Label: usability. uniprot id\n",
    "* Unit: 0/1\n",
    "\n",
    "**PSI Solubility** \n",
    "(https://academic.oup.com/bioinformatics/article/36/18/4691/5860015?login=false)\n",
    "* 11k proteins\n",
    "* Label: solubility, Aa0000 ID scheme (has seq)\n",
    "* Unit: 0/1\n",
    "* Note: ecoli with custom IDs, dropped for now \n",
    "\n",
    "**Meltome Stability** \n",
    "(https://meltomeatlas.proteomics.wzw.tum.de/master_meltomeatlasapp/)\n",
    "* 1M variants \n",
    "* Label: temperature, meltpoint, fold_change, uniprot id \n",
    "* Note: ecoli with custom IDs, dropped for now  \n",
    "\n",
    "**FireprotDB Stability** \n",
    "(https://loschmidt.chemi.muni.cz/fireprotdb/)\n",
    "* 53k variants\n",
    "* Label: ddG, dTm, pH, Tm, mutation_effect, uniprot id \n",
    "\n",
    "**ThermomutDB Stability**\n",
    "(https://biosig.lab.uq.edu.au/thermomutdb/downloads)\n",
    "* 12k variants\n",
    "* Label: pH, ddG, temperature, dTm, uniprot/pdb id \n",
    "* Note: these genes were not retrieved from the database due to removal from uniprotkb:  A0A410ZNC6 D0WVP7 G7LSK3 GQ884175 M5A5Y8 Q9REI6\n",
    "\n",
    "**CAFA** \n",
    "(https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/code)\n",
    "* 142k variants\n",
    "\n",
    "**Novozyme**\n",
    "(https://www.kaggle.com/code/jinyuansun/eda-and-finetune-esm)\n",
    "* 31k variants\n",
    "\n",
    "**Protsol Solubility**\n",
    "(https://huggingface.co/datasets/AI4Protein/ProtSolM)\n",
    "* 71k proteins\n",
    "* Label: solubility, no ID but has sequence\n",
    "* Unit: 0/1 \n",
    "\n",
    "**MaveDB** \n",
    "- https://mavedb.org/search?target-organism-name=Escherichia+coli+K-12 \n",
    "\n",
    "**ProteinGym**\n",
    "-  \n",
    "\n",
    "**Align2023**\n",
    "- there is 4 enzymes with train.csv datasets in each folder, they all have mutation codes, some are multi- so will need to only get the single mutation code ones. they all have the sequence we will only look at beta-glucosidaseB and alphaamylase. \n",
    "\n",
    "**Prothermdb** \n",
    "- sent the email for access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84c185",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No input dataframe found. Either set INTEGRATED_PATH or ensure your dataset DFs are loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 186\u001b[39m\n\u001b[32m    183\u001b[39m             dfs.append((name, \u001b[38;5;28mglobals\u001b[39m()[name]))\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo input dataframe found. Either set INTEGRATED_PATH or ensure your dataset DFs are loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# --- build unified pair table ---\u001b[39;00m\n\u001b[32m    189\u001b[39m pairs = pd.concat([build_pairs_from_df(df, src) \u001b[38;5;28;01mfor\u001b[39;00m src, df \u001b[38;5;129;01min\u001b[39;00m dfs], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: No input dataframe found. Either set INTEGRATED_PATH or ensure your dataset DFs are loaded."
     ]
    }
   ],
   "source": [
    "# benchmark.ipynb — Build unified WT–mutant pair table with explicit per-dataset input paths\n",
    "# Edit DATASET_INPUTS to point to your local files (csv/tsv/parquet/xlsx/jsonl).\n",
    "# Outputs:\n",
    "#   data/benchmark/derived/benchmark_wt_mutant_pairs.tsv\n",
    "#   data/benchmark/derived/benchmark_wt_parents.tsv\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = Path(\"data/benchmark/derived\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) EDIT THESE INPUT PATHS\n",
    "# ----------------------------\n",
    "# Each entry can be:\n",
    "#   - a file path string\n",
    "#   - a Path\n",
    "#   - a glob string (e.g. \"align2023/**/train.csv\")\n",
    "#   - a list of any of the above\n",
    "#\n",
    "# kind:\n",
    "#   - \"variant\": dataset contains mutations and/or mutant sequences (WT↔mutant pairs)\n",
    "#   - \"protein\": dataset is protein-level only (no mutants); becomes WT-only rows\n",
    "DATASET_INPUTS = [\n",
    "    # Variant datasets\n",
    "    {\"name\": \"FireProtDB\",  \"kind\": \"variant\", \"paths\": [\"data/benchmark/fireprotdb/*.tsv\", \"data/benchmark/fireprotdb/*.csv\"]},\n",
    "    {\"name\": \"ThermoMutDB\", \"kind\": \"variant\", \"paths\": [\"data/benchmark/thermomutdb/*.tsv\", \"data/benchmark/thermomutdb/*.csv\"]},\n",
    "    {\"name\": \"Align2023\",   \"kind\": \"variant\", \"paths\": [\"data/benchmark/align2023/**/train.csv\"]},\n",
    "    {\"name\": \"ProteinGym\",  \"kind\": \"variant\", \"paths\": [\"data/benchmark/proteingym/**/*.csv\", \"data/benchmark/proteingym/**/*.tsv\", \"data/benchmark/proteingym/**/*.parquet\"]},\n",
    "    {\"name\": \"MaveDB\",      \"kind\": \"variant\", \"paths\": [\"data/benchmark/mavedb/**/*.csv\", \"data/benchmark/mavedb/**/*.tsv\", \"data/benchmark/mavedb/**/*.parquet\"]},\n",
    "    {\"name\": \"CAFA5\",       \"kind\": \"variant\", \"paths\": [\"data/benchmark/cafa5/**/*.csv\", \"data/benchmark/cafa5/**/*.tsv\", \"data/benchmark/cafa5/**/*.parquet\"]},\n",
    "    {\"name\": \"Novozyme\",    \"kind\": \"variant\", \"paths\": [\"data/benchmark/novozymes/**/*.csv\", \"data/benchmark/novozymes/**/*.tsv\", \"data/benchmark/novozymes/**/*.parquet\"]},\n",
    "\n",
    "    # Protein-level datasets (WT-only rows)\n",
    "    {\"name\": \"NESG_Sol\",    \"kind\": \"protein\", \"paths\": [\"data/benchmark/nesg/*.tsv\", \"data/benchmark/nesg/*.csv\"]},\n",
    "    {\"name\": \"SoluProt\",    \"kind\": \"protein\", \"paths\": [\"data/benchmark/soluprot/*.tsv\", \"data/benchmark/soluprot/*.csv\"]},\n",
    "    {\"name\": \"ProtSolM\",    \"kind\": \"protein\", \"paths\": [\"data/benchmark/protsol/*.parquet\", \"data/benchmark/protsol/*.csv\", \"data/benchmark/protsol/*.tsv\"]},\n",
    "    {\"name\": \"Price_Sol\",   \"kind\": \"protein\", \"paths\": [\"data/benchmark/price_sol/*.tsv\", \"data/benchmark/price_sol/*.csv\"]},\n",
    "    {\"name\": \"PSI_Sol\",     \"kind\": \"protein\", \"paths\": [\"data/benchmark/psi_sol/*.tsv\", \"data/benchmark/psi_sol/*.csv\"]},\n",
    "]\n",
    "\n",
    "# Optional curated WT mapping file to override inference for variant datasets\n",
    "# Columns: protein_id\\twt_seq\n",
    "WT_MAP_PATH = Path(\"data/benchmark/wt_map.tsv\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) HELPERS\n",
    "# ----------------------------\n",
    "def read_any(path: Path) -> pd.DataFrame:\n",
    "    suf = path.suffix.lower()\n",
    "    if suf in [\".tsv\", \".tab\"]:\n",
    "        return pd.read_csv(path, sep=\"\\t\")\n",
    "    if suf == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    if suf == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if suf == \".jsonl\":\n",
    "        return pd.read_json(path, lines=True)\n",
    "    if suf in [\".xlsx\", \".xls\"]:\n",
    "        return pd.read_excel(path)\n",
    "    raise ValueError(f\"Unsupported file: {path}\")\n",
    "\n",
    "def canon_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip().str.lower()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "        .str.replace(\"(\", \"\", regex=False).str.replace(\")\", \"\", regex=False)\n",
    "        .str.replace(\"?\", \"\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def pick_col(cols, candidates):\n",
    "    cols_l = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_l:\n",
    "            return cols_l[cand.lower()]\n",
    "    for cand in candidates:\n",
    "        for c in cols:\n",
    "            if cand.lower() in c.lower():\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def normalize_mut_str(s) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    if s in [\"\", \"WT\", \"wt\", \"wildtype\", \"Wildtype\", \"wild-type\", \"WILD_TYPE\", \"0\", \"nan\", \"None\"]:\n",
    "        return \"\"\n",
    "    s = s.replace(\",\", \";\").replace(\"|\", \";\").replace(\"/\", \";\").replace(\":\", \";\")\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "    s = re.sub(r\";{2,}\", \";\", s).strip(\";\")\n",
    "    return s\n",
    "\n",
    "def split_muts(muts: str) -> list[str]:\n",
    "    muts = normalize_mut_str(muts)\n",
    "    return [m for m in muts.split(\";\") if m] if muts else []\n",
    "\n",
    "AA1 = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "_mut_re = re.compile(r\"^([ACDEFGHIKLMNPQRSTVWY])(\\d+)([ACDEFGHIKLMNPQRSTVWY])$\")\n",
    "\n",
    "def diff_to_mutations(wt: str, mut: str) -> str:\n",
    "    if not isinstance(wt, str) or not isinstance(mut, str) or not wt or not mut or len(wt) != len(mut):\n",
    "        return \"\"\n",
    "    out = []\n",
    "    for i, (a, b) in enumerate(zip(wt, mut), start=1):\n",
    "        if a != b and (a in AA1) and (b in AA1):\n",
    "            out.append(f\"{a}{i}{b}\")\n",
    "    return \";\".join(out)\n",
    "\n",
    "def apply_mutations(wt_seq: str, muts: list[str]):\n",
    "    if not isinstance(wt_seq, str) or not wt_seq:\n",
    "        return \"\", False, 0, len(muts)\n",
    "    seq = list(wt_seq)\n",
    "    mismatch = 0\n",
    "    invalid = 0\n",
    "    for m in muts:\n",
    "        mm = _mut_re.match(m.strip())\n",
    "        if not mm:\n",
    "            invalid += 1\n",
    "            continue\n",
    "        a0, pos_s, a1 = mm.group(1), mm.group(2), mm.group(3)\n",
    "        pos = int(pos_s)\n",
    "        if pos < 1 or pos > len(seq):\n",
    "            invalid += 1\n",
    "            continue\n",
    "        if seq[pos - 1] != a0:\n",
    "            mismatch += 1\n",
    "            continue\n",
    "        seq[pos - 1] = a1\n",
    "    ok = (invalid == 0) and (mismatch == 0)\n",
    "    return \"\".join(seq), ok, mismatch, invalid\n",
    "\n",
    "def infer_wt_seq(df: pd.DataFrame, protein_id_col: str, observed_seq_col: str, muts_col: str | None) -> dict:\n",
    "    wt_map = {}\n",
    "    if not protein_id_col or not observed_seq_col:\n",
    "        return wt_map\n",
    "    gcols = [protein_id_col, observed_seq_col] + ([muts_col] if muts_col else [])\n",
    "    d = df[gcols].copy()\n",
    "    if muts_col:\n",
    "        d[\"_muts_norm\"] = d[muts_col].map(normalize_mut_str)\n",
    "    for pid, g in d.groupby(protein_id_col, dropna=True):\n",
    "        pid = str(pid)\n",
    "        if muts_col:\n",
    "            wt_rows = g[g[\"_muts_norm\"].eq(\"\")]\n",
    "            if len(wt_rows) > 0:\n",
    "                seqs = wt_rows[observed_seq_col].dropna().astype(str)\n",
    "                if len(seqs) > 0:\n",
    "                    wt_map[pid] = seqs.value_counts().idxmax()\n",
    "                    continue\n",
    "        seqs = g[observed_seq_col].dropna().astype(str)\n",
    "        if len(seqs) > 0:\n",
    "            wt_map[pid] = seqs.value_counts().idxmax()\n",
    "    return wt_map\n",
    "\n",
    "def infer_label_col(df: pd.DataFrame):\n",
    "    cols = list(df.columns)\n",
    "    return pick_col(cols, [\n",
    "        \"label\",\"y\",\"fitness\",\"score\",\"assay_value\",\"assay\",\n",
    "        \"ddg\",\"dtm\",\"tm\",\"solubility\",\"sol\",\"exp\",\"expression\",\"temperature\"\n",
    "    ])\n",
    "\n",
    "def expand_paths(paths):\n",
    "    if isinstance(paths, (str, Path)):\n",
    "        paths = [paths]\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if any(ch in str(p) for ch in [\"*\", \"?\", \"[\"]):\n",
    "            out.extend(sorted(Path().glob(str(p))))\n",
    "        else:\n",
    "            if p.exists():\n",
    "                out.append(p)\n",
    "    return out\n",
    "\n",
    "# WT override map\n",
    "WT_MAP = {}\n",
    "if WT_MAP_PATH.exists():\n",
    "    wm = pd.read_csv(WT_MAP_PATH, sep=\"\\t\")\n",
    "    WT_MAP = dict(zip(wm[\"protein_id\"].astype(str), wm[\"wt_seq\"].astype(str)))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) NORMALIZE ONE FILE\n",
    "# ----------------------------\n",
    "def build_pairs_from_df(df: pd.DataFrame, source_dataset: str, kind: str) -> pd.DataFrame:\n",
    "    df = canon_cols(df)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    protein_id_col = pick_col(cols, [\"protein_id\",\"uniprot_id\",\"uniprot\",\"accession\",\"acc\",\"pdb_id\",\"pdb\",\"target_id\",\"gene\",\"name\",\"id\"])\n",
    "    variant_id_col = pick_col(cols, [\"variant_id\",\"mutant_id\",\"mutation_id\",\"sample_id\",\"entry_id\",\"seq_id\",\"name\",\"id\"])\n",
    "\n",
    "    wt_seq_col  = pick_col(cols, [\"wt_seq\",\"wildtype_sequence\",\"wt_sequence\",\"sequence_wt\"])\n",
    "    mut_seq_col = pick_col(cols, [\"mut_seq\",\"mutant_sequence\",\"variant_sequence\",\"sequence_mut\"])\n",
    "    seq_col     = pick_col(cols, [\"sequence\",\"protein_sequence\",\"aa_seq\",\"fasta\",\"seq\"])\n",
    "\n",
    "    muts_col = pick_col(cols, [\"mutations\",\"mutation_code\",\"mutation\",\"mut\",\"variant\",\"aa_substitutions\"])\n",
    "\n",
    "    label_col = infer_label_col(df)\n",
    "\n",
    "    if protein_id_col is None:\n",
    "        df[\"protein_id\"] = source_dataset\n",
    "        protein_id_col = \"protein_id\"\n",
    "    if variant_id_col is None:\n",
    "        df[\"variant_id\"] = [f\"{source_dataset}:{i}\" for i in range(len(df))]\n",
    "        variant_id_col = \"variant_id\"\n",
    "\n",
    "    if kind == \"protein\":\n",
    "        use_seq = wt_seq_col or seq_col or mut_seq_col\n",
    "        seqs = df[use_seq].astype(str) if use_seq else pd.Series([\"\"] * len(df))\n",
    "        out = pd.DataFrame({\n",
    "            \"source_dataset\": source_dataset,\n",
    "            \"protein_id\": df[protein_id_col].astype(str),\n",
    "            \"variant_id\": df[variant_id_col].astype(str),\n",
    "            \"wt_seq\": seqs,\n",
    "            \"mut_seq\": seqs,\n",
    "            \"mutations\": \"\",\n",
    "            \"n_mut\": 0,\n",
    "            \"label\": df[label_col] if label_col else np.nan,\n",
    "            \"label_col\": label_col or \"\",\n",
    "            \"mut_apply_ok\": True,\n",
    "            \"mut_apply_mismatch_n\": 0,\n",
    "            \"mut_apply_invalid_n\": 0,\n",
    "        })\n",
    "        return out\n",
    "\n",
    "    # kind == \"variant\"\n",
    "    observed_seq_col = mut_seq_col or wt_seq_col or seq_col\n",
    "    if observed_seq_col is None and muts_col is None:\n",
    "        raise ValueError(f\"{source_dataset}: need at least a sequence column or mutations column\")\n",
    "\n",
    "    # Base sequences\n",
    "    wt_seq = df[wt_seq_col].astype(str) if wt_seq_col else pd.Series([\"\"] * len(df))\n",
    "    mut_seq = df[mut_seq_col].astype(str) if mut_seq_col else (df[seq_col].astype(str) if seq_col else pd.Series([\"\"] * len(df)))\n",
    "    muts_raw = df[muts_col].map(normalize_mut_str) if muts_col else pd.Series([\"\"] * len(df))\n",
    "\n",
    "    # WT inference per protein_id if wt_seq missing\n",
    "    local_wt_map = {}\n",
    "    if (wt_seq_col is None) and (observed_seq_col is not None):\n",
    "        local_wt_map = infer_wt_seq(df, protein_id_col, observed_seq_col, muts_col)\n",
    "\n",
    "    def get_wt(pid: str) -> str:\n",
    "        pid = str(pid)\n",
    "        if pid in WT_MAP:\n",
    "            return WT_MAP[pid]\n",
    "        if pid in local_wt_map:\n",
    "            return local_wt_map[pid]\n",
    "        return \"\"\n",
    "\n",
    "    wt_out, mut_out, ok_out, mm_out, inv_out, nmut_out, muts_out = [], [], [], [], [], [], []\n",
    "    for pid, w, m, mstr in zip(df[protein_id_col].astype(str), wt_seq, mut_seq, muts_raw):\n",
    "        wt = w if (isinstance(w, str) and w) else get_wt(pid)\n",
    "        mut = m if (isinstance(m, str) and m) else \"\"\n",
    "\n",
    "        mlist = split_muts(mstr)\n",
    "        # If we have both sequences with equal length, derive mutation codes (more reliable than some dataset strings)\n",
    "        if wt and mut and len(wt) == len(mut):\n",
    "            derived = diff_to_mutations(wt, mut)\n",
    "            if derived:\n",
    "                mlist = split_muts(derived)\n",
    "                mstr = derived\n",
    "\n",
    "        # If we have WT + mutation codes but mut seq missing or equals WT, synthesize mut seq\n",
    "        if wt and mlist and (not mut or mut == wt):\n",
    "            newseq, ok, mmc, invc = apply_mutations(wt, mlist)\n",
    "            mut = newseq if newseq else mut\n",
    "        else:\n",
    "            ok, mmc, invc = (True if (not mlist) else False), 0, 0\n",
    "\n",
    "        # WT row normalization\n",
    "        if not mlist:\n",
    "            mut = wt if wt else mut\n",
    "\n",
    "        wt_out.append(wt)\n",
    "        mut_out.append(mut)\n",
    "        ok_out.append(ok)\n",
    "        mm_out.append(mmc)\n",
    "        inv_out.append(invc)\n",
    "        muts_out.append(mstr)\n",
    "        nmut_out.append(len(mlist))\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"source_dataset\": source_dataset,\n",
    "        \"protein_id\": df[protein_id_col].astype(str),\n",
    "        \"variant_id\": df[variant_id_col].astype(str),\n",
    "        \"wt_seq\": wt_out,\n",
    "        \"mut_seq\": mut_out,\n",
    "        \"mutations\": muts_out,\n",
    "        \"n_mut\": nmut_out,\n",
    "        \"label\": df[label_col] if label_col else np.nan,\n",
    "        \"label_col\": label_col or \"\",\n",
    "        \"mut_apply_ok\": ok_out,\n",
    "        \"mut_apply_mismatch_n\": mm_out,\n",
    "        \"mut_apply_invalid_n\": inv_out,\n",
    "    })\n",
    "\n",
    "    # Keep only single mutants for Align2023 if desired\n",
    "    if source_dataset.lower().startswith(\"align2023\"):\n",
    "        out = out[out[\"n_mut\"].isin([0, 1])].reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 4) RUN INGEST + EXPORT\n",
    "# ----------------------------\n",
    "all_rows = []\n",
    "manifest = []\n",
    "\n",
    "for spec in DATASET_INPUTS:\n",
    "    ds_name = spec[\"name\"]\n",
    "    ds_kind = spec[\"kind\"]\n",
    "    files = expand_paths(spec[\"paths\"])\n",
    "\n",
    "    if not files:\n",
    "        manifest.append({\"dataset\": ds_name, \"kind\": ds_kind, \"files_found\": 0, \"note\": \"no files matched\"})\n",
    "        continue\n",
    "\n",
    "    for fp in files:\n",
    "        try:\n",
    "            df = read_any(fp)\n",
    "            norm = build_pairs_from_df(df, source_dataset=ds_name, kind=ds_kind)\n",
    "            norm[\"source_file\"] = str(fp)\n",
    "            all_rows.append(norm)\n",
    "            manifest.append({\"dataset\": ds_name, \"kind\": ds_kind, \"file\": str(fp), \"rows_in\": len(df), \"rows_out\": len(norm)})\n",
    "        except Exception as e:\n",
    "            manifest.append({\"dataset\": ds_name, \"kind\": ds_kind, \"file\": str(fp), \"error\": repr(e)})\n",
    "\n",
    "pairs = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame(columns=[\n",
    "    \"source_dataset\",\"protein_id\",\"variant_id\",\"wt_seq\",\"mut_seq\",\"mutations\",\"n_mut\",\"label\",\"label_col\",\n",
    "    \"mut_apply_ok\",\"mut_apply_mismatch_n\",\"mut_apply_invalid_n\",\"source_file\"\n",
    "])\n",
    "\n",
    "# QC flags\n",
    "pairs[\"has_wt_seq\"] = pairs[\"wt_seq\"].astype(str).str.len().gt(0)\n",
    "pairs[\"has_mut_seq\"] = pairs[\"mut_seq\"].astype(str).str.len().gt(0)\n",
    "pairs[\"len_match\"]  = pairs[\"has_wt_seq\"] & pairs[\"has_mut_seq\"] & (pairs[\"wt_seq\"].astype(str).str.len() == pairs[\"mut_seq\"].astype(str).str.len())\n",
    "pairs[\"is_wt_row\"]  = pairs[\"mutations\"].astype(str).eq(\"\") | pairs[\"n_mut\"].astype(int).eq(0)\n",
    "\n",
    "pairs_path = OUT_DIR / \"benchmark_wt_mutant_pairs.tsv\"\n",
    "pairs.to_csv(pairs_path, sep=\"\\t\", index=False)\n",
    "\n",
    "wt_parents = (\n",
    "    pairs[pairs[\"has_wt_seq\"]]\n",
    "    .groupby(\"protein_id\", as_index=False)\n",
    "    .agg(\n",
    "        wt_seq=(\"wt_seq\", lambda x: x.value_counts().idxmax()),\n",
    "        n_variants=(\"variant_id\", \"nunique\"),\n",
    "        sources=(\"source_dataset\", lambda x: \";\".join(sorted(set(map(str, x)))))\n",
    "    )\n",
    ")\n",
    "wt_path = OUT_DIR / \"benchmark_wt_parents.tsv\"\n",
    "wt_parents.to_csv(wt_path, sep=\"\\t\", index=False)\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest)\n",
    "manifest_path = OUT_DIR / \"ingest_manifest.tsv\"\n",
    "manifest_df.to_csv(manifest_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Wrote:\", pairs_path)\n",
    "print(\"Wrote:\", wt_path)\n",
    "print(\"Wrote:\", manifest_path)\n",
    "print(\"pairs rows:\", len(pairs), \"unique proteins:\", pairs[\"protein_id\"].nunique(), \"WT parents:\", len(wt_parents))\n",
    "print(\"missing WT seq rows:\", int((~pairs[\"has_wt_seq\"]).sum()))\n",
    "print(\"length mismatch rows:\", int((pairs[\"has_wt_seq\"] & pairs[\"has_mut_seq\"] & ~pairs[\"len_match\"]).sum()))\n",
    "print(\"mutation apply mismatches:\", int((pairs[\"mut_apply_mismatch_n\"] > 0).sum()), \"invalid muts:\", int((pairs[\"mut_apply_invalid_n\"] > 0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20319fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now load \"masterdb.csv\" found under data\n",
    "import pandas as pd \n",
    "import os \n",
    "path = \"data/masterdb.tsv\"\n",
    "df = pd.read_csv(path,sep=\"\\t\")\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"(\", \"\")\n",
    "    .str.replace(\")\", \"\")\n",
    "    .str.replace(\"?\", \"\")\n",
    ")\n",
    "df2 = pd.DataFrame()\n",
    "df2[\"id\"]=df[\"name\"]\n",
    "df2[\"sequence\"] = df[\"protein_sequence\"].astype(str)\n",
    "print(df2)\n",
    "\n",
    "def dftofasta(df,outfile):\n",
    "    with open(outfile,\"w\") as f:\n",
    "        for index,row in df.iterrows():\n",
    "            f.write(f\">{row['id']}\\n\")\n",
    "            f.write(f\"{row['sequence']}\\n\")\n",
    "    return outfile \n",
    "\n",
    "#dftofasta(df2,\"data/masterdb.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d23197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#setting the paths, loading them into dataframes, and making the merged fasta file to cdhit \n",
    "############################################\n",
    "\n",
    "\n",
    "#PSI_PATH      = \"data/benchmark/sol_benchmark/PSI_Biology_solubility_trainset.csv\"\n",
    "#psi_detail_path = \"data/benchmark/sol_benchmark/PSI_all_data_esol.tab\"\n",
    "NESG_PATH     = \"data/benchmark/sol_benchmark/nesg/nesg.csv\"\n",
    "nesg_fasta_path = \"data/benchmark/sol_benchmark/nesg/nesg.fasta\"\n",
    "PRICE_PATH    = \"data/benchmark/sol_benchmark/Price_usability_trainset.csv\"\n",
    "soluprot_train_path = \"data/benchmark/sol_benchmark/soluprot_data/training_set.csv\"\n",
    "soluprot_test_path = \"data/benchmark/sol_benchmark/soluprot_data/test_set.csv\" \n",
    "soluprot_train_fasta = \"data/benchmark/sol_benchmark/soluprot_data/training_set.fasta\"\n",
    "soluprot_test_fasta = \"data/benchmark/sol_benchmark/soluprot_data/test_set.fasta\" \n",
    "#meltome_path = \"data/benchmark/stab_benchmark/meltome_cross-species.csv\"\n",
    "#meltome_fasta_path = \"data/benchmark/stab_benchmark/meltome_fasta.fasta\"\n",
    "fireprot_path = \"data/benchmark/stab_benchmark/fireprotdb_results_stability.csv\"\n",
    "thermomutdb_path = \"data/benchmark/stab_benchmark/thermomutdb.json\"\n",
    "thermomutdb_fasta = \"data/benchmark/stab_benchmark/thermomutdb.fasta\"\n",
    "protsol_train_path = \"data/benchmark/protsolm_data/protsolm_train.csv\"\n",
    "protsol_test_path = \"data/benchmark/protsolm_data/protsolm_test.csv\"\n",
    "novozyme_test_path = \"data/benchmark/novozymes-enzyme-stability-prediction/test.csv\"\n",
    "novozyme_train_path = \"data/benchmark/novozymes-enzyme-stability-prediction/train.csv\"\n",
    "novozyme_test_labels_path = \"data/benchmark/novozymes-enzyme-stability-prediction/test_labels.csv\"\n",
    "\n",
    "\n",
    "#LOAD DATASETS \n",
    "#psi = load_psi(\"data/benchmark/sol_benchmark/PSI_Biology_solubility_trainset.csv\",\"data/benchmark/sol_benchmark/PSI_all_data_esol.tab\")\n",
    "nesg = load_nesg(NESG_PATH, nesg_fasta_path) #no seq col \n",
    "price = load_price(PRICE_PATH) #fasta \n",
    "soluprot = load_soluprot(\n",
    "    soluprot_train_path,\n",
    "    soluprot_test_path,\n",
    "    soluprot_train_fasta,\n",
    "    soluprot_test_fasta    \n",
    ")\n",
    "fireprot = load_fireprot(fireprot_path) #sequence\n",
    "thermomut = load_thermomut(thermomutdb_path,thermomutdb_fasta)\n",
    "#meltome = load_meltome(meltome_path,meltome_fasta_path) \n",
    "protsolm = load_protsolm(protsol_train_path, protsol_test_path) #aa_seq\n",
    "novozymes = load_novozymes(\n",
    "    novozyme_train_path, novozyme_test_path, novozyme_test_labels_path\n",
    ")\n",
    "\n",
    "#ADD FASTA TO DATAFRAMES IF THE CSV DID NOT HAVE IT\n",
    "nesg_fasta = read_fasta_dict(nesg_fasta_path)\n",
    "nesg[\"sequence\"] = nesg[\"id\"].map(nesg_fasta)\n",
    "price[\"sequence\"] = price[\"fasta\"]\n",
    "protsolm[\"sequence\"] = protsolm[\"aa_seq\"]\n",
    "\n",
    "\n",
    "#MERGE ALL FASTA FILES FOR CD-HIT\n",
    "def fasta_merger_from_dfs(datasets, outpath):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for i, df in enumerate(datasets):\n",
    "            for idx, row in df.iterrows():\n",
    "                seq = row[\"sequence\"]\n",
    "                if seq is None or pd.isna(seq):\n",
    "                    continue\n",
    "                header = f\"{df.__class__.__name__}_{i}_{idx}\"\n",
    "                out.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "datasets = [nesg, price, soluprot, fireprot, protsolm,thermomut, novozymes]\n",
    "fasta_merger_from_dfs(datasets, \"allbenchmarks.fasta\")\n",
    "\n",
    "#SETTING  CANONICAL IDS\n",
    "for i, df in enumerate(datasets):\n",
    "    df[\"canonical_id\"] = [\n",
    "        f\"{df.__class__.__name__}_{i}_{idx}\" for idx in df.index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10537c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#Setting up functions \n",
    "############################################\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "\n",
    "def read_fasta_dict(path: str):\n",
    "    seqs = {}\n",
    "    with open(path) as fh:\n",
    "        for header, seq in SimpleFastaParser(fh):\n",
    "            sid = header.split()[0].strip()\n",
    "            seqs[sid] = seq.strip()\n",
    "    return seqs\n",
    "\n",
    "def load_nesg(csv_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)  # uses CSV header row directly: id, exp, sol\n",
    "    seqs = read_fasta_dict(fasta_path)\n",
    "    #\"sid\" \"usability\" \"fasta\" \n",
    "    df[\"sequence\"] = df[\"id\"].map(seqs)\n",
    "    return df \n",
    "\n",
    "def load_psi(csv_path: str, psi_detail_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)                        # has sid + fasta + labels\n",
    "    psi_all = pd.read_csv(psi_detail_path, sep=\"\\t\")  # extra metadata\n",
    "\n",
    "    # merge on sid\n",
    "    df = df.merge(psi_all, on=\"sid\", how=\"left\")\n",
    "\n",
    "    # sequence is already in the \"fasta\" column\n",
    "    df[\"sequence\"] = df[\"fasta\"]\n",
    "\n",
    "    return df\n",
    "def load_price(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sequence\"] = df[\"fasta\"]\n",
    "    # \"sid\" \"usability\" \"fasta\" \n",
    "    return df\n",
    "\n",
    "def load_soluprot(train_csv: str, test_csv: str,\n",
    "                  train_fasta_path: str, test_fasta_path: str) -> pd.DataFrame:\n",
    "\n",
    "    # load FASTA → dict, keys exactly as in FASTA headers\n",
    "    train_fasta = read_fasta_dict(train_fasta_path)\n",
    "    test_fasta  = read_fasta_dict(test_fasta_path)\n",
    "\n",
    "    # merge FASTA dicts\n",
    "    fasta = {**train_fasta, **test_fasta}\n",
    "\n",
    "    # load CSVs\n",
    "    df1 = pd.read_csv(train_csv)\n",
    "    df2 = pd.read_csv(test_csv)\n",
    "\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # map sequences using the exact ids\n",
    "    df[\"sequence\"] = df[\"sid\"].map(fasta)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_meltome(csv_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # load fasta into dict: {uniprot_id: sequence}\n",
    "    fasta = read_fasta_dict(fasta_path)\n",
    "\n",
    "    # extract uniprot prefix from Protein_ID (before \"_\")\n",
    "    df[\"uniprot_id\"] = df[\"Protein_ID\"].astype(str).apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "    # map sequences\n",
    "    df[\"sequence\"] = df[\"uniprot_id\"].map(fasta)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_fireprot(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # \"uniprot_id\" \"pdb_id\" \"muutation\" \"ddG\" \"dTm\" \"pH\" \"tm\" \"mutation_effect\" \"sequence\"\n",
    "    return df \n",
    "\n",
    "def load_thermomut(json_path: str, fasta_path: str) -> pd.DataFrame:\n",
    "    # load JSON metadata\n",
    "    with open(json_path) as fh:\n",
    "        data = json.load(fh)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # load FASTA sequences\n",
    "    fasta_dict = read_fasta_dict(fasta_path)\n",
    "\n",
    "    # map UniProt → sequence\n",
    "    # JSON column is \"uniprot\"\n",
    "    df[\"sequence\"] = df[\"uniprot\"].map(fasta_dict)\n",
    "\n",
    "    # ensure required labels exist even if missing in JSON\n",
    "    for col in [\"ph\",\"ddg\",\"temperature\",\"dtm\",\"PDB_wild\",\n",
    "                \"pdb_mutant\",\"mutation_code\",\"mutated chain\",\"effect\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_protsolm(train_csv: str, test_csv: str) -> pd.DataFrame:\n",
    "    df1 = pd.read_csv(train_csv)\n",
    "    df2 = pd.read_csv(test_csv)\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    # \"aa_seq\" \"detail\"\n",
    "    return df\n",
    "\n",
    "def fasta_merger(fasta_paths: list, outpath: str):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for path in fasta_paths:\n",
    "            with open(path) as fh:\n",
    "                for line in fh:\n",
    "                    out.write(line)\n",
    "\n",
    "def parse_cd_hit_clusters(clstr_path):\n",
    "    clusters = {}\n",
    "    current = None\n",
    "\n",
    "    with open(clstr_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">Cluster\"):\n",
    "                current = line.split()[1]\n",
    "                clusters[current] = []\n",
    "            else:\n",
    "                # Example: \"0       50aa, >SEQ123... *\"\n",
    "                sid = line.split(\">\")[1].split(\"...\")[0]\n",
    "                clusters[current].append(sid)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def fasta_merger_from_dfs(datasets, outpath):\n",
    "    with open(outpath, \"w\") as out:\n",
    "        for i, df in enumerate(datasets):\n",
    "            for idx, row in df.iterrows():\n",
    "                seq = row[\"sequence\"]\n",
    "                if seq is None or pd.isna(seq):\n",
    "                    continue\n",
    "                header = f\"{df.__class__.__name__}_{i}_{idx}\"\n",
    "                out.write(f\">{header}\\n{seq}\\n\")\n",
    "\n",
    "def fetch_uniprot_fasta(ids, out_fasta, delay=0.15):\n",
    "    \"\"\"Fetch FASTA for many UniProt IDs with error handling.\"\"\"\n",
    "    with open(out_fasta, \"w\") as out:\n",
    "        for uid in ids:\n",
    "            url = f\"https://rest.uniprot.org/uniprotkb/{uid}.fasta\"\n",
    "            r = requests.get(url, timeout=10)\n",
    "\n",
    "            if r.status_code == 200 and r.text.startswith(\">\"):\n",
    "                out.write(r.text.strip() + \"\\n\")\n",
    "            else:\n",
    "                # write placeholder for failed fetch\n",
    "                out.write(f\">{uid}\\nFAILED_FETCH\\n\")\n",
    "\n",
    "            time.sleep(delay)  # rate-limit to avoid 500 errors\n",
    "\n",
    "\n",
    "\n",
    "def load_novozymes(train_path: str, test_path: str, test_labels_path: str) -> pd.DataFrame:\n",
    "    train_df = pd.read_csv(train_path)         # seq_id, protein_sequence, pH, Tm\n",
    "    test_df = pd.read_csv(test_path)           # seq_id, protein_sequence, pH\n",
    "    test_labels = pd.read_csv(test_labels_path)  # seq_id, Tm\n",
    "\n",
    "    # merge test with its labels\n",
    "    test_df = test_df.merge(test_labels, on=\"seq_id\", how=\"left\")\n",
    "\n",
    "    # unify column names to match your other datasets\n",
    "    train_df[\"sequence\"] = train_df[\"protein_sequence\"]\n",
    "    test_df[\"sequence\"]  = test_df[\"protein_sequence\"]\n",
    "\n",
    "    # combine train + test into one dataframe\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm1v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
