{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# PETase Prediction Pipeline - Google Drive Version\n",
        "\n",
        "## ğŸ“ Folder Structure\n",
        "```\n",
        "/content/drive/MyDrive/PET2025/pipeline/\n",
        "â”œâ”€â”€ resources/              â† Input files here\n",
        "â”‚   â”œâ”€â”€ petase_pipeline_ndcg_optimized.py\n",
        "â”‚   â”œâ”€â”€ predictive-pet-zero-shot-test-2025.csv\n",
        "â”‚   â””â”€â”€ pet-2025-wildtype-cds.csv\n",
        "â”œâ”€â”€ submission/             â† Output files here\n",
        "â”‚   â””â”€â”€ (results will appear here)\n",
        "â””â”€â”€ driver.ipynb            â† This notebook\n",
        "```\n",
        "\n",
        "## ğŸ”„ Disconnect Recovery\n",
        "**If you disconnect:**\n",
        "1. Just re-run all cells\n",
        "2. Pipeline will automatically resume from last checkpoint\n",
        "3. All progress is saved to Google Drive\n",
        "\n",
        "## âš¡ Quick Start\n",
        "1. Run Cell 1: Mount Google Drive\n",
        "2. Run Cell 2: Install dependencies\n",
        "3. Run Cell 3: Prepare data\n",
        "4. Run Cell 4: Run pipeline (2-3 hours)\n",
        "5. Run Cell 5: Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# CELL 1: Mount Google Drive\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“‚ Mounting Google Drive...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL 1: Mount Google Drive\n",
        "# ========================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"ğŸ“‚ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"âœ… Google Drive mounted\")\n",
        "\n",
        "# Define paths\n",
        "BASE_DIR = '/content/drive/MyDrive/PET2025/pipeline'\n",
        "RESOURCES_DIR = f'{BASE_DIR}/resources'\n",
        "SUBMISSION_DIR = f'{BASE_DIR}/submission'\n",
        "\n",
        "# Verify directories exist\n",
        "print(\"\\nğŸ” Verifying folder structure...\")\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"âŒ Base directory not found: {BASE_DIR}\")\n",
        "    print(\"   Please create it first!\")\n",
        "else:\n",
        "    print(f\"âœ… Base directory: {BASE_DIR}\")\n",
        "\n",
        "if not os.path.exists(RESOURCES_DIR):\n",
        "    print(f\"âš ï¸  Resources directory not found: {RESOURCES_DIR}\")\n",
        "    print(\"   Creating it...\")\n",
        "    os.makedirs(RESOURCES_DIR, exist_ok=True)\n",
        "else:\n",
        "    print(f\"âœ… Resources directory: {RESOURCES_DIR}\")\n",
        "\n",
        "if not os.path.exists(SUBMISSION_DIR):\n",
        "    print(f\"âš ï¸  Submission directory not found: {SUBMISSION_DIR}\")\n",
        "    print(\"   Creating it...\")\n",
        "    os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
        "else:\n",
        "    print(f\"âœ… Submission directory: {SUBMISSION_DIR}\")\n",
        "\n",
        "# Check required files in resources\n",
        "print(\"\\nğŸ“‹ Checking required files in resources/...\")\n",
        "required_files = [\n",
        "    'petase_pipeline_ndcg_optimized.py',\n",
        "    'predictive-pet-zero-shot-test-2025.csv',\n",
        "    'pet-2025-wildtype-cds.csv'\n",
        "]\n",
        "\n",
        "missing_files = []\n",
        "for filename in required_files:\n",
        "    filepath = f'{RESOURCES_DIR}/{filename}'\n",
        "    if os.path.exists(filepath):\n",
        "        size = os.path.getsize(filepath) / 1024  # KB\n",
        "        print(f\"   âœ… {filename} ({size:.1f} KB)\")\n",
        "    else:\n",
        "        print(f\"   âŒ {filename} - NOT FOUND\")\n",
        "        missing_files.append(filename)\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\nâš ï¸  Missing {len(missing_files)} file(s). Please upload them to:\")\n",
        "    print(f\"   {RESOURCES_DIR}/\")\n",
        "    print(f\"\\n   Missing: {missing_files}\")\n",
        "else:\n",
        "    print(\"\\nğŸ‰ All required files found!\")\n",
        "\n",
        "# Change to base directory\n",
        "os.chdir(BASE_DIR)\n",
        "print(f\"\\nğŸ“ Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_deps",
        "outputId": "becc76e0-2618-4959-80c5-f8a8a32dc790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ Installing dependencies...\n",
            "   This may take 1-2 minutes...\n",
            "\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m155.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m146.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "âœ… Dependencies installed!\n",
            "\n",
            "ğŸ” Verifying installations...\n",
            "   âœ… All packages imported successfully\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL 2: Install Dependencies\n",
        "# ========================================\n",
        "\n",
        "print(\"ğŸ“¦ Installing dependencies...\")\n",
        "print(\"   This may take 1-2 minutes...\\n\")\n",
        "\n",
        "# Install ESM and other packages\n",
        "!pip install esm pandas numpy scipy tqdm torch -q\n",
        "\n",
        "print(\"\\nâœ… Dependencies installed!\")\n",
        "\n",
        "# Verify imports\n",
        "print(\"\\nğŸ” Verifying installations...\")\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from scipy.stats import gmean\n",
        "    import torch\n",
        "    from esm.sdk import client as esm_client\n",
        "    from esm.sdk.api import ESMProtein, LogitsConfig\n",
        "    print(\"   âœ… All packages imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"   âŒ Import error: {e}\")\n",
        "    print(\"   Please re-run this cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prepare_data",
        "outputId": "87db527c-68dd-4bcf-e0ec-7a2e915f1718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Preparing data files...\n",
            "\n",
            "ğŸ“‹ Copying pipeline script...\n",
            "   âœ… Copied to working directory\n",
            "\n",
            "ğŸ“‹ Copying test data...\n",
            "   âœ… Copied test data\n",
            "\n",
            "ğŸ”§ Fixing CDS file column names...\n",
            "   Original columns: ['Wt AA Sequence', 'CDS']\n",
            "   Fixed columns: ['sequence', 'cds']\n",
            "\n",
            "   Verification:\n",
            "     Protein sequence length: 259 aa\n",
            "     DNA sequence length: 777 bp\n",
            "     Number of sequences: 313\n",
            "\n",
            "âœ… Saved fixed CDS file: pet-2025-wildtype-cds-fixed.csv\n",
            "\n",
            "ğŸ‰ Data preparation complete!\n",
            "\n",
            "ğŸ“ Working directory files:\n",
            "   â€¢ petase_pipeline_ndcg_optimized.py (36.6 KB)\n",
            "   â€¢ predictive-pet-zero-shot-test-2025.csv (1274.2 KB)\n",
            "   â€¢ pet-2025-wildtype-cds-fixed.csv (301.5 KB)\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL 3: Prepare Data (Fix CDS file)\n",
        "# ========================================\n",
        "\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "print(\"ğŸ”§ Preparing data files...\\n\")\n",
        "\n",
        "# Copy pipeline script to working directory\n",
        "print(\"ğŸ“‹ Copying pipeline script...\")\n",
        "pipeline_src = f'{RESOURCES_DIR}/petase_pipeline_ndcg_optimized.py'\n",
        "pipeline_dst = 'petase_pipeline_ndcg_optimized.py'\n",
        "shutil.copy(pipeline_src, pipeline_dst)\n",
        "print(f\"   âœ… Copied to working directory\")\n",
        "\n",
        "# Copy test data to working directory\n",
        "print(\"\\nğŸ“‹ Copying test data...\")\n",
        "test_src = f'{RESOURCES_DIR}/predictive-pet-zero-shot-test-2025.csv'\n",
        "test_dst = 'predictive-pet-zero-shot-test-2025.csv'\n",
        "shutil.copy(test_src, test_dst)\n",
        "print(f\"   âœ… Copied test data\")\n",
        "\n",
        "# Fix CDS file column names\n",
        "print(\"\\nğŸ”§ Fixing CDS file column names...\")\n",
        "cds_src = f'{RESOURCES_DIR}/pet-2025-wildtype-cds.csv'\n",
        "cds_df = pd.read_csv(cds_src)\n",
        "\n",
        "print(f\"   Original columns: {cds_df.columns.tolist()}\")\n",
        "\n",
        "# Rename columns\n",
        "cds_df = cds_df.rename(columns={\n",
        "    'Wt AA Sequence': 'sequence',\n",
        "    'CDS': 'cds'\n",
        "})\n",
        "\n",
        "print(f\"   Fixed columns: {cds_df.columns.tolist()}\")\n",
        "\n",
        "# Verify\n",
        "print(\"\\n   Verification:\")\n",
        "print(f\"     Protein sequence length: {len(cds_df['sequence'].iloc[0])} aa\")\n",
        "print(f\"     DNA sequence length: {len(cds_df['cds'].iloc[0])} bp\")\n",
        "print(f\"     Number of sequences: {len(cds_df)}\")\n",
        "\n",
        "# Save fixed version\n",
        "cds_fixed = 'pet-2025-wildtype-cds-fixed.csv'\n",
        "cds_df.to_csv(cds_fixed, index=False)\n",
        "print(f\"\\nâœ… Saved fixed CDS file: {cds_fixed}\")\n",
        "\n",
        "print(\"\\nğŸ‰ Data preparation complete!\")\n",
        "print(\"\\nğŸ“ Working directory files:\")\n",
        "import os\n",
        "for f in os.listdir('.'):\n",
        "    if f.endswith(('.py', '.csv')):\n",
        "        size = os.path.getsize(f) / 1024\n",
        "        print(f\"   â€¢ {f} ({size:.1f} KB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "run_pipeline",
        "outputId": "dd7ba55a-6db6-4973-a0f9-21bf33bbc62d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "               ğŸš€ STARTING PETASE PIPELINE\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ No checkpoint found - starting from beginning\n",
            "\n",
            "â±ï¸  Estimated time: 2-3 hours\n",
            "ğŸ’¾ Checkpoints saved every 100 sequences\n",
            "ğŸ“ Final outputs will be in: /content/drive/MyDrive/PET2025/pipeline/submission/\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "          PETase Prediction Pipeline - NDCG@10 Optimized\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ Input: predictive-pet-zero-shot-test-2025.csv\n",
            "ğŸ“ CDS: pet-2025-wildtype-cds-fixed.csv\n",
            "ğŸ“ Output prefix: submission\n",
            "ğŸ¯ Strategy: all\n",
            "\n",
            "ğŸ“‚ Loading data...\n",
            "âœ… Loaded 4988 sequences\n",
            "âœ… Loaded 313 wildtype CDS\n",
            "\n",
            "ğŸ” Detecting column names...\n",
            "   Test CSV columns: ['sequence', 'activity_1 (Î¼mol [TPA]/minÂ·mg [E])', 'activity_2 (Î¼mol [TPA]/minÂ·mg [E])', 'expression (mg/mL)']\n",
            "   CDS CSV columns: ['sequence', 'cds']\n",
            "âœ… Column names detected and standardized\n",
            "ğŸ§¬ Initializing ESM3 client...\n",
            "âœ… ESM3 client initialized successfully\n",
            "\n",
            "======================================================================\n",
            "PHASE 1: ESM3 Evolutionary Features\n",
            "======================================================================\n",
            "Processing sequences:  18% 914/4988 [32:56<2:50:55,  2.52s/it]Retrying... Attempt 1 after 1.0s due to: (502, 'Failure in encode: {\"status\":\"error\",\"message\":\"Model unavailable - please retry\"}')\n",
            "Processing sequences:  25% 1263/4988 [45:37<2:12:38,  2.14s/it]Retrying... Attempt 1 after 1.0s due to: (502, 'Failure in logits: <html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n</body>\\r\\n</html>\\r\\n')\n",
            "Processing sequences:  38% 1885/4988 [1:07:58<1:47:48,  2.08s/it]Retrying... Attempt 1 after 1.0s due to: (502, 'Failure in logits: <html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n</body>\\r\\n</html>\\r\\n')\n",
            "Processing sequences:  46% 2292/4988 [1:22:29<1:34:15,  2.10s/it]Retrying... Attempt 1 after 1.0s due to: (502, 'Failure in encode: <html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n</body>\\r\\n</html>\\r\\n')\n",
            "Processing sequences:  83% 4159/4988 [2:31:54<28:45,  2.08s/it]Retrying... Attempt 1 after 1.0s due to: (502, 'Failure in logits: <html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n</body>\\r\\n</html>\\r\\n')\n",
            "Processing sequences:  83% 4164/4988 [2:32:06<29:35,  2.15s/it]Retrying... Attempt 1 after 1.0s due to: (502, 'Failure in logits: <html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n</body>\\r\\n</html>\\r\\n')\n",
            "Processing sequences:  89% 4418/4988 [2:41:43<21:23,  2.25s/it]Retrying... Attempt 1 after 1.0s due to: (502, 'Failure in logits: <html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n</body>\\r\\n</html>\\r\\n')\n",
            "Processing sequences:  90% 4502/4988 [2:44:58<16:58,  2.10s/it]Retrying... Attempt 1 after 1.0s due to: (502, 'Failure in logits: <html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n</body>\\r\\n</html>\\r\\n')\n",
            "Processing sequences: 100% 4988/4988 [2:55:24<00:00,  2.11s/it]\n",
            "\n",
            "âœ… Phase 1 complete: 4988 sequences processed\n",
            "ğŸ’¾ Saved phase1_complete.csv\n",
            "\n",
            "======================================================================\n",
            "PHASE 2: Structure Features\n",
            "======================================================================\n",
            "ğŸ“ Computing distances to active site...\n",
            "ğŸ§¬ Predicting secondary structure...\n",
            "ğŸ”ï¸  Estimating burial status...\n",
            "âš ï¸  Computing structure risk scores...\n",
            "âœ… Phase 2 complete: 16 features total\n",
            "ğŸ’¾ Saved phase2_complete.csv\n",
            "\n",
            "======================================================================\n",
            "PHASE 3: pH and Codon Features\n",
            "======================================================================\n",
            "ğŸ§ª Computing pH-dependent charges...\n",
            "ğŸ§¬ Computing codon usage features...\n",
            "âœ… Phase 3 complete: 26 features total\n",
            "ğŸ’¾ Saved phase3_complete.csv\n",
            "\n",
            "======================================================================\n",
            "PHASE 4: NDCG-Optimized Predictions\n",
            "======================================================================\n",
            "\n",
            "ğŸ¯ Generating base model predictions...\n",
            "\n",
            "ğŸ”„ Creating balanced ensemble...\n",
            "\n",
            "ğŸ“Š Top 498 variants for activity_1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'mutation_llr'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/PET2025/pipeline/petase_pipeline_ndcg_optimized.py\", line 980, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/PET2025/pipeline/petase_pipeline_ndcg_optimized.py\", line 945, in main\n",
            "    submissions = Phase4NDCGOptimized.generate_predictions(test_df)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/PET2025/pipeline/petase_pipeline_ndcg_optimized.py\", line 827, in generate_predictions\n",
            "    Phase4NDCGOptimized.analyze_top_variants(submission, target_name, top_k)\n",
            "  File \"/content/drive/MyDrive/PET2025/pipeline/petase_pipeline_ndcg_optimized.py\", line 770, in analyze_top_variants\n",
            "    print(f\"   Mean LLR: {top_variants['mutation_llr'].mean():.2f}\")\n",
            "                          ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
            "    indexer = self.columns.get_loc(key)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'mutation_llr'\n",
            "\n",
            "======================================================================\n",
            "                    âœ… PIPELINE COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL 4: Run Pipeline\n",
        "# ========================================\n",
        "# âš ï¸  This cell takes 2-3 hours to complete!\n",
        "#\n",
        "# Features:\n",
        "# - Auto-saves checkpoint every 100 sequences\n",
        "# - If disconnected, re-run this cell to resume\n",
        "# - All progress saved to Google Drive\n",
        "# ========================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" \"*15 + \"ğŸš€ STARTING PETASE PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Your ESM3 token\n",
        "ESM3_TOKEN = \"4KmDJvB1gQaRJJDrWoBa91\"\n",
        "\n",
        "# Check if checkpoint exists\n",
        "checkpoint_file = 'phase1_checkpoint.pkl'\n",
        "if os.path.exists(checkpoint_file):\n",
        "    import pickle\n",
        "    with open(checkpoint_file, 'rb') as f:\n",
        "        checkpoint = pickle.load(f)\n",
        "    last_idx = checkpoint.get('last_index', -1)\n",
        "    print(f\"\\nğŸ“‚ Checkpoint found!\")\n",
        "    print(f\"   Last processed: {last_idx + 1} / 4988 sequences\")\n",
        "    print(f\"   Progress: {100*(last_idx+1)/4988:.1f}%\")\n",
        "    print(f\"\\nğŸ”„ Pipeline will resume from sequence {last_idx + 2}\")\n",
        "else:\n",
        "    print(f\"\\nğŸ“ No checkpoint found - starting from beginning\")\n",
        "\n",
        "print(f\"\\nâ±ï¸  Estimated time: 2-3 hours\")\n",
        "print(f\"ğŸ’¾ Checkpoints saved every 100 sequences\")\n",
        "print(f\"ğŸ“ Final outputs will be in: {SUBMISSION_DIR}/\")\n",
        "print(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "# Run pipeline\n",
        "!python petase_pipeline_ndcg_optimized.py \\\n",
        "    --input predictive-pet-zero-shot-test-2025.csv \\\n",
        "    --cds pet-2025-wildtype-cds-fixed.csv \\\n",
        "    --output submission \\\n",
        "    --token {ESM3_TOKEN}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" \"*20 + \"âœ… PIPELINE COMPLETE!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "move_results"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 5: Move Results to Submission Folder\n",
        "# ========================================\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ“¦ Moving results to submission folder...\\n\")\n",
        "\n",
        "# List of output files to move\n",
        "output_files = [\n",
        "    'submission_balanced.csv',\n",
        "    'submission_conservative.csv',\n",
        "    'submission_aggressive.csv',\n",
        "    'submission_diverse.csv',\n",
        "    'submission_base_only.csv',\n",
        "    'phase1_complete.csv',\n",
        "    'phase2_complete.csv',\n",
        "    'phase3_complete.csv'\n",
        "]\n",
        "\n",
        "# Add timestamp to folder name\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "run_dir = f'{SUBMISSION_DIR}/run_{timestamp}'\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ“ Creating directory: {run_dir}\\n\")\n",
        "\n",
        "moved_count = 0\n",
        "for filename in output_files:\n",
        "    src = filename\n",
        "    if os.path.exists(src):\n",
        "        dst = f'{run_dir}/{filename}'\n",
        "        shutil.copy(src, dst)\n",
        "        size = os.path.getsize(dst) / 1024  # KB\n",
        "        print(f\"   âœ… {filename} ({size:.1f} KB)\")\n",
        "        moved_count += 1\n",
        "    else:\n",
        "        print(f\"   âš ï¸  {filename} - not found (skipped)\")\n",
        "\n",
        "print(f\"\\nâœ… Moved {moved_count} files to submission folder\")\n",
        "print(f\"\\nğŸ“‚ Results location:\")\n",
        "print(f\"   {run_dir}\")\n",
        "\n",
        "# Also copy to main submission folder (latest)\n",
        "print(f\"\\nğŸ“‹ Copying submission files to main folder...\")\n",
        "submission_files = ['submission_balanced.csv', 'submission_conservative.csv']\n",
        "for filename in submission_files:\n",
        "    src = filename\n",
        "    if os.path.exists(src):\n",
        "        dst = f'{SUBMISSION_DIR}/{filename}'\n",
        "        shutil.copy(src, dst)\n",
        "        print(f\"   âœ… {filename}\")\n",
        "\n",
        "print(f\"\\nğŸ‰ All results saved!\")\n",
        "print(f\"\\nğŸ“Š Access your submissions at:\")\n",
        "print(f\"   Main: {SUBMISSION_DIR}/\")\n",
        "print(f\"   This run: {run_dir}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_results"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 6: Check Results\n",
        "# ========================================\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" \"*20 + \"ğŸ“Š RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check submission files\n",
        "submission_files = {\n",
        "    'Balanced (Recommended)': 'submission_balanced.csv',\n",
        "    'Conservative': 'submission_conservative.csv',\n",
        "    'Aggressive': 'submission_aggressive.csv',\n",
        "    'Diverse': 'submission_diverse.csv',\n",
        "    'Base Only': 'submission_base_only.csv'\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“‹ Generated Submission Files:\\n\")\n",
        "\n",
        "for strategy, filename in submission_files.items():\n",
        "    filepath = f'{SUBMISSION_DIR}/{filename}'\n",
        "    if os.path.exists(filepath):\n",
        "        df = pd.read_csv(filepath)\n",
        "        size = os.path.getsize(filepath) / 1024\n",
        "        print(f\"âœ… {strategy:25} {filename}\")\n",
        "        print(f\"   Sequences: {len(df):,}   Size: {size:.1f} KB\")\n",
        "        print(f\"   Columns: {list(df.columns)}\")\n",
        "        print(f\"   Activity 1: mean={df['activity_1'].mean():.2f}, range=[{df['activity_1'].min():.2f}, {df['activity_1'].max():.2f}]\")\n",
        "        print(f\"   Activity 2: mean={df['activity_2'].mean():.2f}, range=[{df['activity_2'].min():.2f}, {df['activity_2'].max():.2f}]\")\n",
        "        print(f\"   Expression: mean={df['expression'].mean():.2f}, range=[{df['expression'].min():.2f}, {df['expression'].max():.2f}]\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"âŒ {strategy:25} {filename} - NOT FOUND\")\n",
        "        print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ’¡ Recommendations:\")\n",
        "print(\"   1st submission: submission_balanced.csv\")\n",
        "print(\"   2nd submission: submission_conservative.csv\")\n",
        "print(\"   3rd submission: submission_aggressive.csv or submission_diverse.csv\")\n",
        "print(\"\\nğŸ“‚ All files are in: \" + SUBMISSION_DIR)\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recovery_info"
      },
      "source": [
        "# ğŸ”„ Disconnect Recovery Instructions\n",
        "\n",
        "## If Colab Disconnects During Phase 1:\n",
        "\n",
        "### Option 1: Quick Resume (Recommended)\n",
        "1. Just re-run Cell 4 (Run Pipeline)\n",
        "2. Pipeline will automatically detect checkpoint\n",
        "3. It will resume from last saved position\n",
        "\n",
        "### Option 2: Full Restart\n",
        "If you want to start fresh:\n",
        "1. Delete checkpoint file:\n",
        "   ```python\n",
        "   import os\n",
        "   if os.path.exists('phase1_checkpoint.pkl'):\n",
        "       os.remove('phase1_checkpoint.pkl')\n",
        "       print('Checkpoint deleted')\n",
        "   ```\n",
        "2. Re-run Cell 4\n",
        "\n",
        "## How Checkpointing Works:\n",
        "- Pipeline saves progress every 100 sequences\n",
        "- Checkpoint file: `phase1_checkpoint.pkl`\n",
        "- Stored in Google Drive (persists across sessions)\n",
        "- Contains: processed sequences + computed features\n",
        "\n",
        "## Progress Tracking:\n",
        "You'll see output like:\n",
        "```\n",
        "Processing sequences: 45% 2244/4988 [1:15:00<1:22:00, 1.8s/it]\n",
        "```\n",
        "\n",
        "This means:\n",
        "- 45% complete (2244 out of 4988 sequences)\n",
        "- 1:15:00 elapsed\n",
        "- 1:22:00 remaining\n",
        "- 1.8 seconds per sequence\n",
        "\n",
        "## Tips for Avoiding Disconnects:\n",
        "1. Keep browser tab active (don't minimize)\n",
        "2. Use Colab Pro+ for more stable connections\n",
        "3. Don't close laptop (if applicable)\n",
        "4. Check internet connection is stable\n",
        "\n",
        "## Checking Progress After Reconnect:\n",
        "```python\n",
        "import pickle\n",
        "with open('phase1_checkpoint.pkl', 'rb') as f:\n",
        "    checkpoint = pickle.load(f)\n",
        "print(f\"Last processed: {checkpoint['last_index'] + 1} / 4988\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_checkpoint"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 7: Check Checkpoint Status (Optional)\n",
        "# ========================================\n",
        "# Run this anytime to check progress\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "checkpoint_file = 'phase1_checkpoint.pkl'\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    try:\n",
        "        with open(checkpoint_file, 'rb') as f:\n",
        "            checkpoint = pickle.load(f)\n",
        "\n",
        "        last_idx = checkpoint.get('last_index', -1)\n",
        "        total = 4988\n",
        "\n",
        "        print(\"ğŸ“Š Checkpoint Status\\n\")\n",
        "        print(f\"   Last processed index: {last_idx}\")\n",
        "        print(f\"   Completed sequences: {last_idx + 1} / {total}\")\n",
        "        print(f\"   Progress: {100*(last_idx+1)/total:.1f}%\")\n",
        "        print(f\"   Remaining: {total - (last_idx + 1)} sequences\")\n",
        "\n",
        "        # Estimate time remaining (assuming 2s per sequence)\n",
        "        remaining = total - (last_idx + 1)\n",
        "        est_seconds = remaining * 2\n",
        "        est_hours = est_seconds / 3600\n",
        "        est_minutes = (est_seconds % 3600) / 60\n",
        "\n",
        "        print(f\"\\n   Estimated time remaining: {int(est_hours)}h {int(est_minutes)}m\")\n",
        "        print(f\"   (assuming 2 seconds per sequence)\")\n",
        "\n",
        "        # Check if any features are computed\n",
        "        df = checkpoint.get('df')\n",
        "        if df is not None:\n",
        "            print(f\"\\n   DataFrame shape: {df.shape}\")\n",
        "            print(f\"   Columns: {len(df.columns)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error reading checkpoint: {e}\")\n",
        "else:\n",
        "    print(\"ğŸ“ No checkpoint file found\")\n",
        "    print(\"   Either pipeline hasn't started, or Phase 1 is complete\")\n",
        "\n",
        "    # Check if Phase 1 complete file exists\n",
        "    if os.path.exists('phase1_complete.csv'):\n",
        "        import pandas as pd\n",
        "        df = pd.read_csv('phase1_complete.csv')\n",
        "        print(f\"\\nâœ… Phase 1 complete!\")\n",
        "        print(f\"   Sequences processed: {len(df)}\")\n",
        "        print(f\"   Features computed: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_results"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELL 8: Download Results to Local (Optional)\n",
        "# ========================================\n",
        "# Only needed if you want to download files to your computer\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"ğŸ“¥ Downloading submission files...\\n\")\n",
        "\n",
        "# Download main submission files\n",
        "download_files = [\n",
        "    f'{SUBMISSION_DIR}/submission_balanced.csv',\n",
        "    f'{SUBMISSION_DIR}/submission_conservative.csv'\n",
        "]\n",
        "\n",
        "for filepath in download_files:\n",
        "    if os.path.exists(filepath):\n",
        "        filename = os.path.basename(filepath)\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        files.download(filepath)\n",
        "    else:\n",
        "        print(f\"âŒ {filepath} not found\")\n",
        "\n",
        "print(\"\\nâœ… Downloads complete!\")\n",
        "print(\"\\nğŸ’¡ Note: Files are already saved in Google Drive at:\")\n",
        "print(f\"   {SUBMISSION_DIR}/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "esm1v",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
